program=ml.DeveloperApiExample
SPARKLORD_MODE=CONFIG_INJECTION
cpu_cores=2
cpu_util=200
memory=4g
config_file_name=spark-defaults.conf
config_key=spark.shuffle.push.enabled
config_value=True

2025-03-25 12:11:46,075 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for TERM
2025-03-25 12:11:46,079 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for HUP
2025-03-25 12:11:46,079 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for INT
2025-03-25 12:11:46,309 DEBUG [main] org.apache.hadoop.util.Shell: setsid exited with exit code 0
2025-03-25 12:11:46,438 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-25 12:11:46,439 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-25 12:11:46,439 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-25 12:11:46,439 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-25 12:11:46,440 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-25 12:11:46,482 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
2025-03-25 12:11:46,487 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
2025-03-25 12:11:46,487 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
2025-03-25 12:11:46,487 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
2025-03-25 12:11:46,487 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
2025-03-25 12:11:46,488 DEBUG [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
2025-03-25 12:11:46,504 DEBUG [main] org.apache.hadoop.security.SecurityUtil: Setting hadoop.security.token.service.use_ip to true
2025-03-25 12:11:46,508 DEBUG [main] org.apache.hadoop.security.Groups:  Creating new Groups object
2025-03-25 12:11:46,509 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
2025-03-25 12:11:46,509 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2025-03-25 12:11:46,509 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-25 12:11:46,509 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-03-25 12:11:46,510 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Falling back to shell based
2025-03-25 12:11:46,511 DEBUG [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2025-03-25 12:11:46,553 DEBUG [main] org.apache.hadoop.security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2025-03-25 12:11:46,618 DEBUG [main] org.apache.spark.deploy.SparkHadoopUtil: creating UGI for user: root
2025-03-25 12:11:46,620 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Hadoop login
2025-03-25 12:11:46,620 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: hadoop login commit
2025-03-25 12:11:46,621 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using local user: UnixPrincipal: root
2025-03-25 12:11:46,622 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using user: "UnixPrincipal: root" with name: root
2025-03-25 12:11:46,622 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: User entry: "root"
2025-03-25 12:11:46,623 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Reading credentials from location /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000001/container_tokens
2025-03-25 12:11:46,625 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Loaded 1 tokens from /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000001/container_tokens
2025-03-25 12:11:46,626 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: UGI loginUser: root (auth:SIMPLE)
2025-03-25 12:11:46,626 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3@1522d8a0]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-25 12:11:46,631 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1742904630293_0001_000001
2025-03-25 12:11:46,632 DEBUG [main] org.apache.spark.util.ShutdownHookManager: Adding shutdown hook
2025-03-25 12:11:46,655 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Starting the user application in a separate Thread
2025-03-25 12:11:46,659 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Waiting for spark context initialization...
2025-03-25 12:11:46,698 INFO [Driver] org.apache.spark.SparkContext: Running Spark version 3.3.2
2025-03-25 12:11:46,714 INFO [Driver] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-25 12:11:46,715 INFO [Driver] org.apache.spark.resource.ResourceUtils: No custom resources configured for spark.driver.
2025-03-25 12:11:46,715 INFO [Driver] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-25 12:11:46,715 INFO [Driver] org.apache.spark.SparkContext: Submitted application: DeveloperApiExample
2025-03-25 12:11:46,727 INFO [Driver] org.apache.spark.resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-03-25 12:11:46,731 INFO [Driver] org.apache.spark.resource.ResourceProfile: Limiting resource is cpus at 2 tasks per executor
2025-03-25 12:11:46,733 INFO [Driver] org.apache.spark.resource.ResourceProfileManager: Added ResourceProfile id: 0
2025-03-25 12:11:46,757 INFO [Driver] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-25 12:11:46,757 INFO [Driver] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-25 12:11:46,757 INFO [Driver] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-25 12:11:46,757 INFO [Driver] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-25 12:11:46,757 INFO [Driver] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-25 12:11:46,790 DEBUG [Driver] io.netty.util.internal.logging.InternalLoggerFactory: Using SLF4J as the default logging framework
2025-03-25 12:11:46,794 DEBUG [Driver] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2025-03-25 12:11:46,794 DEBUG [Driver] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2025-03-25 12:11:46,801 DEBUG [Driver] io.netty.channel.MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
2025-03-25 12:11:46,820 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: -Dio.netty.noUnsafe: false
2025-03-25 12:11:46,821 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: Java version: 8
2025-03-25 12:11:46,821 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
2025-03-25 12:11:46,821 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.copyMemory: available
2025-03-25 12:11:46,822 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.Buffer.address: available
2025-03-25 12:11:46,822 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: direct buffer constructor: available
2025-03-25 12:11:46,822 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.Bits.unaligned: available, true
2025-03-25 12:11:46,822 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2025-03-25 12:11:46,822 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
2025-03-25 12:11:46,822 DEBUG [Driver] io.netty.util.internal.PlatformDependent: sun.misc.Unsafe: available
2025-03-25 12:11:46,822 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.tmpdir: /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000001/tmp (java.io.tmpdir)
2025-03-25 12:11:46,823 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
2025-03-25 12:11:46,823 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.maxDirectMemory: 3817865216 bytes
2025-03-25 12:11:46,823 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
2025-03-25 12:11:46,824 DEBUG [Driver] io.netty.util.internal.CleanerJava6: java.nio.ByteBuffer.cleaner(): available
2025-03-25 12:11:46,824 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.noPreferDirect: false
2025-03-25 12:11:46,824 DEBUG [Driver] io.netty.channel.nio.NioEventLoop: -Dio.netty.noKeySetOptimization: false
2025-03-25 12:11:46,824 DEBUG [Driver] io.netty.channel.nio.NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
2025-03-25 12:11:46,828 DEBUG [Driver] io.netty.util.internal.PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
2025-03-25 12:11:46,839 DEBUG [Driver] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
2025-03-25 12:11:46,839 DEBUG [Driver] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
2025-03-25 12:11:46,840 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
2025-03-25 12:11:46,840 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
2025-03-25 12:11:46,840 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
2025-03-25 12:11:46,840 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
2025-03-25 12:11:46,840 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
2025-03-25 12:11:46,841 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
2025-03-25 12:11:46,841 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
2025-03-25 12:11:46,841 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2025-03-25 12:11:46,841 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
2025-03-25 12:11:46,841 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2025-03-25 12:11:46,841 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
2025-03-25 12:11:46,841 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2025-03-25 12:11:46,860 DEBUG [Driver] io.netty.channel.DefaultChannelId: -Dio.netty.processId: 1141 (auto-detected)
2025-03-25 12:11:46,861 DEBUG [Driver] io.netty.util.NetUtil: -Djava.net.preferIPv4Stack: false
2025-03-25 12:11:46,861 DEBUG [Driver] io.netty.util.NetUtil: -Djava.net.preferIPv6Addresses: false
2025-03-25 12:11:46,862 DEBUG [Driver] io.netty.util.NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
2025-03-25 12:11:46,862 DEBUG [Driver] io.netty.util.NetUtil: /proc/sys/net/core/somaxconn: 4096
2025-03-25 12:11:46,863 DEBUG [Driver] io.netty.channel.DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:14:01:0d (auto-detected)
2025-03-25 12:11:46,871 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.allocator.type: pooled
2025-03-25 12:11:46,872 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0
2025-03-25 12:11:46,872 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
2025-03-25 12:11:46,882 DEBUG [Driver] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 40591
2025-03-25 12:11:46,889 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 40591.
2025-03-25 12:11:46,890 DEBUG [Driver] org.apache.spark.SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
2025-03-25 12:11:46,905 WARN [Driver] org.apache.spark.util.Utils: Push-based shuffle can only be enabled when the application is submitted to run in YARN mode, with external shuffle service enabled, IO encryption disabled, and relocation of serialized objects supported.
2025-03-25 12:11:46,910 INFO [Driver] org.apache.spark.SparkEnv: Registering MapOutputTracker
2025-03-25 12:11:46,910 DEBUG [Driver] org.apache.spark.MapOutputTrackerMasterEndpoint: init
2025-03-25 12:11:46,932 INFO [Driver] org.apache.spark.SparkEnv: Registering BlockManagerMaster
2025-03-25 12:11:46,944 INFO [Driver] org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-03-25 12:11:46,945 WARN [Driver] org.apache.spark.util.Utils: Push-based shuffle can only be enabled when the application is submitted to run in YARN mode, with external shuffle service enabled, IO encryption disabled, and relocation of serialized objects supported.
2025-03-25 12:11:46,945 INFO [Driver] org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-03-25 12:11:46,985 INFO [Driver] org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
2025-03-25 12:11:47,008 INFO [Driver] org.apache.spark.storage.DiskBlockManager: Created local directory at /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/blockmgr-a6af2211-77d3-464f-8a19-6b2ee2b736ca
2025-03-25 12:11:47,009 WARN [Driver] org.apache.spark.util.Utils: Push-based shuffle can only be enabled when the application is submitted to run in YARN mode, with external shuffle service enabled, IO encryption disabled, and relocation of serialized objects supported.
2025-03-25 12:11:47,010 DEBUG [Driver] org.apache.spark.storage.DiskBlockManager: Adding shutdown hook
2025-03-25 12:11:47,021 INFO [Driver] org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 2004.6 MiB
2025-03-25 12:11:47,054 INFO [Driver] org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2025-03-25 12:11:47,055 DEBUG [Driver] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: init
2025-03-25 12:11:47,064 DEBUG [Driver] org.apache.spark.SecurityManager: Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
2025-03-25 12:11:47,220 DEBUG [Driver] org.apache.spark.ui.JettyUtils: Using requestHeaderSize: 8192
2025-03-25 12:11:47,239 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 36621.
2025-03-25 12:11:47,240 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,300 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Created YarnClusterScheduler
2025-03-25 12:11:47,318 WARN [Driver] org.apache.spark.util.Utils: Push-based shuffle can only be enabled when the application is submitted to run in YARN mode, with external shuffle service enabled, IO encryption disabled, and relocation of serialized objects supported.
2025-03-25 12:11:47,339 DEBUG [Driver] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 41185
2025-03-25 12:11:47,339 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41185.
2025-03-25 12:11:47,339 INFO [Driver] org.apache.spark.network.netty.NettyBlockTransferService: Server created on slave2:41185
2025-03-25 12:11:47,341 INFO [Driver] org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-03-25 12:11:47,345 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, slave2, 41185, None)
2025-03-25 12:11:47,347 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave2
2025-03-25 12:11:47,348 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave2:41185 with 2004.6 MiB RAM, BlockManagerId(driver, slave2, 41185, None)
2025-03-25 12:11:47,352 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, slave2, 41185, None)
2025-03-25 12:11:47,353 INFO [Driver] org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, slave2, 41185, None)
2025-03-25 12:11:47,463 DEBUG [Driver] org.apache.spark.util.YarnContainerInfoHelper: Base URL for logs: http://slave2:8042/node/containerlogs/container_1742904630293_0001_01_000001/root
2025-03-25 12:11:47,478 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,480 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,480 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,481 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,482 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,483 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,483 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,486 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,487 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,488 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,489 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,491 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,491 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,492 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,493 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,493 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,494 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,496 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,501 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,502 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,503 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,510 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,511 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,513 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,515 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,520 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:47,521 DEBUG [Driver] org.apache.spark.SparkContext: Adding shutdown hook
2025-03-25 12:11:47,528 DEBUG [main] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl entered state INITED
2025-03-25 12:11:47,533 INFO [main] org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master/172.20.1.10:8030
2025-03-25 12:11:47,533 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.RMProxy$1@12dae582]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.RMProxy.getProxy(RMProxy.java:145)
	at org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider.init(DefaultNoHARMFailoverProxyProvider.java:65)
	at org.apache.hadoop.yarn.client.RMProxy.createNonHaRMFailoverProxyProvider(RMProxy.java:172)
	at org.apache.hadoop.yarn.client.RMProxy.newProxyInstance(RMProxy.java:132)
	at org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:103)
	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:73)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.serviceStart(AMRMClientImpl.java:193)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:63)
	at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:440)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:518)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-25 12:11:47,534 DEBUG [main] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-25 12:11:47,534 DEBUG [main] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ApplicationMasterProtocol
2025-03-25 12:11:47,543 DEBUG [main] org.apache.hadoop.ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker@2611b9a3
2025-03-25 12:11:47,548 DEBUG [main] org.apache.hadoop.ipc.Client: getting client out of cache: Client-0c6efa569297424fa748f3835aa0493e
2025-03-25 12:11:47,562 DEBUG [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl is started
2025-03-25 12:11:47,563 INFO [main] org.apache.spark.deploy.yarn.YarnRMClient: Registering the ApplicationMaster
2025-03-25 12:11:47,590 DEBUG [main] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-25 12:11:47,590 DEBUG [main] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.10:8030
2025-03-25 12:11:47,590 DEBUG [main] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.10:8030
2025-03-25 12:11:47,593 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@3ebff828]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy31.registerApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.registerApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:108)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy32.registerApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:247)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:234)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:214)
	at org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:72)
	at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:440)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:518)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-25 12:11:47,640 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-25 12:11:47,644 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB info:org.apache.hadoop.yarn.security.SchedulerSecurityInfo$1@2a76b80a
2025-03-25 12:11:47,645 DEBUG [main] org.apache.hadoop.yarn.security.AMRMTokenSelector: Looking for a token with service 172.20.1.10:8030
2025-03-25 12:11:47,645 DEBUG [main] org.apache.hadoop.yarn.security.AMRMTokenSelector: Token kind is YARN_AM_RM_TOKEN and the token's service name is 172.20.1.10:8030
2025-03-25 12:11:47,647 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-25 12:11:47,648 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ApplicationMasterProtocolPB
2025-03-25 12:11:47,649 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEJW42OncMhABEP/MieAB
2025-03-25 12:11:47,649 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-25 12:11:47,649 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-25 12:11:47,651 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEJW42OncMhABEP/MieAB\",realm=\"default\",nonce=\"vmblfzaah2Ng+q6lKyvZ4df9Jn5Nn/+ajQJOIJLD\",nc=00000001,cnonce=\"dMbD9D4EzzhcWb/Yw9qd3u5FtNuzAqGY+YnvmbJ2\",digest-uri=\"/default\",maxbuf=65536,response=b67f847a2b735067d9fa5ded3eb51859,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-25 12:11:47,653 DEBUG [main] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-25 12:11:47,655 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root: starting, having connections 1
2025-03-25 12:11:47,656 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #0 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster
2025-03-25 12:11:47,676 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #0
2025-03-25 12:11:47,676 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: registerApplicationMaster took 109ms
2025-03-25 12:11:47,685 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Preparing Local resources
2025-03-25 12:11:47,716 DEBUG [main] org.apache.hadoop.fs.FileSystem: Starting: Acquiring creator semaphore for hdfs://master:9000/user/root/.sparkStaging/application_1742904630293_0001/__spark_conf__.zip
2025-03-25 12:11:47,716 DEBUG [main] org.apache.hadoop.fs.FileSystem: Acquiring creator semaphore for hdfs://master:9000/user/root/.sparkStaging/application_1742904630293_0001/__spark_conf__.zip: duration 0:00.000s
2025-03-25 12:11:47,716 DEBUG [main] org.apache.hadoop.fs.FileSystem: Starting: Creating FS hdfs://master:9000/user/root/.sparkStaging/application_1742904630293_0001/__spark_conf__.zip
2025-03-25 12:11:47,716 DEBUG [main] org.apache.hadoop.fs.FileSystem: Loading filesystems
2025-03-25 12:11:47,721 DEBUG [main] org.apache.hadoop.fs.FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hadoop-client-api-3.3.2.jar
2025-03-25 12:11:47,723 DEBUG [main] org.apache.hadoop.fs.FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hadoop-client-api-3.3.2.jar
2025-03-25 12:11:47,724 DEBUG [main] org.apache.hadoop.fs.FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hadoop-client-api-3.3.2.jar
2025-03-25 12:11:47,725 DEBUG [main] org.apache.hadoop.fs.FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hadoop-client-api-3.3.2.jar
2025-03-25 12:11:47,725 DEBUG [main] org.apache.hadoop.fs.FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hadoop-client-api-3.3.2.jar
2025-03-25 12:11:47,729 DEBUG [main] org.apache.hadoop.fs.FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hadoop-client-api-3.3.2.jar
2025-03-25 12:11:47,732 DEBUG [main] org.apache.hadoop.fs.FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hadoop-client-api-3.3.2.jar
2025-03-25 12:11:47,733 DEBUG [main] org.apache.hadoop.fs.FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hadoop-client-api-3.3.2.jar
2025-03-25 12:11:47,733 DEBUG [main] org.apache.hadoop.fs.FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hive-exec-2.3.9-core.jar
2025-03-25 12:11:47,734 DEBUG [main] org.apache.hadoop.fs.FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__6364021304033835350.zip/hive-exec-2.3.9-core.jar
2025-03-25 12:11:47,734 DEBUG [main] org.apache.hadoop.fs.FileSystem: Looking for FS supporting hdfs
2025-03-25 12:11:47,734 DEBUG [main] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.hdfs.impl
2025-03-25 12:11:47,745 DEBUG [main] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-25 12:11:47,746 DEBUG [main] org.apache.hadoop.fs.FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem
2025-03-25 12:11:47,756 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.use.legacy.blockreader.local = false
2025-03-25 12:11:47,756 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.read.shortcircuit = false
2025-03-25 12:11:47,756 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.domain.socket.data.traffic = false
2025-03-25 12:11:47,756 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.domain.socket.path = 
2025-03-25 12:11:47,760 DEBUG [main] org.apache.hadoop.hdfs.DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2025-03-25 12:11:47,762 DEBUG [main] org.apache.hadoop.io.retry.RetryUtils: multipleLinearRandomRetry = null
2025-03-25 12:11:47,765 DEBUG [main] org.apache.hadoop.ipc.Client: getting client out of cache: Client-0c6efa569297424fa748f3835aa0493e
2025-03-25 12:11:47,910 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
2025-03-25 12:11:47,914 DEBUG [main] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
2025-03-25 12:11:47,915 DEBUG [main] org.apache.hadoop.fs.FileSystem: Creating FS hdfs://master:9000/user/root/.sparkStaging/application_1742904630293_0001/__spark_conf__.zip: duration 0:00.199s
2025-03-25 12:11:47,919 DEBUG [main] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-25 12:11:47,919 DEBUG [main] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.10:9000
2025-03-25 12:11:47,919 DEBUG [main] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.10:9000
2025-03-25 12:11:47,919 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@322803db]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy36.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy37.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$prepareLocalResources$4(ApplicationMaster.scala:200)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$prepareLocalResources$4$adapted(ApplicationMaster.scala:197)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.deploy.yarn.ApplicationMaster.prepareLocalResources(ApplicationMaster.scala:197)
	at org.apache.spark.deploy.yarn.ApplicationMaster.createAllocator(ApplicationMaster.scala:463)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:523)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-25 12:11:47,920 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-25 12:11:47,920 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
2025-03-25 12:11:47,920 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one
2025-03-25 12:11:47,920 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Use SIMPLE authentication for protocol ClientNamenodeProtocolPB
2025-03-25 12:11:47,920 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
auths {
  method: "SIMPLE"
  mechanism: ""
}

2025-03-25 12:11:47,921 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2025-03-25 12:11:47,921 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root: starting, having connections 2
2025-03-25 12:11:47,921 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root got value #1
2025-03-25 12:11:47,921 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: getFileInfo took 2ms
2025-03-25 12:11:47,948 DEBUG [main] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-25 12:11:47,969 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: 
===============================================================================
Default YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
    SPARK_YARN_STAGING_DIR -> hdfs://master:9000/user/root/.sparkStaging/application_1742904630293_0001
    SPARK_USER -> root

  command:
    {{JAVA_HOME}}/bin/java \ 
      -server \ 
      -Xmx2048m \ 
      '-XX:+IgnoreUnrecognizedVMOptions' \ 
      '--add-opens=java.base/java.lang=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.io=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.net=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.nio=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.security.action=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED' \ 
      '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED' \ 
      -Djava.io.tmpdir={{PWD}}/tmp \ 
      '-Dspark.driver.port=40591' \ 
      '-Dspark.ui.port=0' \ 
      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
      -XX:OnOutOfMemoryError='kill %p' \ 
      org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
      --driver-url \ 
      spark://CoarseGrainedScheduler@slave2:40591 \ 
      --executor-id \ 
      <executorId> \ 
      --hostname \ 
      <hostname> \ 
      --cores \ 
      2 \ 
      --app-id \ 
      application_1742904630293_0001 \ 
      --resourceProfileId \ 
      0 \ 
      1><LOG_DIR>/stdout \ 
      2><LOG_DIR>/stderr

  resources:
    __app__.jar -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742904630293_0001/scopt_2.12-3.7.1.jar" } size: 78803 timestamp: 1742904702216 type: FILE visibility: PRIVATE
    __spark_libs__ -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742904630293_0001/__spark_libs__6364021304033835350.zip" } size: 301733843 timestamp: 1742904702158 type: ARCHIVE visibility: PRIVATE
    __spark_conf__ -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742904630293_0001/__spark_conf__.zip" } size: 946986 timestamp: 1742904702350 type: ARCHIVE visibility: PRIVATE
    spark-examples_2.12-3.3.2.jar -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742904630293_0001/spark-examples_2.12-3.3.2.jar" } size: 1567446 timestamp: 1742904702245 type: FILE visibility: PRIVATE

===============================================================================
2025-03-25 12:11:47,984 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Resource profile 0 doesn't exist, adding it
2025-03-25 12:11:48,002 INFO [main] org.apache.hadoop.conf.Configuration: resource-types.xml not found
2025-03-25 12:11:48,002 INFO [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
2025-03-25 12:11:48,005 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
2025-03-25 12:11:48,005 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
2025-03-25 12:11:48,005 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.memory-mb.minimum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.minimum-allocation-mb'
2025-03-25 12:11:48,005 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.memory-mb.maximum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.maximum-allocation-mb'
2025-03-25 12:11:48,005 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.vcores.minimum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.minimum-allocation-vcores'
2025-03-25 12:11:48,005 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.vcores.maximum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.maximum-allocation-vcores'
2025-03-25 12:11:48,007 DEBUG [main] org.apache.spark.deploy.yarn.ResourceRequestHelper: Custom resources requested: Map()
2025-03-25 12:11:48,007 DEBUG [main] org.apache.spark.deploy.yarn.YarnAllocator: Created resource capability: <memory:2432, vCores:2>
2025-03-25 12:11:48,009 INFO [dispatcher-event-loop-1] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@slave2:40591)
2025-03-25 12:11:48,010 DEBUG [main] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 0, executorsStarting: 0
2025-03-25 12:11:48,013 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Will request 3 executor container(s) for  ResourceProfile Id: 0, each with 2 core(s) and 2432 MB memory. with custom resources: <memory:2432, vCores:2>
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added priority=0
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added resourceName=*
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added Execution Type=GUARANTEED
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 1, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 1, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=1 #asks=1
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 2, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 2, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=2 #asks=1
2025-03-25 12:11:48,023 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-25 12:11:48,024 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-25 12:11:48,024 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=3 #asks=1
2025-03-25 12:11:48,024 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Submitted 3 unlocalized container requests.
2025-03-25 12:11:48,033 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #2 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-25 12:11:48,039 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #2
2025-03-25 12:11:48,039 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 6ms
2025-03-25 12:11:48,047 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
2025-03-25 12:11:48,047 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-25 12:11:48,048 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 3, running: 0, executorsStarting: 0
2025-03-25 12:11:48,048 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #3 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-25 12:11:48,050 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #3
2025-03-25 12:11:48,050 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 2ms
2025-03-25 12:11:48,251 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 3. Slept for 200122067/200.
2025-03-25 12:11:48,251 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-25 12:11:48,251 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 3, running: 0, executorsStarting: 0
2025-03-25 12:11:48,252 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #4 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-25 12:11:48,253 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #4
2025-03-25 12:11:48,253 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 1ms
2025-03-25 12:11:48,654 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 3. Slept for 400122194/400.
2025-03-25 12:11:48,654 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-25 12:11:48,654 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 3, running: 0, executorsStarting: 0
2025-03-25 12:11:48,655 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #5 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-25 12:11:48,660 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #5
2025-03-25 12:11:48,660 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 6ms
2025-03-25 12:11:48,662 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave0:34073
2025-03-25 12:11:48,662 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave2:38023
2025-03-25 12:11:48,662 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave1:44559
2025-03-25 12:11:48,664 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Allocated containers: 3. Current executor count: 0. Launching executor count: 0. Cluster resources: <memory:10240, vCores:20>.
2025-03-25 12:11:48,665 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave0, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,665 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave1, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,665 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave2, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,666 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,666 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,666 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=3
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=2 #asks=1
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=2
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=1 #asks=1
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=1
2025-03-25 12:11:48,667 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=0 #asks=1
2025-03-25 12:11:48,668 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742904630293_0001_01_000002 on host slave0 for executor with ID 1 for ResourceProfile Id 0
2025-03-25 12:11:48,669 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-25 12:11:48,669 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742904630293_0001_01_000003 on host slave1 for executor with ID 2 for ResourceProfile Id 0
2025-03-25 12:11:48,670 DEBUG [ContainerLauncher-0] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-25 12:11:48,670 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742904630293_0001_01_000004 on host slave2 for executor with ID 3 for ResourceProfile Id 0
2025-03-25 12:11:48,671 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Received 3 containers from YARN, launching executors on 3 of them.
2025-03-25 12:11:48,671 DEBUG [ContainerLauncher-0] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-25 12:11:48,671 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-25 12:11:48,671 DEBUG [ContainerLauncher-2] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-25 12:11:48,672 DEBUG [ContainerLauncher-2] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-25 12:11:48,672 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-25 12:11:48,672 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-25 12:11:48,672 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-25 12:11:48,672 DEBUG [ContainerLauncher-1] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-25 12:11:48,672 DEBUG [ContainerLauncher-1] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-25 12:11:48,673 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-25 12:11:48,673 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-25 12:11:48,673 DEBUG [ContainerLauncher-1] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-25 12:11:48,675 DEBUG [ContainerLauncher-2] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-25 12:11:48,676 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-25 12:11:48,676 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-25 12:11:48,676 DEBUG [ContainerLauncher-0] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-25 12:11:48,683 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave0:34073
2025-03-25 12:11:48,685 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave2:38023
2025-03-25 12:11:48,686 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave1:44559
2025-03-25 12:11:48,694 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.13:38023, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742904630293 } attemptId: 1 } nodeId { host: "slave2" port: 38023 } appSubmitter: "root" keyId: 793575209)
2025-03-25 12:11:48,694 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.12:44559, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742904630293 } attemptId: 1 } nodeId { host: "slave1" port: 44559 } appSubmitter: "root" keyId: 793575209)
2025-03-25 12:11:48,695 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.11:34073, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742904630293 } attemptId: 1 } nodeId { host: "slave0" port: 34073 } appSubmitter: "root" keyId: 793575209)
2025-03-25 12:11:48,696 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742904630293_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@4c887b22]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-25 12:11:48,696 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-25 12:11:48,697 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742904630293_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@5fcc639f]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-25 12:11:48,697 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-25 12:11:48,699 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: getting client out of cache: Client-0c6efa569297424fa748f3835aa0493e
2025-03-25 12:11:48,699 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742904630293_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@7e2127ed]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-25 12:11:48,699 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-25 12:11:48,700 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: getting client out of cache: Client-0c6efa569297424fa748f3835aa0493e
2025-03-25 12:11:48,700 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: getting client out of cache: Client-0c6efa569297424fa748f3835aa0493e
2025-03-25 12:11:48,790 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-25 12:11:48,791 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-25 12:11:48,791 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Connecting to slave2/172.20.1.13:38023
2025-03-25 12:11:48,791 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Connecting to slave0/172.20.1.11:34073
2025-03-25 12:11:48,791 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Setup connection to slave2/172.20.1.13:38023
2025-03-25 12:11:48,791 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Setup connection to slave0/172.20.1.11:34073
2025-03-25 12:11:48,791 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742904630293_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@13b4a1fa]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-25 12:11:48,791 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-25 12:11:48,792 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742904630293_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@4d07f42]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-25 12:11:48,792 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-25 12:11:48,793 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@41a868ec
2025-03-25 12:11:48,793 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.13:38023. Current token is Kind: NMToken, Service: 172.20.1.13:38023, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742904630293 } attemptId: 1 } nodeId { host: "slave2" port: 38023 } appSubmitter: "root" keyId: 793575209)
2025-03-25 12:11:48,794 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-25 12:11:48,794 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-25 12:11:48,794 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEJW42OncMhABEgwKBnNsYXZlMhCHqQIaBHJvb3Qgqf6z+gI=
2025-03-25 12:11:48,794 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-25 12:11:48,794 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-25 12:11:48,796 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEJW42OncMhABEgwKBnNsYXZlMhCHqQIaBHJvb3Qgqf6z+gI=\",realm=\"default\",nonce=\"1ERINzQg7ktAetUaZZ+BZxibYBJ/sZrOoH1GfO87\",nc=00000001,cnonce=\"XPDTiEq7s+rrXbTJB0vl1D8JmsLe7dq2DldhGFqx\",digest-uri=\"/default\",maxbuf=65536,response=2dc6f7c44c679e3b00f262606f54aa69,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-25 12:11:48,798 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@56e2a42
2025-03-25 12:11:48,798 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.11:34073. Current token is Kind: NMToken, Service: 172.20.1.11:34073, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742904630293 } attemptId: 1 } nodeId { host: "slave0" port: 34073 } appSubmitter: "root" keyId: 793575209)
2025-03-25 12:11:48,798 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-25 12:11:48,798 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-25 12:11:48,798 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEJW42OncMhABEgwKBnNsYXZlMBCZigIaBHJvb3Qgqf6z+gI=
2025-03-25 12:11:48,798 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-25 12:11:48,798 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-25 12:11:48,798 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEJW42OncMhABEgwKBnNsYXZlMBCZigIaBHJvb3Qgqf6z+gI=\",realm=\"default\",nonce=\"JqfOLrJYCj/wY2SASCbhN4zEtqbcFn/U8+nwDBXx\",nc=00000001,cnonce=\"Xv+F7p4JTVNGcWOuKdWhqNOnvXqPHHCsGjwMmGiM\",digest-uri=\"/default\",maxbuf=65536,response=b97e9fce813ab06793fbffaf73daa46e,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-25 12:11:48,799 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-25 12:11:48,799 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001 sending #7 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-25 12:11:48,800 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-25 12:11:48,800 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Connecting to slave1/172.20.1.12:44559
2025-03-25 12:11:48,800 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Setup connection to slave1/172.20.1.12:44559
2025-03-25 12:11:48,800 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742904630293_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@7fb3be3b]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-25 12:11:48,800 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-25 12:11:48,801 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001: starting, having connections 5
2025-03-25 12:11:48,804 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001 got value #7
2025-03-25 12:11:48,804 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 14ms
2025-03-25 12:11:48,806 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@7cb17793
2025-03-25 12:11:48,806 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.12:44559. Current token is Kind: NMToken, Service: 172.20.1.12:44559, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742904630293 } attemptId: 1 } nodeId { host: "slave1" port: 44559 } appSubmitter: "root" keyId: 793575209)
2025-03-25 12:11:48,807 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-25 12:11:48,807 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-25 12:11:48,807 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEJW42OncMhABEgwKBnNsYXZlMRCP3AIaBHJvb3Qgqf6z+gI=
2025-03-25 12:11:48,807 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-25 12:11:48,807 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-25 12:11:48,807 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEJW42OncMhABEgwKBnNsYXZlMRCP3AIaBHJvb3Qgqf6z+gI=\",realm=\"default\",nonce=\"AI1lWNCAeYJykcUVnrDsRs3vn4Xq9cWDKmUDKQwf\",nc=00000001,cnonce=\"CFFMNwTVJbhOXeq1uc2AoEi4oPQY4OZLoHyZcJLa\",digest-uri=\"/default\",maxbuf=65536,response=6350660f629fa0683212691c01a47a06,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-25 12:11:48,807 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001: closed
2025-03-25 12:11:48,807 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:38023 from appattempt_1742904630293_0001_000001: stopped, remaining connections 4
2025-03-25 12:11:48,807 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-25 12:11:48,812 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001 sending #6 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-25 12:11:48,813 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001: starting, having connections 4
2025-03-25 12:11:48,816 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-25 12:11:48,817 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001 sending #8 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-25 12:11:48,817 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001: starting, having connections 4
2025-03-25 12:11:48,932 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001 got value #6
2025-03-25 12:11:48,932 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001: closed
2025-03-25 12:11:48,932 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:34073 from appattempt_1742904630293_0001_000001: stopped, remaining connections 3
2025-03-25 12:11:48,933 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 143ms
2025-03-25 12:11:48,934 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001 got value #8
2025-03-25 12:11:48,934 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 135ms
2025-03-25 12:11:48,934 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001: closed
2025-03-25 12:11:48,934 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:44559 from appattempt_1742904630293_0001_000001: stopped, remaining connections 2
2025-03-25 12:11:49,691 INFO [main] org.apache.spark.executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1225@slave2
2025-03-25 12:11:49,698 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for TERM
2025-03-25 12:11:49,698 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for HUP
2025-03-25 12:11:49,699 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for INT
2025-03-25 12:11:50,002 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
2025-03-25 12:11:50,007 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
2025-03-25 12:11:50,008 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
2025-03-25 12:11:50,008 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
2025-03-25 12:11:50,008 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
2025-03-25 12:11:50,009 DEBUG [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
2025-03-25 12:11:50,033 DEBUG [main] org.apache.hadoop.util.Shell: setsid exited with exit code 0
2025-03-25 12:11:50,033 DEBUG [main] org.apache.hadoop.security.SecurityUtil: Setting hadoop.security.token.service.use_ip to true
2025-03-25 12:11:50,037 DEBUG [main] org.apache.hadoop.security.Groups:  Creating new Groups object
2025-03-25 12:11:50,038 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
2025-03-25 12:11:50,038 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2025-03-25 12:11:50,038 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-25 12:11:50,038 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-03-25 12:11:50,039 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Falling back to shell based
2025-03-25 12:11:50,039 DEBUG [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2025-03-25 12:11:50,107 DEBUG [main] org.apache.hadoop.security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2025-03-25 12:11:50,109 DEBUG [main] org.apache.spark.deploy.SparkHadoopUtil: creating UGI for user: root
2025-03-25 12:11:50,112 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Hadoop login
2025-03-25 12:11:50,112 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: hadoop login commit
2025-03-25 12:11:50,113 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using local user: UnixPrincipal: root
2025-03-25 12:11:50,115 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using user: "UnixPrincipal: root" with name: root
2025-03-25 12:11:50,115 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: User entry: "root"
2025-03-25 12:11:50,115 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Reading credentials from location /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000004/container_tokens
2025-03-25 12:11:50,120 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Loaded 1 tokens from /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000004/container_tokens
2025-03-25 12:11:50,120 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: UGI loginUser: root (auth:SIMPLE)
2025-03-25 12:11:50,120 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.spark.deploy.SparkHadoopUtil$$anon$1@59d2400d]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:427)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend$.main(YarnCoarseGrainedExecutorBackend.scala:83)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main(YarnCoarseGrainedExecutorBackend.scala)
2025-03-25 12:11:50,130 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-25 12:11:50,130 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-25 12:11:50,131 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-25 12:11:50,131 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-25 12:11:50,132 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-25 12:11:50,213 DEBUG [main] io.netty.util.internal.logging.InternalLoggerFactory: Using SLF4J as the default logging framework
2025-03-25 12:11:50,218 DEBUG [main] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2025-03-25 12:11:50,218 DEBUG [main] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2025-03-25 12:11:50,225 DEBUG [main] io.netty.channel.MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
2025-03-25 12:11:50,242 DEBUG [main] io.netty.util.internal.PlatformDependent0: -Dio.netty.noUnsafe: false
2025-03-25 12:11:50,242 DEBUG [main] io.netty.util.internal.PlatformDependent0: Java version: 8
2025-03-25 12:11:50,243 DEBUG [main] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
2025-03-25 12:11:50,243 DEBUG [main] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.copyMemory: available
2025-03-25 12:11:50,243 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.Buffer.address: available
2025-03-25 12:11:50,243 DEBUG [main] io.netty.util.internal.PlatformDependent0: direct buffer constructor: available
2025-03-25 12:11:50,244 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.Bits.unaligned: available, true
2025-03-25 12:11:50,244 DEBUG [main] io.netty.util.internal.PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2025-03-25 12:11:50,244 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
2025-03-25 12:11:50,244 DEBUG [main] io.netty.util.internal.PlatformDependent: sun.misc.Unsafe: available
2025-03-25 12:11:50,244 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.tmpdir: /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000004/tmp (java.io.tmpdir)
2025-03-25 12:11:50,244 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
2025-03-25 12:11:50,245 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.maxDirectMemory: 1908932608 bytes
2025-03-25 12:11:50,245 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
2025-03-25 12:11:50,245 DEBUG [main] io.netty.util.internal.CleanerJava6: java.nio.ByteBuffer.cleaner(): available
2025-03-25 12:11:50,245 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.noPreferDirect: false
2025-03-25 12:11:50,246 DEBUG [main] io.netty.channel.nio.NioEventLoop: -Dio.netty.noKeySetOptimization: false
2025-03-25 12:11:50,246 DEBUG [main] io.netty.channel.nio.NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
2025-03-25 12:11:50,250 DEBUG [main] io.netty.util.internal.PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
2025-03-25 12:11:50,261 DEBUG [main] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
2025-03-25 12:11:50,261 DEBUG [main] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
2025-03-25 12:11:50,263 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2025-03-25 12:11:50,295 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave2/172.20.1.13:40591
2025-03-25 12:11:50,317 DEBUG [netty-rpc-connection-0] io.netty.channel.DefaultChannelId: -Dio.netty.processId: 1225 (auto-detected)
2025-03-25 12:11:50,318 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: -Djava.net.preferIPv4Stack: false
2025-03-25 12:11:50,318 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: -Djava.net.preferIPv6Addresses: false
2025-03-25 12:11:50,319 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
2025-03-25 12:11:50,319 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: /proc/sys/net/core/somaxconn: 4096
2025-03-25 12:11:50,320 DEBUG [netty-rpc-connection-0] io.netty.channel.DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:14:01:0d (auto-detected)
2025-03-25 12:11:50,341 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.allocator.type: pooled
2025-03-25 12:11:50,341 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0
2025-03-25 12:11:50,341 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
2025-03-25 12:11:50,364 DEBUG [rpc-client-1-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkAccessible: true
2025-03-25 12:11:50,364 DEBUG [rpc-client-1-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkBounds: true
2025-03-25 12:11:50,365 DEBUG [rpc-client-1-1] io.netty.util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@31491a42
2025-03-25 12:11:50,381 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d] REGISTERED
2025-03-25 12:11:50,381 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d] CONNECT: slave2/172.20.1.13:40591
2025-03-25 12:11:50,389 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] ACTIVE
2025-03-25 12:11:50,392 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Connection to slave2/172.20.1.13:40591 successful, running bootstraps...
2025-03-25 12:11:50,392 INFO [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave2/172.20.1.13:40591 after 77 ms (0 ms spent in bootstraps)
2025-03-25 12:11:50,395 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096
2025-03-25 12:11:50,395 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.ratio: 8
2025-03-25 12:11:50,395 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.chunkSize: 32
2025-03-25 12:11:50,395 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.blocking: false
2025-03-25 12:11:50,397 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.13:34244.
2025-03-25 12:11:50,410 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 168]
2025-03-25 12:11:50,411 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:50,414 DEBUG [rpc-server-4-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkAccessible: true
2025-03-25 12:11:50,414 DEBUG [rpc-server-4-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkBounds: true
2025-03-25 12:11:50,414 DEBUG [rpc-server-4-1] io.netty.util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@1f0aebf5
2025-03-25 12:11:50,423 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] REGISTERED
2025-03-25 12:11:50,424 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] ACTIVE
2025-03-25 12:11:50,427 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096
2025-03-25 12:11:50,427 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.ratio: 8
2025-03-25 12:11:50,427 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.chunkSize: 32
2025-03-25 12:11:50,427 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.blocking: false
2025-03-25 12:11:50,431 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] READ 189B
2025-03-25 12:11:50,443 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] READ COMPLETE
2025-03-25 12:11:50,447 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:50,448 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] FLUSH
2025-03-25 12:11:50,450 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] READ 21B
2025-03-25 12:11:50,452 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,452 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] READ 47B
2025-03-25 12:11:50,463 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,465 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 169]
2025-03-25 12:11:50,465 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:50,465 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] READ 190B
2025-03-25 12:11:50,479 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] READ COMPLETE
2025-03-25 12:11:50,496 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4358]
2025-03-25 12:11:50,496 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] FLUSH
2025-03-25 12:11:50,496 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] READ 1024B
2025-03-25 12:11:50,496 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] READ 3355B
2025-03-25 12:11:50,516 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,520 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 - R:slave2/172.20.1.13:40591] CLOSE
2025-03-25 12:11:50,520 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 - R:/172.20.1.13:34244] READ COMPLETE
2025-03-25 12:11:50,520 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 ! R:slave2/172.20.1.13:40591] INACTIVE
2025-03-25 12:11:50,521 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 ! R:/172.20.1.13:34244] INACTIVE
2025-03-25 12:11:50,521 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0xb6d9825d, L:/172.20.1.13:34244 ! R:slave2/172.20.1.13:40591] UNREGISTERED
2025-03-25 12:11:50,522 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xc1af2f19, L:/172.20.1.13:40591 ! R:/172.20.1.13:34244] UNREGISTERED
2025-03-25 12:11:50,533 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-25 12:11:50,534 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-25 12:11:50,534 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-25 12:11:50,534 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-25 12:11:50,534 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-25 12:11:50,554 DEBUG [main] org.apache.spark.SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
2025-03-25 12:11:50,573 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave2/172.20.1.13:40591
2025-03-25 12:11:50,574 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f] REGISTERED
2025-03-25 12:11:50,574 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f] CONNECT: slave2/172.20.1.13:40591
2025-03-25 12:11:50,574 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Connection to slave2/172.20.1.13:40591 successful, running bootstraps...
2025-03-25 12:11:50,574 INFO [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave2/172.20.1.13:40591 after 1 ms (0 ms spent in bootstraps)
2025-03-25 12:11:50,575 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] ACTIVE
2025-03-25 12:11:50,575 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 162]
2025-03-25 12:11:50,575 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:50,576 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.13:34252.
2025-03-25 12:11:50,577 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] REGISTERED
2025-03-25 12:11:50,577 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] ACTIVE
2025-03-25 12:11:50,577 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 183B
2025-03-25 12:11:50,578 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:50,578 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:50,578 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:50,578 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 21B
2025-03-25 12:11:50,578 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,579 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 47B
2025-03-25 12:11:50,579 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,609 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 164]
2025-03-25 12:11:50,609 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:50,610 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 185B
2025-03-25 12:11:50,611 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:50,611 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:50,611 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:50,611 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:50,611 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,614 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 173]
2025-03-25 12:11:50,614 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:50,614 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 194B
2025-03-25 12:11:50,615 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:50,615 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:50,616 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:50,616 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:50,616 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,633 INFO [main] org.apache.spark.storage.DiskBlockManager: Created local directory at /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/blockmgr-dcbed711-a8d3-4a93-a29d-7362d83669d8
2025-03-25 12:11:50,636 WARN [main] org.apache.spark.util.Utils: Push-based shuffle can only be enabled when the application is submitted to run in YARN mode, with external shuffle service enabled, IO encryption disabled, and relocation of serialized objects supported.
2025-03-25 12:11:50,636 DEBUG [main] org.apache.spark.storage.DiskBlockManager: Adding shutdown hook
2025-03-25 12:11:50,638 DEBUG [main] org.apache.spark.util.ShutdownHookManager: Adding shutdown hook
2025-03-25 12:11:50,661 INFO [main] org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 912.3 MiB
2025-03-25 12:11:50,825 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 169]
2025-03-25 12:11:50,825 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:50,826 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 190B
2025-03-25 12:11:50,826 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:50,827 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:50,827 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:50,827 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:50,828 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,846 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@slave2:40591
2025-03-25 12:11:50,871 DEBUG [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Resource profile id is: 0
2025-03-25 12:11:50,877 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-25 12:11:50,878 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: No custom resources configured for spark.executor.
2025-03-25 12:11:50,878 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-25 12:11:50,879 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 168]
2025-03-25 12:11:50,879 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:50,879 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 189B
2025-03-25 12:11:50,880 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:50,880 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:50,880 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:50,881 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:50,910 DEBUG [rpc-client-3-1] org.apache.spark.util.YarnContainerInfoHelper: Base URL for logs: http://slave2:8042/node/containerlogs/container_1742904630293_0001_01_000004/root
2025-03-25 12:11:50,933 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 1916]
2025-03-25 12:11:50,933 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:50,934 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 21B
2025-03-25 12:11:50,936 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:50,936 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 496B
2025-03-25 12:11:50,936 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 1420B
2025-03-25 12:11:50,936 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,952 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.13:34252) with ID 3,  ResourceProfileId 0
2025-03-25 12:11:50,954 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:50,954 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:50,955 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:50,955 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:50,959 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver
2025-03-25 12:11:50,964 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:50,965 INFO [dispatcher-Executor] org.apache.spark.executor.Executor: Starting executor ID 3 on host slave2
2025-03-25 12:11:50,993 DEBUG [dispatcher-Executor] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 39967
2025-03-25 12:11:50,993 INFO [dispatcher-Executor] org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39967.
2025-03-25 12:11:50,994 INFO [dispatcher-Executor] org.apache.spark.network.netty.NettyBlockTransferService: Server created on slave2:39967
2025-03-25 12:11:50,995 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-03-25 12:11:51,000 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, slave2, 39967, None)
2025-03-25 12:11:51,003 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 1456]
2025-03-25 12:11:51,003 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:51,003 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 1477B
2025-03-25 12:11:51,006 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:51,006 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave2
2025-03-25 12:11:51,007 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave2:39967 with 912.3 MiB RAM, BlockManagerId(3, slave2, 39967, None)
2025-03-25 12:11:51,009 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-25 12:11:51,009 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:51,009 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 100B
2025-03-25 12:11:51,010 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:51,011 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, slave2, 39967, None)
2025-03-25 12:11:51,012 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(3, slave2, 39967, None)
2025-03-25 12:11:51,019 INFO [dispatcher-Executor] org.apache.spark.executor.Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000004/__app__.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000004/spark-examples_2.12-3.3.2.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000004/__app__.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000004/spark-examples_2.12-3.3.2.jar'
2025-03-25 12:11:51,025 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 163]
2025-03-25 12:11:51,026 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:51,026 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 184B
2025-03-25 12:11:51,027 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:51,027 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:51,027 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:51,028 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:51,028 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:51,045 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 177]
2025-03-25 12:11:51,045 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:51,046 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ 190B
2025-03-25 12:11:51,048 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:51,671 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000113501/3000.
2025-03-25 12:11:51,671 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-25 12:11:51,672 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-25 12:11:51,672 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #9 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-25 12:11:51,675 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #9
2025-03-25 12:11:51,675 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 3ms
2025-03-25 12:11:52,234 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.11:46548.
2025-03-25 12:11:52,234 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] REGISTERED
2025-03-25 12:11:52,234 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] ACTIVE
2025-03-25 12:11:52,243 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] READ 189B
2025-03-25 12:11:52,244 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] READ COMPLETE
2025-03-25 12:11:52,244 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,244 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] FLUSH
2025-03-25 12:11:52,254 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] READ 190B
2025-03-25 12:11:52,254 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] READ COMPLETE
2025-03-25 12:11:52,255 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4358]
2025-03-25 12:11:52,255 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] FLUSH
2025-03-25 12:11:52,283 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 - R:/172.20.1.11:46548] READ COMPLETE
2025-03-25 12:11:52,283 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 ! R:/172.20.1.11:46548] INACTIVE
2025-03-25 12:11:52,284 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xe55d3b1a, L:/172.20.1.13:40591 ! R:/172.20.1.11:46548] UNREGISTERED
2025-03-25 12:11:52,287 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.12:42202.
2025-03-25 12:11:52,287 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] REGISTERED
2025-03-25 12:11:52,288 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] ACTIVE
2025-03-25 12:11:52,297 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] READ 189B
2025-03-25 12:11:52,298 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] READ COMPLETE
2025-03-25 12:11:52,298 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,298 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] FLUSH
2025-03-25 12:11:52,306 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] READ 190B
2025-03-25 12:11:52,307 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] READ COMPLETE
2025-03-25 12:11:52,308 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4358]
2025-03-25 12:11:52,308 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] FLUSH
2025-03-25 12:11:52,339 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 - R:/172.20.1.12:42202] READ COMPLETE
2025-03-25 12:11:52,340 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 ! R:/172.20.1.12:42202] INACTIVE
2025-03-25 12:11:52,340 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xb01d917f, L:/172.20.1.13:40591 ! R:/172.20.1.12:42202] UNREGISTERED
2025-03-25 12:11:52,353 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.11:46554.
2025-03-25 12:11:52,354 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] REGISTERED
2025-03-25 12:11:52,354 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] ACTIVE
2025-03-25 12:11:52,354 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 183B
2025-03-25 12:11:52,355 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,355 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,355 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:52,382 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.12:42218.
2025-03-25 12:11:52,383 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 185B
2025-03-25 12:11:52,383 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] REGISTERED
2025-03-25 12:11:52,383 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] ACTIVE
2025-03-25 12:11:52,383 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,383 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,383 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:52,384 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 183B
2025-03-25 12:11:52,384 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:52,384 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,384 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:52,386 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 194B
2025-03-25 12:11:52,387 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,387 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,387 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:52,414 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 185B
2025-03-25 12:11:52,414 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:52,414 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,415 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:52,418 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 194B
2025-03-25 12:11:52,418 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:52,419 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,419 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:52,617 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 190B
2025-03-25 12:11:52,617 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,618 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,618 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:52,663 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 190B
2025-03-25 12:11:52,664 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:52,664 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,665 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:52,673 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 189B
2025-03-25 12:11:52,674 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,674 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,674 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:52,719 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 189B
2025-03-25 12:11:52,720 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:52,720 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,720 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:52,729 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 512B
2025-03-25 12:11:52,729 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 1425B
2025-03-25 12:11:52,730 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,730 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.11:46554) with ID 1,  ResourceProfileId 0
2025-03-25 12:11:52,731 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,731 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:52,776 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 512B
2025-03-25 12:11:52,776 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 1425B
2025-03-25 12:11:52,777 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:52,777 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.12:42218) with ID 2,  ResourceProfileId 0
2025-03-25 12:11:52,778 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,778 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:52,786 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 1477B
2025-03-25 12:11:52,787 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,787 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave0
2025-03-25 12:11:52,787 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave0:39127 with 912.3 MiB RAM, BlockManagerId(1, slave0, 39127, None)
2025-03-25 12:11:52,788 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-25 12:11:52,788 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:52,809 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 184B
2025-03-25 12:11:52,809 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,809 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,809 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:52,833 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ 190B
2025-03-25 12:11:52,834 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:52,846 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 1477B
2025-03-25 12:11:52,847 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:52,847 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave1
2025-03-25 12:11:52,847 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave1:38635 with 912.3 MiB RAM, BlockManagerId(2, slave1, 38635, None)
2025-03-25 12:11:52,848 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-25 12:11:52,848 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:52,869 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 184B
2025-03-25 12:11:52,869 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:52,869 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:52,870 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:52,876 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2025-03-25 12:11:52,877 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
2025-03-25 12:11:52,889 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 190B
2025-03-25 12:11:52,890 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:53,713 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-25 12:11:53,713 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-25 12:11:53,713 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-25 12:11:53,713 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-25 12:11:53,714 INFO [Driver] org.apache.spark.sql.internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.spark.sql.internal.SharedState: Applying other initial session options to HadoopConf: spark.app.name -> DeveloperApiExample
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Starting: Acquiring creator semaphore for file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000001/spark-warehouse
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Acquiring creator semaphore for file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000001/spark-warehouse: duration 0:00.000s
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Starting: Creating FS file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000001/spark-warehouse
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-25 12:11:53,715 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Creating FS file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000001/spark-warehouse: duration 0:00.000s
2025-03-25 12:11:53,716 INFO [Driver] org.apache.spark.sql.internal.SharedState: Warehouse path is 'file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000001/spark-warehouse'.
2025-03-25 12:11:53,725 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:53,726 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol http
2025-03-25 12:11:53,726 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Unknown protocol http, delegating to default implementation
2025-03-25 12:11:53,726 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:53,727 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:53,728 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:53,729 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-25 12:11:53,729 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol jar
2025-03-25 12:11:53,729 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting jar
2025-03-25 12:11:53,729 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.jar.impl
2025-03-25 12:11:53,729 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-25 12:11:53,730 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation
2025-03-25 12:11:53,730 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol file
2025-03-25 12:11:53,730 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-25 12:11:53,730 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-25 12:11:53,730 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-25 12:11:53,730 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-25 12:11:53,730 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Found implementation of file: class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-25 12:11:53,730 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Using handler for protocol file
2025-03-25 12:11:54,194 DEBUG [Driver] org.apache.spark.sql.catalyst.parser.CatalystSqlParser: Parsing command: spark_grouping_id
2025-03-25 12:11:54,675 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000066832/3000.
2025-03-25 12:11:54,675 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-25 12:11:54,675 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-25 12:11:54,676 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #10 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-25 12:11:54,677 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #10
2025-03-25 12:11:54,678 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 2ms
2025-03-25 12:11:54,736 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegression: Input schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}}]}
2025-03-25 12:11:54,739 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegression: Expected output schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{}},{"name":"rawPrediction","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":false,"metadata":{}}]}
2025-03-25 12:11:54,765 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#0
2025-03-25 12:11:54,787 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#7
2025-03-25 12:11:54,787 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#1
2025-03-25 12:11:55,271 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, double, false],input[1, vector, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     double value_0 = i.getDouble(0);
/* 036 */     mutableStateArray_0[0].write(0, value_0);
/* 037 */
/* 038 */     boolean isNull_1 = i.isNullAt(1);
/* 039 */     InternalRow value_1 = isNull_1 ?
/* 040 */     null : (i.getStruct(1, 4));
/* 041 */     if (isNull_1) {
/* 042 */       mutableStateArray_0[0].setNullAt(1);
/* 043 */     } else {
/* 044 */       final InternalRow tmpInput_0 = value_1;
/* 045 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 046 */         mutableStateArray_0[0].write(1, (UnsafeRow) tmpInput_0);
/* 047 */       } else {
/* 048 */         // Remember the current cursor so that we can calculate how many bytes are
/* 049 */         // written later.
/* 050 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 051 */
/* 052 */         mutableStateArray_0[1].resetRowWriter();
/* 053 */
/* 054 */
/* 055 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 056 */
/* 057 */
/* 058 */         if ((tmpInput_0.isNullAt(1))) {
/* 059 */           mutableStateArray_0[1].setNullAt(1);
/* 060 */         } else {
/* 061 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 062 */         }
/* 063 */
/* 064 */
/* 065 */         if ((tmpInput_0.isNullAt(2))) {
/* 066 */           mutableStateArray_0[1].setNullAt(2);
/* 067 */         } else {
/* 068 */           // Remember the current cursor so that we can calculate how many bytes are
/* 069 */           // written later.
/* 070 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 071 */
/* 072 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 073 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 074 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 075 */           } else {
/* 076 */             final int numElements_0 = tmpInput_1.numElements();
/* 077 */             mutableStateArray_1[0].initialize(numElements_0);
/* 078 */
/* 079 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 080 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 081 */             }
/* 082 */           }
/* 083 */
/* 084 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 085 */         }
/* 086 */
/* 087 */
/* 088 */         if ((tmpInput_0.isNullAt(3))) {
/* 089 */           mutableStateArray_0[1].setNullAt(3);
/* 090 */         } else {
/* 091 */           // Remember the current cursor so that we can calculate how many bytes are
/* 092 */           // written later.
/* 093 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 094 */
/* 095 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 096 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 097 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 098 */           } else {
/* 099 */             final int numElements_1 = tmpInput_2.numElements();
/* 100 */             mutableStateArray_1[1].initialize(numElements_1);
/* 101 */
/* 102 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 103 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 104 */             }
/* 105 */           }
/* 106 */
/* 107 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 108 */         }
/* 109 */
/* 110 */
/* 111 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, previousCursor_0);
/* 112 */       }
/* 113 */     }
/* 114 */     return (mutableStateArray_0[0].getRow());
/* 115 */   }
/* 116 */
/* 117 */
/* 118 */ }

2025-03-25 12:11:55,294 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     double value_0 = i.getDouble(0);
/* 036 */     mutableStateArray_0[0].write(0, value_0);
/* 037 */
/* 038 */     boolean isNull_1 = i.isNullAt(1);
/* 039 */     InternalRow value_1 = isNull_1 ?
/* 040 */     null : (i.getStruct(1, 4));
/* 041 */     if (isNull_1) {
/* 042 */       mutableStateArray_0[0].setNullAt(1);
/* 043 */     } else {
/* 044 */       final InternalRow tmpInput_0 = value_1;
/* 045 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 046 */         mutableStateArray_0[0].write(1, (UnsafeRow) tmpInput_0);
/* 047 */       } else {
/* 048 */         // Remember the current cursor so that we can calculate how many bytes are
/* 049 */         // written later.
/* 050 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 051 */
/* 052 */         mutableStateArray_0[1].resetRowWriter();
/* 053 */
/* 054 */
/* 055 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 056 */
/* 057 */
/* 058 */         if ((tmpInput_0.isNullAt(1))) {
/* 059 */           mutableStateArray_0[1].setNullAt(1);
/* 060 */         } else {
/* 061 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 062 */         }
/* 063 */
/* 064 */
/* 065 */         if ((tmpInput_0.isNullAt(2))) {
/* 066 */           mutableStateArray_0[1].setNullAt(2);
/* 067 */         } else {
/* 068 */           // Remember the current cursor so that we can calculate how many bytes are
/* 069 */           // written later.
/* 070 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 071 */
/* 072 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 073 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 074 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 075 */           } else {
/* 076 */             final int numElements_0 = tmpInput_1.numElements();
/* 077 */             mutableStateArray_1[0].initialize(numElements_0);
/* 078 */
/* 079 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 080 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 081 */             }
/* 082 */           }
/* 083 */
/* 084 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 085 */         }
/* 086 */
/* 087 */
/* 088 */         if ((tmpInput_0.isNullAt(3))) {
/* 089 */           mutableStateArray_0[1].setNullAt(3);
/* 090 */         } else {
/* 091 */           // Remember the current cursor so that we can calculate how many bytes are
/* 092 */           // written later.
/* 093 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 094 */
/* 095 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 096 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 097 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 098 */           } else {
/* 099 */             final int numElements_1 = tmpInput_2.numElements();
/* 100 */             mutableStateArray_1[1].initialize(numElements_1);
/* 101 */
/* 102 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 103 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 104 */             }
/* 105 */           }
/* 106 */
/* 107 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 108 */         }
/* 109 */
/* 110 */
/* 111 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, previousCursor_0);
/* 112 */       }
/* 113 */     }
/* 114 */     return (mutableStateArray_0[0].getRow());
/* 115 */   }
/* 116 */
/* 117 */
/* 118 */ }

2025-03-25 12:11:55,451 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 178.732266 ms
2025-03-25 12:11:55,466 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$1
2025-03-25 12:11:55,476 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$1) is now cleaned +++
2025-03-25 12:11:55,486 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1
2025-03-25 12:11:55,493 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
2025-03-25 12:11:55,511 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$extractLabeledPoints$1
2025-03-25 12:11:55,512 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$extractLabeledPoints$1) is now cleaned +++
2025-03-25 12:11:55,520 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$take$2
2025-03-25 12:11:55,523 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$take$2) is now cleaned +++
2025-03-25 12:11:55,587 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5
2025-03-25 12:11:55,589 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
2025-03-25 12:11:55,592 INFO [Driver] org.apache.spark.SparkContext: Starting job: take at DeveloperApiExample.scala:127
2025-03-25 12:11:55,594 DEBUG [Driver] org.apache.spark.scheduler.DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 5 took 0.000692 seconds
2025-03-25 12:11:55,596 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Merging stage rdd profiles: Set()
2025-03-25 12:11:55,606 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Got job 0 (take at DeveloperApiExample.scala:127) with 1 output partitions
2025-03-25 12:11:55,606 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (take at DeveloperApiExample.scala:127)
2025-03-25 12:11:55,606 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2025-03-25 12:11:55,607 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2025-03-25 12:11:55,626 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: submitStage(ResultStage 0 (name=take at DeveloperApiExample.scala:127;jobs=0))
2025-03-25 12:11:55,627 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: missing: List()
2025-03-25 12:11:55,628 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at Predictor.scala:185), which has no missing parents
2025-03-25 12:11:55,628 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: submitMissingTasks(ResultStage 0)
2025-03-25 12:11:55,703 INFO [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.0 KiB, free 2004.6 MiB)
2025-03-25 12:11:55,705 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Put block broadcast_0 locally took 20 ms
2025-03-25 12:11:55,706 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Putting block broadcast_0 without replication took 21 ms
2025-03-25 12:11:55,728 INFO [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 2004.6 MiB)
2025-03-25 12:11:55,729 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, slave2, 41185, None)
2025-03-25 12:11:55,730 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on slave2:41185 (size: 9.4 KiB, free: 2004.6 MiB)
2025-03-25 12:11:55,734 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
2025-03-25 12:11:55,735 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Told master about block broadcast_0_piece0
2025-03-25 12:11:55,735 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Put block broadcast_0_piece0 locally took 9 ms
2025-03-25 12:11:55,735 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Putting block broadcast_0_piece0 without replication took 9 ms
2025-03-25 12:11:55,736 INFO [dag-scheduler-event-loop] org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
2025-03-25 12:11:55,748 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at Predictor.scala:185) (first 15 tasks are for partitions Vector(0))
2025-03-25 12:11:55,748 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Adding task set 0.0 with 1 tasks resource profile 0
2025-03-25 12:11:55,763 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Epoch for TaskSet 0.0: 0
2025-03-25 12:11:55,766 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Adding pending tasks took 2 ms
2025-03-25 12:11:55,768 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2025-03-25 12:11:55,771 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnClusterScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0
2025-03-25 12:11:55,789 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (slave1, executor 2, partition 0, PROCESS_LOCAL, 4702 bytes) taskResourceAssignments Map()
2025-03-25 12:11:55,802 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
2025-03-25 12:11:55,807 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 0 on executor id: 2 hostname: slave1.
2025-03-25 12:11:55,812 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 5149]
2025-03-25 12:11:55,812 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:55,840 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 1642B
2025-03-25 12:11:55,848 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:55,970 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 338B
2025-03-25 12:11:55,974 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:55,978 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 760]
2025-03-25 12:11:55,978 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:55,997 DEBUG [shuffle-server-7-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.12:35278.
2025-03-25 12:11:55,998 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] REGISTERED
2025-03-25 12:11:55,998 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] ACTIVE
2025-03-25 12:11:56,001 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] READ 92B
2025-03-25 12:11:56,009 DEBUG [shuffle-server-7-1] org.apache.spark.storage.BlockManager: Getting local block broadcast_0_piece0 as bytes
2025-03-25 12:11:56,010 DEBUG [shuffle-server-7-1] org.apache.spark.storage.BlockManager: Level for block broadcast_0_piece0 is StorageLevel(disk, memory, 1 replicas)
2025-03-25 12:11:56,013 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 13]
2025-03-25 12:11:56,013 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] FLUSH
2025-03-25 12:11:56,013 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] READ COMPLETE
2025-03-25 12:11:56,018 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] READ 21B
2025-03-25 12:11:56,021 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 9633]
2025-03-25 12:11:56,021 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] FLUSH
2025-03-25 12:11:56,022 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] READ COMPLETE
2025-03-25 12:11:56,038 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 194B
2025-03-25 12:11:56,040 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(2, slave1, 38635, None)
2025-03-25 12:11:56,040 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:56,041 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on slave1:38635 (size: 9.4 KiB, free: 912.3 MiB)
2025-03-25 12:11:56,042 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-25 12:11:56,042 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:57,678 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000069078/3000.
2025-03-25 12:11:57,678 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-25 12:11:57,678 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-25 12:11:57,679 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #11 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-25 12:11:57,680 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #11
2025-03-25 12:11:57,680 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 2ms
2025-03-25 12:11:57,921 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root: closed
2025-03-25 12:11:57,921 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root: stopped, remaining connections 1
2025-03-25 12:11:58,052 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 1024B
2025-03-25 12:11:58,052 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ 2045B
2025-03-25 12:11:58,053 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:58,061 INFO [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2286 ms on slave1 (executor 2) (1/1)
2025-03-25 12:11:58,062 INFO [task-result-getter-0] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-03-25 12:11:58,066 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (take at DeveloperApiExample.scala:127) finished in 2.401 s
2025-03-25 12:11:58,068 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: After removal of stage 0, remaining stages = 0
2025-03-25 12:11:58,075 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-03-25 12:11:58,075 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Killing all running tasks in stage 0: Stage finished
2025-03-25 12:11:58,078 INFO [Driver] org.apache.spark.scheduler.DAGScheduler: Job 0 finished: take at DeveloperApiExample.scala:127, took 2.486044 s
2025-03-25 12:11:58,099 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegressionModel: Input schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}}]}
2025-03-25 12:11:58,110 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegressionModel: Expected output schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{"ml_attr":{"type":"nominal","num_vals":2}}},{"name":"rawPrediction","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":false,"metadata":{"ml_attr":{"num_attrs":2}}}]}
2025-03-25 12:11:58,139 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#14
2025-03-25 12:11:58,156 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'rawPrediction to rawPrediction#19
2025-03-25 12:11:58,178 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#14
2025-03-25 12:11:58,178 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#13
2025-03-25 12:11:58,178 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'prediction to prediction#26
2025-03-25 12:11:58,212 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for newInstance(class org.apache.spark.ml.linalg.VectorUDT).serialize:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private boolean resultIsNull_0;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private org.apache.spark.ml.linalg.Vector[] mutableStateArray_0 = new org.apache.spark.ml.linalg.Vector[1];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */
/* 016 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_1[0], 4);
/* 018 */     mutableStateArray_2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 4);
/* 019 */     mutableStateArray_2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 8);
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public void initialize(int partitionIndex) {
/* 024 */
/* 025 */   }
/* 026 */
/* 027 */   // Scala.Function1 need this
/* 028 */   public java.lang.Object apply(java.lang.Object row) {
/* 029 */     return apply((InternalRow) row);
/* 030 */   }
/* 031 */
/* 032 */   public UnsafeRow apply(InternalRow i) {
/* 033 */     mutableStateArray_1[0].reset();
/* 034 */
/* 035 */
/* 036 */     mutableStateArray_1[0].zeroOutNullBytes();
/* 037 */
/* 038 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 039 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 040 */     boolean isNull_0 = true;
/* 041 */     InternalRow value_0 = null;
/* 042 */     resultIsNull_0 = false;
/* 043 */     if (!resultIsNull_0) {
/* 044 */       boolean isNull_2 = i.isNullAt(0);
/* 045 */       org.apache.spark.ml.linalg.Vector value_2 = isNull_2 ?
/* 046 */       null : ((org.apache.spark.ml.linalg.Vector)i.get(0, null));
/* 047 */       resultIsNull_0 = isNull_2;
/* 048 */       mutableStateArray_0[0] = value_2;
/* 049 */     }
/* 050 */
/* 051 */     isNull_0 = resultIsNull_0;
/* 052 */     if (!isNull_0) {
/* 053 */
/* 054 */       Object funcResult_0 = null;
/* 055 */       funcResult_0 = value_1.serialize(mutableStateArray_0[0]);
/* 056 */
/* 057 */       if (funcResult_0 != null) {
/* 058 */         value_0 = (InternalRow) funcResult_0;
/* 059 */       } else {
/* 060 */         isNull_0 = true;
/* 061 */       }
/* 062 */
/* 063 */
/* 064 */     }
/* 065 */     if (isNull_0) {
/* 066 */       mutableStateArray_1[0].setNullAt(0);
/* 067 */     } else {
/* 068 */       final InternalRow tmpInput_0 = value_0;
/* 069 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 070 */         mutableStateArray_1[0].write(0, (UnsafeRow) tmpInput_0);
/* 071 */       } else {
/* 072 */         // Remember the current cursor so that we can calculate how many bytes are
/* 073 */         // written later.
/* 074 */         final int previousCursor_0 = mutableStateArray_1[0].cursor();
/* 075 */
/* 076 */         mutableStateArray_1[1].resetRowWriter();
/* 077 */
/* 078 */
/* 079 */         mutableStateArray_1[1].write(0, (tmpInput_0.getByte(0)));
/* 080 */
/* 081 */
/* 082 */         if ((tmpInput_0.isNullAt(1))) {
/* 083 */           mutableStateArray_1[1].setNullAt(1);
/* 084 */         } else {
/* 085 */           mutableStateArray_1[1].write(1, (tmpInput_0.getInt(1)));
/* 086 */         }
/* 087 */
/* 088 */
/* 089 */         if ((tmpInput_0.isNullAt(2))) {
/* 090 */           mutableStateArray_1[1].setNullAt(2);
/* 091 */         } else {
/* 092 */           // Remember the current cursor so that we can calculate how many bytes are
/* 093 */           // written later.
/* 094 */           final int previousCursor_1 = mutableStateArray_1[1].cursor();
/* 095 */
/* 096 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 097 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 098 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_1);
/* 099 */           } else {
/* 100 */             final int numElements_0 = tmpInput_1.numElements();
/* 101 */             mutableStateArray_2[0].initialize(numElements_0);
/* 102 */
/* 103 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 104 */               mutableStateArray_2[0].write(index_0, tmpInput_1.getInt(index_0));
/* 105 */             }
/* 106 */           }
/* 107 */
/* 108 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 109 */         }
/* 110 */
/* 111 */
/* 112 */         if ((tmpInput_0.isNullAt(3))) {
/* 113 */           mutableStateArray_1[1].setNullAt(3);
/* 114 */         } else {
/* 115 */           // Remember the current cursor so that we can calculate how many bytes are
/* 116 */           // written later.
/* 117 */           final int previousCursor_2 = mutableStateArray_1[1].cursor();
/* 118 */
/* 119 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 120 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 121 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_2);
/* 122 */           } else {
/* 123 */             final int numElements_1 = tmpInput_2.numElements();
/* 124 */             mutableStateArray_2[1].initialize(numElements_1);
/* 125 */
/* 126 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 127 */               mutableStateArray_2[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 128 */             }
/* 129 */           }
/* 130 */
/* 131 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 132 */         }
/* 133 */
/* 134 */
/* 135 */         mutableStateArray_1[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 136 */       }
/* 137 */     }
/* 138 */     return (mutableStateArray_1[0].getRow());
/* 139 */   }
/* 140 */
/* 141 */
/* 142 */ }

2025-03-25 12:11:58,214 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private boolean resultIsNull_0;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private org.apache.spark.ml.linalg.Vector[] mutableStateArray_0 = new org.apache.spark.ml.linalg.Vector[1];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */
/* 016 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_1[0], 4);
/* 018 */     mutableStateArray_2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 4);
/* 019 */     mutableStateArray_2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 8);
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public void initialize(int partitionIndex) {
/* 024 */
/* 025 */   }
/* 026 */
/* 027 */   // Scala.Function1 need this
/* 028 */   public java.lang.Object apply(java.lang.Object row) {
/* 029 */     return apply((InternalRow) row);
/* 030 */   }
/* 031 */
/* 032 */   public UnsafeRow apply(InternalRow i) {
/* 033 */     mutableStateArray_1[0].reset();
/* 034 */
/* 035 */
/* 036 */     mutableStateArray_1[0].zeroOutNullBytes();
/* 037 */
/* 038 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 039 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 040 */     boolean isNull_0 = true;
/* 041 */     InternalRow value_0 = null;
/* 042 */     resultIsNull_0 = false;
/* 043 */     if (!resultIsNull_0) {
/* 044 */       boolean isNull_2 = i.isNullAt(0);
/* 045 */       org.apache.spark.ml.linalg.Vector value_2 = isNull_2 ?
/* 046 */       null : ((org.apache.spark.ml.linalg.Vector)i.get(0, null));
/* 047 */       resultIsNull_0 = isNull_2;
/* 048 */       mutableStateArray_0[0] = value_2;
/* 049 */     }
/* 050 */
/* 051 */     isNull_0 = resultIsNull_0;
/* 052 */     if (!isNull_0) {
/* 053 */
/* 054 */       Object funcResult_0 = null;
/* 055 */       funcResult_0 = value_1.serialize(mutableStateArray_0[0]);
/* 056 */
/* 057 */       if (funcResult_0 != null) {
/* 058 */         value_0 = (InternalRow) funcResult_0;
/* 059 */       } else {
/* 060 */         isNull_0 = true;
/* 061 */       }
/* 062 */
/* 063 */
/* 064 */     }
/* 065 */     if (isNull_0) {
/* 066 */       mutableStateArray_1[0].setNullAt(0);
/* 067 */     } else {
/* 068 */       final InternalRow tmpInput_0 = value_0;
/* 069 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 070 */         mutableStateArray_1[0].write(0, (UnsafeRow) tmpInput_0);
/* 071 */       } else {
/* 072 */         // Remember the current cursor so that we can calculate how many bytes are
/* 073 */         // written later.
/* 074 */         final int previousCursor_0 = mutableStateArray_1[0].cursor();
/* 075 */
/* 076 */         mutableStateArray_1[1].resetRowWriter();
/* 077 */
/* 078 */
/* 079 */         mutableStateArray_1[1].write(0, (tmpInput_0.getByte(0)));
/* 080 */
/* 081 */
/* 082 */         if ((tmpInput_0.isNullAt(1))) {
/* 083 */           mutableStateArray_1[1].setNullAt(1);
/* 084 */         } else {
/* 085 */           mutableStateArray_1[1].write(1, (tmpInput_0.getInt(1)));
/* 086 */         }
/* 087 */
/* 088 */
/* 089 */         if ((tmpInput_0.isNullAt(2))) {
/* 090 */           mutableStateArray_1[1].setNullAt(2);
/* 091 */         } else {
/* 092 */           // Remember the current cursor so that we can calculate how many bytes are
/* 093 */           // written later.
/* 094 */           final int previousCursor_1 = mutableStateArray_1[1].cursor();
/* 095 */
/* 096 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 097 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 098 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_1);
/* 099 */           } else {
/* 100 */             final int numElements_0 = tmpInput_1.numElements();
/* 101 */             mutableStateArray_2[0].initialize(numElements_0);
/* 102 */
/* 103 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 104 */               mutableStateArray_2[0].write(index_0, tmpInput_1.getInt(index_0));
/* 105 */             }
/* 106 */           }
/* 107 */
/* 108 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 109 */         }
/* 110 */
/* 111 */
/* 112 */         if ((tmpInput_0.isNullAt(3))) {
/* 113 */           mutableStateArray_1[1].setNullAt(3);
/* 114 */         } else {
/* 115 */           // Remember the current cursor so that we can calculate how many bytes are
/* 116 */           // written later.
/* 117 */           final int previousCursor_2 = mutableStateArray_1[1].cursor();
/* 118 */
/* 119 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 120 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 121 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_2);
/* 122 */           } else {
/* 123 */             final int numElements_1 = tmpInput_2.numElements();
/* 124 */             mutableStateArray_2[1].initialize(numElements_1);
/* 125 */
/* 126 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 127 */               mutableStateArray_2[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 128 */             }
/* 129 */           }
/* 130 */
/* 131 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 132 */         }
/* 133 */
/* 134 */
/* 135 */         mutableStateArray_1[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 136 */       }
/* 137 */     }
/* 138 */     return (mutableStateArray_1[0].getRow());
/* 139 */   }
/* 140 */
/* 141 */
/* 142 */ }

2025-03-25 12:11:58,229 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 16.583012 ms
2025-03-25 12:11:58,239 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 026 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 027 */     boolean isNull_0 = true;
/* 028 */     org.apache.spark.ml.linalg.Vector value_0 = null;
/* 029 */     resultIsNull_0 = false;
/* 030 */     if (!resultIsNull_0) {
/* 031 */       boolean isNull_2 = i.isNullAt(0);
/* 032 */       InternalRow value_2 = isNull_2 ?
/* 033 */       null : (i.getStruct(0, 4));
/* 034 */       resultIsNull_0 = isNull_2;
/* 035 */       mutableStateArray_0[0] = value_2;
/* 036 */     }
/* 037 */
/* 038 */     isNull_0 = resultIsNull_0;
/* 039 */     if (!isNull_0) {
/* 040 */
/* 041 */       Object funcResult_0 = null;
/* 042 */       funcResult_0 = value_1.deserialize(mutableStateArray_0[0]);
/* 043 */
/* 044 */       if (funcResult_0 != null) {
/* 045 */         value_0 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 046 */       } else {
/* 047 */         isNull_0 = true;
/* 048 */       }
/* 049 */
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableRow.setNullAt(0);
/* 054 */     } else {
/* 055 */
/* 056 */       mutableRow.update(0, value_0);
/* 057 */     }
/* 058 */
/* 059 */     return mutableRow;
/* 060 */   }
/* 061 */
/* 062 */
/* 063 */ }

2025-03-25 12:11:58,240 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 026 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 027 */     boolean isNull_0 = true;
/* 028 */     org.apache.spark.ml.linalg.Vector value_0 = null;
/* 029 */     resultIsNull_0 = false;
/* 030 */     if (!resultIsNull_0) {
/* 031 */       boolean isNull_2 = i.isNullAt(0);
/* 032 */       InternalRow value_2 = isNull_2 ?
/* 033 */       null : (i.getStruct(0, 4));
/* 034 */       resultIsNull_0 = isNull_2;
/* 035 */       mutableStateArray_0[0] = value_2;
/* 036 */     }
/* 037 */
/* 038 */     isNull_0 = resultIsNull_0;
/* 039 */     if (!isNull_0) {
/* 040 */
/* 041 */       Object funcResult_0 = null;
/* 042 */       funcResult_0 = value_1.deserialize(mutableStateArray_0[0]);
/* 043 */
/* 044 */       if (funcResult_0 != null) {
/* 045 */         value_0 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 046 */       } else {
/* 047 */         isNull_0 = true;
/* 048 */       }
/* 049 */
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableRow.setNullAt(0);
/* 054 */     } else {
/* 055 */
/* 056 */       mutableRow.update(0, value_0);
/* 057 */     }
/* 058 */
/* 059 */     return mutableRow;
/* 060 */   }
/* 061 */
/* 062 */
/* 063 */ }

2025-03-25 12:11:58,246 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 7.212216 ms
2025-03-25 12:11:58,251 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, double, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     double value_0 = i.getDouble(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

2025-03-25 12:11:58,252 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     double value_0 = i.getDouble(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

2025-03-25 12:11:58,258 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 6.866475 ms
2025-03-25 12:11:58,291 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, vector, true],input[1, double, false],input[2, double, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */     writeFields_0_0(i);
/* 035 */     writeFields_0_1(i);
/* 036 */     return (mutableStateArray_0[0].getRow());
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */   private void writeFields_0_1(InternalRow i) {
/* 041 */
/* 042 */     double value_1 = i.getDouble(1);
/* 043 */     mutableStateArray_0[0].write(1, value_1);
/* 044 */
/* 045 */     double value_2 = i.getDouble(2);
/* 046 */     mutableStateArray_0[0].write(2, value_2);
/* 047 */
/* 048 */   }
/* 049 */
/* 050 */
/* 051 */   private void writeFields_0_0(InternalRow i) {
/* 052 */
/* 053 */     boolean isNull_0 = i.isNullAt(0);
/* 054 */     InternalRow value_0 = isNull_0 ?
/* 055 */     null : (i.getStruct(0, 4));
/* 056 */     if (isNull_0) {
/* 057 */       mutableStateArray_0[0].setNullAt(0);
/* 058 */     } else {
/* 059 */       final InternalRow tmpInput_0 = value_0;
/* 060 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 061 */         mutableStateArray_0[0].write(0, (UnsafeRow) tmpInput_0);
/* 062 */       } else {
/* 063 */         // Remember the current cursor so that we can calculate how many bytes are
/* 064 */         // written later.
/* 065 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 066 */
/* 067 */         mutableStateArray_0[1].resetRowWriter();
/* 068 */
/* 069 */
/* 070 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 071 */
/* 072 */
/* 073 */         if ((tmpInput_0.isNullAt(1))) {
/* 074 */           mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 077 */         }
/* 078 */
/* 079 */
/* 080 */         if ((tmpInput_0.isNullAt(2))) {
/* 081 */           mutableStateArray_0[1].setNullAt(2);
/* 082 */         } else {
/* 083 */           // Remember the current cursor so that we can calculate how many bytes are
/* 084 */           // written later.
/* 085 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 086 */
/* 087 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 088 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 089 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 090 */           } else {
/* 091 */             final int numElements_0 = tmpInput_1.numElements();
/* 092 */             mutableStateArray_1[0].initialize(numElements_0);
/* 093 */
/* 094 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 095 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 096 */             }
/* 097 */           }
/* 098 */
/* 099 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 100 */         }
/* 101 */
/* 102 */
/* 103 */         if ((tmpInput_0.isNullAt(3))) {
/* 104 */           mutableStateArray_0[1].setNullAt(3);
/* 105 */         } else {
/* 106 */           // Remember the current cursor so that we can calculate how many bytes are
/* 107 */           // written later.
/* 108 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 109 */
/* 110 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 111 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 112 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 113 */           } else {
/* 114 */             final int numElements_1 = tmpInput_2.numElements();
/* 115 */             mutableStateArray_1[1].initialize(numElements_1);
/* 116 */
/* 117 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 118 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 119 */             }
/* 120 */           }
/* 121 */
/* 122 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 123 */         }
/* 124 */
/* 125 */
/* 126 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 127 */       }
/* 128 */     }
/* 129 */
/* 130 */   }
/* 131 */
/* 132 */ }

2025-03-25 12:11:58,292 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */     writeFields_0_0(i);
/* 035 */     writeFields_0_1(i);
/* 036 */     return (mutableStateArray_0[0].getRow());
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */   private void writeFields_0_1(InternalRow i) {
/* 041 */
/* 042 */     double value_1 = i.getDouble(1);
/* 043 */     mutableStateArray_0[0].write(1, value_1);
/* 044 */
/* 045 */     double value_2 = i.getDouble(2);
/* 046 */     mutableStateArray_0[0].write(2, value_2);
/* 047 */
/* 048 */   }
/* 049 */
/* 050 */
/* 051 */   private void writeFields_0_0(InternalRow i) {
/* 052 */
/* 053 */     boolean isNull_0 = i.isNullAt(0);
/* 054 */     InternalRow value_0 = isNull_0 ?
/* 055 */     null : (i.getStruct(0, 4));
/* 056 */     if (isNull_0) {
/* 057 */       mutableStateArray_0[0].setNullAt(0);
/* 058 */     } else {
/* 059 */       final InternalRow tmpInput_0 = value_0;
/* 060 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 061 */         mutableStateArray_0[0].write(0, (UnsafeRow) tmpInput_0);
/* 062 */       } else {
/* 063 */         // Remember the current cursor so that we can calculate how many bytes are
/* 064 */         // written later.
/* 065 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 066 */
/* 067 */         mutableStateArray_0[1].resetRowWriter();
/* 068 */
/* 069 */
/* 070 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 071 */
/* 072 */
/* 073 */         if ((tmpInput_0.isNullAt(1))) {
/* 074 */           mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 077 */         }
/* 078 */
/* 079 */
/* 080 */         if ((tmpInput_0.isNullAt(2))) {
/* 081 */           mutableStateArray_0[1].setNullAt(2);
/* 082 */         } else {
/* 083 */           // Remember the current cursor so that we can calculate how many bytes are
/* 084 */           // written later.
/* 085 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 086 */
/* 087 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 088 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 089 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 090 */           } else {
/* 091 */             final int numElements_0 = tmpInput_1.numElements();
/* 092 */             mutableStateArray_1[0].initialize(numElements_0);
/* 093 */
/* 094 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 095 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 096 */             }
/* 097 */           }
/* 098 */
/* 099 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 100 */         }
/* 101 */
/* 102 */
/* 103 */         if ((tmpInput_0.isNullAt(3))) {
/* 104 */           mutableStateArray_0[1].setNullAt(3);
/* 105 */         } else {
/* 106 */           // Remember the current cursor so that we can calculate how many bytes are
/* 107 */           // written later.
/* 108 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 109 */
/* 110 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 111 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 112 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 113 */           } else {
/* 114 */             final int numElements_1 = tmpInput_2.numElements();
/* 115 */             mutableStateArray_1[1].initialize(numElements_1);
/* 116 */
/* 117 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 118 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 119 */             }
/* 120 */           }
/* 121 */
/* 122 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 123 */         }
/* 124 */
/* 125 */
/* 126 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 127 */       }
/* 128 */     }
/* 129 */
/* 130 */   }
/* 131 */
/* 132 */ }

2025-03-25 12:11:58,305 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 13.59526 ms
2025-03-25 12:11:58,312 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for createexternalrow(newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, input[1, double, false], input[2, double, false], StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true), StructField(label,DoubleType,false), StructField(prediction,DoubleType,false)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_6 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_6);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[3];
/* 039 */
/* 040 */     final org.apache.spark.ml.linalg.VectorUDT value_2 = false ?
/* 041 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 042 */     boolean isNull_1 = true;
/* 043 */     org.apache.spark.ml.linalg.Vector value_1 = null;
/* 044 */     resultIsNull_0 = false;
/* 045 */     if (!resultIsNull_0) {
/* 046 */       boolean isNull_3 = i.isNullAt(0);
/* 047 */       InternalRow value_3 = isNull_3 ?
/* 048 */       null : (i.getStruct(0, 4));
/* 049 */       resultIsNull_0 = isNull_3;
/* 050 */       mutableStateArray_0[0] = value_3;
/* 051 */     }
/* 052 */
/* 053 */     isNull_1 = resultIsNull_0;
/* 054 */     if (!isNull_1) {
/* 055 */
/* 056 */       Object funcResult_0 = null;
/* 057 */       funcResult_0 = value_2.deserialize(mutableStateArray_0[0]);
/* 058 */
/* 059 */       if (funcResult_0 != null) {
/* 060 */         value_1 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 061 */       } else {
/* 062 */         isNull_1 = true;
/* 063 */       }
/* 064 */
/* 065 */
/* 066 */     }
/* 067 */     if (isNull_1) {
/* 068 */       values_0[0] = null;
/* 069 */     } else {
/* 070 */       values_0[0] = value_1;
/* 071 */     }
/* 072 */
/* 073 */     double value_4 = i.getDouble(1);
/* 074 */     if (false) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_4;
/* 078 */     }
/* 079 */
/* 080 */     double value_5 = i.getDouble(2);
/* 081 */     if (false) {
/* 082 */       values_0[2] = null;
/* 083 */     } else {
/* 084 */       values_0[2] = value_5;
/* 085 */     }
/* 086 */
/* 087 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 088 */
/* 089 */     return value_0;
/* 090 */   }
/* 091 */
/* 092 */ }

2025-03-25 12:11:58,313 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_6 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_6);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[3];
/* 039 */
/* 040 */     final org.apache.spark.ml.linalg.VectorUDT value_2 = false ?
/* 041 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 042 */     boolean isNull_1 = true;
/* 043 */     org.apache.spark.ml.linalg.Vector value_1 = null;
/* 044 */     resultIsNull_0 = false;
/* 045 */     if (!resultIsNull_0) {
/* 046 */       boolean isNull_3 = i.isNullAt(0);
/* 047 */       InternalRow value_3 = isNull_3 ?
/* 048 */       null : (i.getStruct(0, 4));
/* 049 */       resultIsNull_0 = isNull_3;
/* 050 */       mutableStateArray_0[0] = value_3;
/* 051 */     }
/* 052 */
/* 053 */     isNull_1 = resultIsNull_0;
/* 054 */     if (!isNull_1) {
/* 055 */
/* 056 */       Object funcResult_0 = null;
/* 057 */       funcResult_0 = value_2.deserialize(mutableStateArray_0[0]);
/* 058 */
/* 059 */       if (funcResult_0 != null) {
/* 060 */         value_1 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 061 */       } else {
/* 062 */         isNull_1 = true;
/* 063 */       }
/* 064 */
/* 065 */
/* 066 */     }
/* 067 */     if (isNull_1) {
/* 068 */       values_0[0] = null;
/* 069 */     } else {
/* 070 */       values_0[0] = value_1;
/* 071 */     }
/* 072 */
/* 073 */     double value_4 = i.getDouble(1);
/* 074 */     if (false) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_4;
/* 078 */     }
/* 079 */
/* 080 */     double value_5 = i.getDouble(2);
/* 081 */     if (false) {
/* 082 */       values_0[2] = null;
/* 083 */     } else {
/* 084 */       values_0[2] = value_5;
/* 085 */     }
/* 086 */
/* 087 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 088 */
/* 089 */     return value_0;
/* 090 */   }
/* 091 */
/* 092 */ }

2025-03-25 12:11:58,321 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 8.9217 ms
2025-03-25 12:11:58,339 INFO [Driver] org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://slave2:36621
2025-03-25 12:11:58,359 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend: Shutting down all executors
2025-03-25 12:11:58,359 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
2025-03-25 12:11:58,361 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-25 12:11:58,361 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] FLUSH
2025-03-25 12:11:58,361 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-25 12:11:58,361 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] FLUSH
2025-03-25 12:11:58,361 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-25 12:11:58,361 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] FLUSH
2025-03-25 12:11:58,361 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ 135B
2025-03-25 12:11:58,364 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:58,366 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Driver commanded a shutdown
2025-03-25 12:11:58,370 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 - R:/172.20.1.12:35278] READ COMPLETE
2025-03-25 12:11:58,370 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 ! R:/172.20.1.12:35278] INACTIVE
2025-03-25 12:11:58,370 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x09f045d0, L:/172.20.1.13:41185 ! R:/172.20.1.12:35278] UNREGISTERED
2025-03-25 12:11:58,371 WARN [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.util.Utils: Push-based shuffle can only be enabled when the application is submitted to run in YARN mode, with external shuffle service enabled, IO encryption disabled, and relocation of serialized objects supported.
2025-03-25 12:11:58,379 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 - R:/172.20.1.12:42218] READ COMPLETE
2025-03-25 12:11:58,379 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 ! R:/172.20.1.12:42218] INACTIVE
2025-03-25 12:11:58,379 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xc3b45770, L:/172.20.1.13:40591 ! R:/172.20.1.12:42218] UNREGISTERED
2025-03-25 12:11:58,380 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 - R:/172.20.1.11:46554] READ COMPLETE
2025-03-25 12:11:58,380 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 ! R:/172.20.1.11:46554] INACTIVE
2025-03-25 12:11:58,380 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x1ad3766a, L:/172.20.1.13:40591 ! R:/172.20.1.11:46554] UNREGISTERED
2025-03-25 12:11:58,391 INFO [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-03-25 12:11:58,394 INFO [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2025-03-25 12:11:58,395 INFO [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.storage.BlockManager: BlockManager stopped
2025-03-25 12:11:58,399 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 - R:slave2/172.20.1.13:40591] CLOSE
2025-03-25 12:11:58,399 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 ! R:slave2/172.20.1.13:40591] INACTIVE
2025-03-25 12:11:58,399 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xd316ce7f, L:/172.20.1.13:34252 ! R:slave2/172.20.1.13:40591] UNREGISTERED
2025-03-25 12:11:58,400 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 - R:/172.20.1.13:34252] READ COMPLETE
2025-03-25 12:11:58,400 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 ! R:/172.20.1.13:34252] INACTIVE
2025-03-25 12:11:58,400 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2025-03-25 12:11:58,400 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xad8d8592, L:/172.20.1.13:40591 ! R:/172.20.1.13:34252] UNREGISTERED
2025-03-25 12:11:58,401 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: Completed shutdown in 0.001 seconds; Timeouts: 0
2025-03-25 12:11:58,402 WARN [rpc-server-4-2] io.netty.channel.nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@7899d9d9.
2025-03-25 12:11:58,402 INFO [rpc-server-4-2] io.netty.channel.nio.NioEventLoop: Migrated 0 channel(s) to the new Selector.
2025-03-25 12:11:58,410 INFO [Driver] org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2025-03-25 12:11:58,410 INFO [Driver] org.apache.spark.storage.BlockManager: BlockManager stopped
2025-03-25 12:11:58,416 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2025-03-25 12:11:58,418 INFO [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-03-25 12:11:58,425 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: ShutdownHookManager completed shutdown.
2025-03-25 12:11:58,428 INFO [Driver] org.apache.spark.SparkContext: Successfully stopped SparkContext
2025-03-25 12:11:58,429 INFO [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
2025-03-25 12:11:58,433 DEBUG [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: shutting down reporter thread
2025-03-25 12:11:58,434 DEBUG [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: Done running user class
2025-03-25 12:11:58,438 INFO [shutdown-hook-0] org.apache.spark.deploy.yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
2025-03-25 12:11:58,441 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #12 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.finishApplicationMaster
2025-03-25 12:11:58,452 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #12
2025-03-25 12:11:58,452 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: finishApplicationMaster took 12ms
2025-03-25 12:11:58,454 INFO [shutdown-hook-0] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
2025-03-25 12:11:58,555 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #13 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.finishApplicationMaster
2025-03-25 12:11:58,557 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #13
2025-03-25 12:11:58,557 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: finishApplicationMaster took 3ms
2025-03-25 12:11:58,557 DEBUG [shutdown-hook-0] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl entered state STOPPED
2025-03-25 12:11:58,557 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: stopping client from cache: Client-0c6efa569297424fa748f3835aa0493e
2025-03-25 12:11:58,558 INFO [shutdown-hook-0] org.apache.spark.deploy.yarn.ApplicationMaster: Deleting staging directory hdfs://master:9000/user/root/.sparkStaging/application_1742904630293_0001
2025-03-25 12:11:58,560 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-25 12:11:58,560 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.10:9000
2025-03-25 12:11:58,560 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.10:9000
2025-03-25 12:11:58,560 DEBUG [shutdown-hook-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@6a902ab6]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy36.delete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:655)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy37.delete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1662)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:992)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:989)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:999)
	at org.apache.spark.deploy.yarn.ApplicationMaster.cleanupStagingDir(ApplicationMaster.scala:686)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$run$2(ApplicationMaster.scala:265)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-25 12:11:58,561 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-25 12:11:58,562 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
2025-03-25 12:11:58,563 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one
2025-03-25 12:11:58,563 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Use SIMPLE authentication for protocol ClientNamenodeProtocolPB
2025-03-25 12:11:58,563 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
auths {
  method: "SIMPLE"
  mechanism: ""
}

2025-03-25 12:11:58,571 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root sending #14 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete
2025-03-25 12:11:58,572 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root: starting, having connections 2
2025-03-25 12:11:58,588 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root got value #14
2025-03-25 12:11:58,588 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: delete took 29ms
2025-03-25 12:11:58,590 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2025-03-25 12:11:58,590 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Deleting directory /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/spark-88e5a414-db5b-4626-95f7-e4d59fdb14b5
2025-03-25 12:11:58,593 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.fs.FilterFileSystem.close(FilterFileSystem.java:529)); Key: (root (auth:SIMPLE))@file://; URI: file:///; Object Identity Hash: 733bf09a
2025-03-25 12:11:58,593 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.fs.RawLocalFileSystem.close(RawLocalFileSystem.java:759)); Key: null; URI: file:///; Object Identity Hash: 7fdb8799
2025-03-25 12:11:58,593 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1518)); Key: (root (auth:SIMPLE))@hdfs://master:9000; URI: hdfs://master:9000; Object Identity Hash: 5bf3764c
2025-03-25 12:11:58,593 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: stopping client from cache: Client-0c6efa569297424fa748f3835aa0493e
2025-03-25 12:11:58,593 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: Completed shutdown in 0.158 seconds; Timeouts: 0
2025-03-25 12:11:58,608 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: ShutdownHookManager completed shutdown.
