program=ml.DeveloperApiExample
SPARKLORD_MODE=CONFIG_INJECTION
cpu_cores=2
cpu_util=200
memory=4g
config_file_name=spark-defaults.conf
config_key=spark.shuffle.push.enabled
config_value=True

2025-03-25 12:11:51,585 INFO [main] org.apache.spark.executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1505@slave1
2025-03-25 12:11:51,595 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for TERM
2025-03-25 12:11:51,595 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for HUP
2025-03-25 12:11:51,595 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for INT
2025-03-25 12:11:51,909 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
2025-03-25 12:11:51,915 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
2025-03-25 12:11:51,915 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
2025-03-25 12:11:51,916 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
2025-03-25 12:11:51,916 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
2025-03-25 12:11:51,917 DEBUG [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
2025-03-25 12:11:51,944 DEBUG [main] org.apache.hadoop.util.Shell: setsid exited with exit code 0
2025-03-25 12:11:51,944 DEBUG [main] org.apache.hadoop.security.SecurityUtil: Setting hadoop.security.token.service.use_ip to true
2025-03-25 12:11:51,947 DEBUG [main] org.apache.hadoop.security.Groups:  Creating new Groups object
2025-03-25 12:11:51,948 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
2025-03-25 12:11:51,948 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2025-03-25 12:11:51,948 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-25 12:11:51,948 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-03-25 12:11:51,948 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Falling back to shell based
2025-03-25 12:11:51,949 DEBUG [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2025-03-25 12:11:52,006 DEBUG [main] org.apache.hadoop.security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2025-03-25 12:11:52,009 DEBUG [main] org.apache.spark.deploy.SparkHadoopUtil: creating UGI for user: root
2025-03-25 12:11:52,012 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Hadoop login
2025-03-25 12:11:52,012 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: hadoop login commit
2025-03-25 12:11:52,014 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using local user: UnixPrincipal: root
2025-03-25 12:11:52,016 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using user: "UnixPrincipal: root" with name: root
2025-03-25 12:11:52,016 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: User entry: "root"
2025-03-25 12:11:52,016 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Reading credentials from location /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000003/container_tokens
2025-03-25 12:11:52,022 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Loaded 1 tokens from /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000003/container_tokens
2025-03-25 12:11:52,023 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: UGI loginUser: root (auth:SIMPLE)
2025-03-25 12:11:52,024 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.spark.deploy.SparkHadoopUtil$$anon$1@59d2400d]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:427)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend$.main(YarnCoarseGrainedExecutorBackend.scala:83)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main(YarnCoarseGrainedExecutorBackend.scala)
2025-03-25 12:11:52,040 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-25 12:11:52,041 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-25 12:11:52,041 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-25 12:11:52,041 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-25 12:11:52,042 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-25 12:11:52,118 DEBUG [main] io.netty.util.internal.logging.InternalLoggerFactory: Using SLF4J as the default logging framework
2025-03-25 12:11:52,121 DEBUG [main] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2025-03-25 12:11:52,121 DEBUG [main] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2025-03-25 12:11:52,130 DEBUG [main] io.netty.channel.MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
2025-03-25 12:11:52,150 DEBUG [main] io.netty.util.internal.PlatformDependent0: -Dio.netty.noUnsafe: false
2025-03-25 12:11:52,150 DEBUG [main] io.netty.util.internal.PlatformDependent0: Java version: 8
2025-03-25 12:11:52,150 DEBUG [main] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
2025-03-25 12:11:52,151 DEBUG [main] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.copyMemory: available
2025-03-25 12:11:52,151 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.Buffer.address: available
2025-03-25 12:11:52,151 DEBUG [main] io.netty.util.internal.PlatformDependent0: direct buffer constructor: available
2025-03-25 12:11:52,152 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.Bits.unaligned: available, true
2025-03-25 12:11:52,152 DEBUG [main] io.netty.util.internal.PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2025-03-25 12:11:52,152 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
2025-03-25 12:11:52,152 DEBUG [main] io.netty.util.internal.PlatformDependent: sun.misc.Unsafe: available
2025-03-25 12:11:52,152 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.tmpdir: /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000003/tmp (java.io.tmpdir)
2025-03-25 12:11:52,152 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
2025-03-25 12:11:52,152 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.maxDirectMemory: 1908932608 bytes
2025-03-25 12:11:52,152 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
2025-03-25 12:11:52,153 DEBUG [main] io.netty.util.internal.CleanerJava6: java.nio.ByteBuffer.cleaner(): available
2025-03-25 12:11:52,153 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.noPreferDirect: false
2025-03-25 12:11:52,154 DEBUG [main] io.netty.channel.nio.NioEventLoop: -Dio.netty.noKeySetOptimization: false
2025-03-25 12:11:52,154 DEBUG [main] io.netty.channel.nio.NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
2025-03-25 12:11:52,157 DEBUG [main] io.netty.util.internal.PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
2025-03-25 12:11:52,166 DEBUG [main] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
2025-03-25 12:11:52,166 DEBUG [main] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
2025-03-25 12:11:52,168 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
2025-03-25 12:11:52,168 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
2025-03-25 12:11:52,168 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
2025-03-25 12:11:52,168 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
2025-03-25 12:11:52,168 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
2025-03-25 12:11:52,168 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
2025-03-25 12:11:52,168 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
2025-03-25 12:11:52,168 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2025-03-25 12:11:52,169 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
2025-03-25 12:11:52,169 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2025-03-25 12:11:52,169 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
2025-03-25 12:11:52,169 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2025-03-25 12:11:52,201 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave2/172.20.1.13:40591
2025-03-25 12:11:52,212 DEBUG [netty-rpc-connection-0] io.netty.channel.DefaultChannelId: -Dio.netty.processId: 1505 (auto-detected)
2025-03-25 12:11:52,219 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: -Djava.net.preferIPv4Stack: false
2025-03-25 12:11:52,219 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: -Djava.net.preferIPv6Addresses: false
2025-03-25 12:11:52,221 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
2025-03-25 12:11:52,221 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: /proc/sys/net/core/somaxconn: 4096
2025-03-25 12:11:52,222 DEBUG [netty-rpc-connection-0] io.netty.channel.DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:14:01:0c (auto-detected)
2025-03-25 12:11:52,244 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.allocator.type: pooled
2025-03-25 12:11:52,244 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0
2025-03-25 12:11:52,244 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
2025-03-25 12:11:52,270 DEBUG [rpc-client-1-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkAccessible: true
2025-03-25 12:11:52,270 DEBUG [rpc-client-1-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkBounds: true
2025-03-25 12:11:52,271 DEBUG [rpc-client-1-1] io.netty.util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@ebc48ad
2025-03-25 12:11:52,285 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3] REGISTERED
2025-03-25 12:11:52,286 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3] CONNECT: slave2/172.20.1.13:40591
2025-03-25 12:11:52,288 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Connection to slave2/172.20.1.13:40591 successful, running bootstraps...
2025-03-25 12:11:52,288 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] ACTIVE
2025-03-25 12:11:52,288 INFO [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave2/172.20.1.13:40591 after 79 ms (0 ms spent in bootstraps)
2025-03-25 12:11:52,291 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096
2025-03-25 12:11:52,291 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.ratio: 8
2025-03-25 12:11:52,291 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.chunkSize: 32
2025-03-25 12:11:52,291 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.blocking: false
2025-03-25 12:11:52,297 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 168]
2025-03-25 12:11:52,297 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,299 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:52,305 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,306 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 169]
2025-03-25 12:11:52,306 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,309 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] READ 2048B
2025-03-25 12:11:52,311 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] READ 2331B
2025-03-25 12:11:52,336 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,339 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 - R:slave2/172.20.1.13:40591] CLOSE
2025-03-25 12:11:52,340 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 ! R:slave2/172.20.1.13:40591] INACTIVE
2025-03-25 12:11:52,340 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x9b5020c3, L:/172.20.1.12:42202 ! R:slave2/172.20.1.13:40591] UNREGISTERED
2025-03-25 12:11:52,348 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-25 12:11:52,348 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-25 12:11:52,348 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-25 12:11:52,348 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-25 12:11:52,348 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-25 12:11:52,366 DEBUG [main] org.apache.spark.SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
2025-03-25 12:11:52,380 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave2/172.20.1.13:40591
2025-03-25 12:11:52,382 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c] REGISTERED
2025-03-25 12:11:52,382 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c] CONNECT: slave2/172.20.1.13:40591
2025-03-25 12:11:52,382 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] ACTIVE
2025-03-25 12:11:52,382 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Connection to slave2/172.20.1.13:40591 successful, running bootstraps...
2025-03-25 12:11:52,382 INFO [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave2/172.20.1.13:40591 after 1 ms (0 ms spent in bootstraps)
2025-03-25 12:11:52,382 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 162]
2025-03-25 12:11:52,382 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,385 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:52,385 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,413 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 164]
2025-03-25 12:11:52,413 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,415 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:52,415 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,417 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 173]
2025-03-25 12:11:52,418 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,419 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:52,419 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,434 INFO [main] org.apache.spark.storage.DiskBlockManager: Created local directory at /data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/blockmgr-3337c27e-3061-4646-b0ed-d0ec646a2aec
2025-03-25 12:11:52,436 WARN [main] org.apache.spark.util.Utils: Push-based shuffle can only be enabled when the application is submitted to run in YARN mode, with external shuffle service enabled, IO encryption disabled, and relocation of serialized objects supported.
2025-03-25 12:11:52,436 DEBUG [main] org.apache.spark.storage.DiskBlockManager: Adding shutdown hook
2025-03-25 12:11:52,438 DEBUG [main] org.apache.spark.util.ShutdownHookManager: Adding shutdown hook
2025-03-25 12:11:52,459 INFO [main] org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 912.3 MiB
2025-03-25 12:11:52,663 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 169]
2025-03-25 12:11:52,663 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,665 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:52,665 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,684 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@slave2:40591
2025-03-25 12:11:52,712 DEBUG [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Resource profile id is: 0
2025-03-25 12:11:52,718 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-25 12:11:52,718 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: No custom resources configured for spark.executor.
2025-03-25 12:11:52,718 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-25 12:11:52,719 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 168]
2025-03-25 12:11:52,719 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,720 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:52,750 DEBUG [rpc-client-3-1] org.apache.spark.util.YarnContainerInfoHelper: Base URL for logs: http://slave1:8042/node/containerlogs/container_1742904630293_0001_01_000003/root
2025-03-25 12:11:52,775 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 1916]
2025-03-25 12:11:52,775 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,776 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,778 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:52,781 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver
2025-03-25 12:11:52,787 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,789 INFO [dispatcher-Executor] org.apache.spark.executor.Executor: Starting executor ID 2 on host slave1
2025-03-25 12:11:52,829 DEBUG [dispatcher-Executor] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 38635
2025-03-25 12:11:52,830 INFO [dispatcher-Executor] org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38635.
2025-03-25 12:11:52,830 INFO [dispatcher-Executor] org.apache.spark.network.netty.NettyBlockTransferService: Server created on slave1:38635
2025-03-25 12:11:52,831 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-03-25 12:11:52,838 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, slave1, 38635, None)
2025-03-25 12:11:52,845 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 1456]
2025-03-25 12:11:52,845 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,848 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 100B
2025-03-25 12:11:52,849 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,850 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, slave1, 38635, None)
2025-03-25 12:11:52,852 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(2, slave1, 38635, None)
2025-03-25 12:11:52,862 INFO [dispatcher-Executor] org.apache.spark.executor.Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000003/__app__.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000003/spark-examples_2.12-3.3.2.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000003/__app__.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742904630293_0001/container_1742904630293_0001_01_000003/spark-examples_2.12-3.3.2.jar'
2025-03-25 12:11:52,868 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 163]
2025-03-25 12:11:52,868 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:52,870 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:52,871 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:52,888 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 177]
2025-03-25 12:11:52,888 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:55,812 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 480B
2025-03-25 12:11:55,812 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 2048B
2025-03-25 12:11:55,812 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 2634B
2025-03-25 12:11:55,816 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:55,820 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0
2025-03-25 12:11:55,828 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2025-03-25 12:11:55,840 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 1629]
2025-03-25 12:11:55,840 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:55,897 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: task 0.0 in stage 0.0 (TID 0)'s epoch is 0
2025-03-25 12:11:55,901 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1
2025-03-25 12:11:55,958 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting local block broadcast_0
2025-03-25 12:11:55,959 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Block broadcast_0 was not found
2025-03-25 12:11:55,960 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-03-25 12:11:55,966 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Reading piece broadcast_0_piece0 of broadcast_0
2025-03-25 12:11:55,966 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting local block broadcast_0_piece0 as bytes
2025-03-25 12:11:55,968 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting remote block broadcast_0_piece0
2025-03-25 12:11:55,969 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 317]
2025-03-25 12:11:55,970 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:55,979 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 781B
2025-03-25 12:11:55,985 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:55,989 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting remote block broadcast_0_piece0 from BlockManagerId(driver, slave2, 41185, None)
2025-03-25 12:11:55,995 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave2/172.20.1.13:41185
2025-03-25 12:11:55,996 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684] REGISTERED
2025-03-25 12:11:55,996 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684] CONNECT: slave2/172.20.1.13:41185
2025-03-25 12:11:55,997 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Connection to slave2/172.20.1.13:41185 successful, running bootstraps...
2025-03-25 12:11:55,997 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave2/172.20.1.13:41185 after 2 ms (0 ms spent in bootstraps)
2025-03-25 12:11:55,997 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] ACTIVE
2025-03-25 12:11:56,000 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 71]
2025-03-25 12:11:56,001 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] FLUSH
2025-03-25 12:11:56,013 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] READ 34B
2025-03-25 12:11:56,016 DEBUG [shuffle-client-4-1] org.apache.spark.network.client.TransportClient: Sending fetch chunk request 0 to slave2/172.20.1.13:41185
2025-03-25 12:11:56,017 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] WRITE 21B
2025-03-25 12:11:56,017 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] FLUSH
2025-03-25 12:11:56,017 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] READ COMPLETE
2025-03-25 12:11:56,021 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] READ 2048B
2025-03-25 12:11:56,021 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] READ 7606B
2025-03-25 12:11:56,023 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] READ COMPLETE
2025-03-25 12:11:56,035 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 912.3 MiB)
2025-03-25 12:11:56,038 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 173]
2025-03-25 12:11:56,038 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:56,042 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 68B
2025-03-25 12:11:56,043 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:56,043 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
2025-03-25 12:11:56,045 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Told master about block broadcast_0_piece0
2025-03-25 12:11:56,047 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Put block broadcast_0_piece0 locally took 14 ms
2025-03-25 12:11:56,048 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Putting block broadcast_0_piece0 without replication took 15 ms
2025-03-25 12:11:56,049 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Reading broadcast variable 0 took 88 ms
2025-03-25 12:11:56,094 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.0 KiB, free 912.3 MiB)
2025-03-25 12:11:56,095 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Put block broadcast_0 locally took 30 ms
2025-03-25 12:11:56,095 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Putting block broadcast_0 without replication took 31 ms
2025-03-25 12:11:57,819 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for createexternalrow(input[0, double, false], newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, StructField(label,DoubleType,false), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_5 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_5);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[2];
/* 039 */
/* 040 */     double value_1 = i.getDouble(0);
/* 041 */     if (false) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.ml.linalg.VectorUDT value_3 = false ?
/* 048 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 049 */     boolean isNull_2 = true;
/* 050 */     org.apache.spark.ml.linalg.Vector value_2 = null;
/* 051 */     resultIsNull_0 = false;
/* 052 */     if (!resultIsNull_0) {
/* 053 */       boolean isNull_4 = i.isNullAt(1);
/* 054 */       InternalRow value_4 = isNull_4 ?
/* 055 */       null : (i.getStruct(1, 4));
/* 056 */       resultIsNull_0 = isNull_4;
/* 057 */       mutableStateArray_0[0] = value_4;
/* 058 */     }
/* 059 */
/* 060 */     isNull_2 = resultIsNull_0;
/* 061 */     if (!isNull_2) {
/* 062 */
/* 063 */       Object funcResult_0 = null;
/* 064 */       funcResult_0 = value_3.deserialize(mutableStateArray_0[0]);
/* 065 */
/* 066 */       if (funcResult_0 != null) {
/* 067 */         value_2 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 068 */       } else {
/* 069 */         isNull_2 = true;
/* 070 */       }
/* 071 */
/* 072 */
/* 073 */     }
/* 074 */     if (isNull_2) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_2;
/* 078 */     }
/* 079 */
/* 080 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 081 */
/* 082 */     return value_0;
/* 083 */   }
/* 084 */
/* 085 */ }

2025-03-25 12:11:57,842 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_5 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_5);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[2];
/* 039 */
/* 040 */     double value_1 = i.getDouble(0);
/* 041 */     if (false) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.ml.linalg.VectorUDT value_3 = false ?
/* 048 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 049 */     boolean isNull_2 = true;
/* 050 */     org.apache.spark.ml.linalg.Vector value_2 = null;
/* 051 */     resultIsNull_0 = false;
/* 052 */     if (!resultIsNull_0) {
/* 053 */       boolean isNull_4 = i.isNullAt(1);
/* 054 */       InternalRow value_4 = isNull_4 ?
/* 055 */       null : (i.getStruct(1, 4));
/* 056 */       resultIsNull_0 = isNull_4;
/* 057 */       mutableStateArray_0[0] = value_4;
/* 058 */     }
/* 059 */
/* 060 */     isNull_2 = resultIsNull_0;
/* 061 */     if (!isNull_2) {
/* 062 */
/* 063 */       Object funcResult_0 = null;
/* 064 */       funcResult_0 = value_3.deserialize(mutableStateArray_0[0]);
/* 065 */
/* 066 */       if (funcResult_0 != null) {
/* 067 */         value_2 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 068 */       } else {
/* 069 */         isNull_2 = true;
/* 070 */       }
/* 071 */
/* 072 */
/* 073 */     }
/* 074 */     if (isNull_2) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_2;
/* 078 */     }
/* 079 */
/* 080 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 081 */
/* 082 */     return value_0;
/* 083 */   }
/* 084 */
/* 085 */ }

2025-03-25 12:11:58,034 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 214.446482 ms
2025-03-25 12:11:58,051 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1419 bytes result sent to driver
2025-03-25 12:11:58,052 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 3056]
2025-03-25 12:11:58,052 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] FLUSH
2025-03-25 12:11:58,052 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.ExecutorMetricsPoller: stageTCMP: (0, 0) -> 0
2025-03-25 12:11:58,361 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ 135B
2025-03-25 12:11:58,363 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] READ COMPLETE
2025-03-25 12:11:58,363 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Driver commanded a shutdown
2025-03-25 12:11:58,365 WARN [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.util.Utils: Push-based shuffle can only be enabled when the application is submitted to run in YARN mode, with external shuffle service enabled, IO encryption disabled, and relocation of serialized objects supported.
2025-03-25 12:11:58,369 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 - R:slave2/172.20.1.13:41185] CLOSE
2025-03-25 12:11:58,370 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 ! R:slave2/172.20.1.13:41185] INACTIVE
2025-03-25 12:11:58,370 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa156d684, L:/172.20.1.12:35278 ! R:slave2/172.20.1.13:41185] UNREGISTERED
2025-03-25 12:11:58,377 INFO [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2025-03-25 12:11:58,377 INFO [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.storage.BlockManager: BlockManager stopped
2025-03-25 12:11:58,378 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 - R:slave2/172.20.1.13:40591] CLOSE
2025-03-25 12:11:58,378 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 ! R:slave2/172.20.1.13:40591] INACTIVE
2025-03-25 12:11:58,378 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0xbde9c74c, L:/172.20.1.12:42218 ! R:slave2/172.20.1.13:40591] UNREGISTERED
2025-03-25 12:11:58,381 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2025-03-25 12:11:58,382 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: Completed shutdown in 0.001 seconds; Timeouts: 0
2025-03-25 12:11:58,395 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: ShutdownHookManager completed shutdown.
