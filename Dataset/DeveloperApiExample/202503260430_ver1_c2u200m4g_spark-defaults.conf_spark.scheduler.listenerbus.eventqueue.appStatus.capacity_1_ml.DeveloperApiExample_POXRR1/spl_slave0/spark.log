program=ml.DeveloperApiExample
SPARKLORD_MODE=CONFIG_INJECTION
cpu_cores=2
cpu_util=200
memory=4g
config_file_name=spark-defaults.conf
config_key=spark.scheduler.listenerbus.eventqueue.appStatus.capacity
config_value=1

2025-03-26 04:31:28,464 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for TERM
2025-03-26 04:31:28,470 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for HUP
2025-03-26 04:31:28,470 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for INT
2025-03-26 04:31:28,674 DEBUG [main] org.apache.hadoop.util.Shell: setsid exited with exit code 0
2025-03-26 04:31:28,767 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-26 04:31:28,767 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-26 04:31:28,767 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-26 04:31:28,768 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-26 04:31:28,768 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-26 04:31:28,819 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
2025-03-26 04:31:28,824 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
2025-03-26 04:31:28,824 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
2025-03-26 04:31:28,824 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
2025-03-26 04:31:28,824 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
2025-03-26 04:31:28,825 DEBUG [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
2025-03-26 04:31:28,840 DEBUG [main] org.apache.hadoop.security.SecurityUtil: Setting hadoop.security.token.service.use_ip to true
2025-03-26 04:31:28,843 DEBUG [main] org.apache.hadoop.security.Groups:  Creating new Groups object
2025-03-26 04:31:28,844 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
2025-03-26 04:31:28,844 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2025-03-26 04:31:28,844 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-26 04:31:28,844 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-03-26 04:31:28,845 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Falling back to shell based
2025-03-26 04:31:28,845 DEBUG [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2025-03-26 04:31:28,884 DEBUG [main] org.apache.hadoop.security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2025-03-26 04:31:28,941 DEBUG [main] org.apache.spark.deploy.SparkHadoopUtil: creating UGI for user: root
2025-03-26 04:31:28,943 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Hadoop login
2025-03-26 04:31:28,943 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: hadoop login commit
2025-03-26 04:31:28,944 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using local user: UnixPrincipal: root
2025-03-26 04:31:28,945 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using user: "UnixPrincipal: root" with name: root
2025-03-26 04:31:28,945 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: User entry: "root"
2025-03-26 04:31:28,945 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Reading credentials from location /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000001/container_tokens
2025-03-26 04:31:28,948 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Loaded 1 tokens from /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000001/container_tokens
2025-03-26 04:31:28,948 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: UGI loginUser: root (auth:SIMPLE)
2025-03-26 04:31:28,948 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3@1522d8a0]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-26 04:31:28,953 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1742963439991_0001_000001
2025-03-26 04:31:28,954 DEBUG [main] org.apache.spark.util.ShutdownHookManager: Adding shutdown hook
2025-03-26 04:31:28,974 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Starting the user application in a separate Thread
2025-03-26 04:31:28,975 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Waiting for spark context initialization...
2025-03-26 04:31:29,018 INFO [Driver] org.apache.spark.SparkContext: Running Spark version 3.3.2
2025-03-26 04:31:29,033 INFO [Driver] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-26 04:31:29,034 INFO [Driver] org.apache.spark.resource.ResourceUtils: No custom resources configured for spark.driver.
2025-03-26 04:31:29,034 INFO [Driver] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-26 04:31:29,035 INFO [Driver] org.apache.spark.SparkContext: Submitted application: DeveloperApiExample
2025-03-26 04:31:29,048 INFO [Driver] org.apache.spark.resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-03-26 04:31:29,052 INFO [Driver] org.apache.spark.resource.ResourceProfile: Limiting resource is cpus at 2 tasks per executor
2025-03-26 04:31:29,053 INFO [Driver] org.apache.spark.resource.ResourceProfileManager: Added ResourceProfile id: 0
2025-03-26 04:31:29,081 INFO [Driver] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-26 04:31:29,082 INFO [Driver] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-26 04:31:29,082 INFO [Driver] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-26 04:31:29,082 INFO [Driver] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-26 04:31:29,082 INFO [Driver] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-26 04:31:29,118 DEBUG [Driver] io.netty.util.internal.logging.InternalLoggerFactory: Using SLF4J as the default logging framework
2025-03-26 04:31:29,122 DEBUG [Driver] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2025-03-26 04:31:29,122 DEBUG [Driver] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2025-03-26 04:31:29,129 DEBUG [Driver] io.netty.channel.MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
2025-03-26 04:31:29,147 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: -Dio.netty.noUnsafe: false
2025-03-26 04:31:29,147 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: Java version: 8
2025-03-26 04:31:29,147 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
2025-03-26 04:31:29,147 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.copyMemory: available
2025-03-26 04:31:29,148 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.Buffer.address: available
2025-03-26 04:31:29,148 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: direct buffer constructor: available
2025-03-26 04:31:29,148 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.Bits.unaligned: available, true
2025-03-26 04:31:29,148 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2025-03-26 04:31:29,148 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
2025-03-26 04:31:29,148 DEBUG [Driver] io.netty.util.internal.PlatformDependent: sun.misc.Unsafe: available
2025-03-26 04:31:29,148 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.tmpdir: /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000001/tmp (java.io.tmpdir)
2025-03-26 04:31:29,148 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
2025-03-26 04:31:29,149 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.maxDirectMemory: 3817865216 bytes
2025-03-26 04:31:29,149 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
2025-03-26 04:31:29,149 DEBUG [Driver] io.netty.util.internal.CleanerJava6: java.nio.ByteBuffer.cleaner(): available
2025-03-26 04:31:29,149 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.noPreferDirect: false
2025-03-26 04:31:29,150 DEBUG [Driver] io.netty.channel.nio.NioEventLoop: -Dio.netty.noKeySetOptimization: false
2025-03-26 04:31:29,150 DEBUG [Driver] io.netty.channel.nio.NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
2025-03-26 04:31:29,152 DEBUG [Driver] io.netty.util.internal.PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
2025-03-26 04:31:29,159 DEBUG [Driver] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
2025-03-26 04:31:29,159 DEBUG [Driver] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
2025-03-26 04:31:29,160 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2025-03-26 04:31:29,161 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
2025-03-26 04:31:29,161 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2025-03-26 04:31:29,176 DEBUG [Driver] io.netty.channel.DefaultChannelId: -Dio.netty.processId: 1502 (auto-detected)
2025-03-26 04:31:29,177 DEBUG [Driver] io.netty.util.NetUtil: -Djava.net.preferIPv4Stack: false
2025-03-26 04:31:29,177 DEBUG [Driver] io.netty.util.NetUtil: -Djava.net.preferIPv6Addresses: false
2025-03-26 04:31:29,178 DEBUG [Driver] io.netty.util.NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
2025-03-26 04:31:29,178 DEBUG [Driver] io.netty.util.NetUtil: /proc/sys/net/core/somaxconn: 4096
2025-03-26 04:31:29,179 DEBUG [Driver] io.netty.channel.DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:14:01:0f (auto-detected)
2025-03-26 04:31:29,187 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.allocator.type: pooled
2025-03-26 04:31:29,187 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0
2025-03-26 04:31:29,187 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
2025-03-26 04:31:29,195 DEBUG [Driver] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 36275
2025-03-26 04:31:29,202 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 36275.
2025-03-26 04:31:29,203 DEBUG [Driver] org.apache.spark.SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
2025-03-26 04:31:29,219 INFO [Driver] org.apache.spark.SparkEnv: Registering MapOutputTracker
2025-03-26 04:31:29,219 DEBUG [Driver] org.apache.spark.MapOutputTrackerMasterEndpoint: init
2025-03-26 04:31:29,239 INFO [Driver] org.apache.spark.SparkEnv: Registering BlockManagerMaster
2025-03-26 04:31:29,252 INFO [Driver] org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-03-26 04:31:29,252 INFO [Driver] org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-03-26 04:31:29,287 INFO [Driver] org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
2025-03-26 04:31:29,310 INFO [Driver] org.apache.spark.storage.DiskBlockManager: Created local directory at /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/blockmgr-0fc5c0db-7050-4d74-89e4-a89767e5da80
2025-03-26 04:31:29,312 DEBUG [Driver] org.apache.spark.storage.DiskBlockManager: Adding shutdown hook
2025-03-26 04:31:29,320 INFO [Driver] org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 2004.6 MiB
2025-03-26 04:31:29,352 INFO [Driver] org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2025-03-26 04:31:29,352 DEBUG [Driver] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: init
2025-03-26 04:31:29,358 DEBUG [Driver] org.apache.spark.SecurityManager: Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
2025-03-26 04:31:29,498 DEBUG [Driver] org.apache.spark.ui.JettyUtils: Using requestHeaderSize: 8192
2025-03-26 04:31:29,515 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 43989.
2025-03-26 04:31:29,516 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,579 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Created YarnClusterScheduler
2025-03-26 04:31:29,636 DEBUG [Driver] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 46565
2025-03-26 04:31:29,636 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46565.
2025-03-26 04:31:29,636 INFO [Driver] org.apache.spark.network.netty.NettyBlockTransferService: Server created on slave0:46565
2025-03-26 04:31:29,638 INFO [Driver] org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-03-26 04:31:29,643 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, slave0, 46565, None)
2025-03-26 04:31:29,645 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave0
2025-03-26 04:31:29,646 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave0:46565 with 2004.6 MiB RAM, BlockManagerId(driver, slave0, 46565, None)
2025-03-26 04:31:29,648 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, slave0, 46565, None)
2025-03-26 04:31:29,651 INFO [Driver] org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, slave0, 46565, None)
2025-03-26 04:31:29,749 ERROR [Driver] org.apache.spark.scheduler.AsyncEventQueue: Dropping event from queue appStatus. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
2025-03-26 04:31:29,751 WARN [Driver] org.apache.spark.scheduler.AsyncEventQueue: Dropped 1 events from appStatus since the application started.
2025-03-26 04:31:29,802 DEBUG [Driver] org.apache.spark.util.YarnContainerInfoHelper: Base URL for logs: http://slave0:8042/node/containerlogs/container_1742963439991_0001_01_000001/root
2025-03-26 04:31:29,829 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,830 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,832 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,833 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,833 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,834 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,835 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,836 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,837 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,837 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,837 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,838 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,839 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,839 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,840 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,841 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,842 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,842 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,843 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,844 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,845 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,853 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,854 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,858 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,859 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,864 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:29,866 DEBUG [Driver] org.apache.spark.SparkContext: Adding shutdown hook
2025-03-26 04:31:29,872 DEBUG [main] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl entered state INITED
2025-03-26 04:31:29,879 INFO [main] org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master/172.20.1.14:8030
2025-03-26 04:31:29,879 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.RMProxy$1@12dae582]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.RMProxy.getProxy(RMProxy.java:145)
	at org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider.init(DefaultNoHARMFailoverProxyProvider.java:65)
	at org.apache.hadoop.yarn.client.RMProxy.createNonHaRMFailoverProxyProvider(RMProxy.java:172)
	at org.apache.hadoop.yarn.client.RMProxy.newProxyInstance(RMProxy.java:132)
	at org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:103)
	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:73)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.serviceStart(AMRMClientImpl.java:193)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:63)
	at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:440)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:518)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-26 04:31:29,880 DEBUG [main] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:31:29,880 DEBUG [main] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ApplicationMasterProtocol
2025-03-26 04:31:29,890 DEBUG [main] org.apache.hadoop.ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker@2611b9a3
2025-03-26 04:31:29,893 DEBUG [main] org.apache.hadoop.ipc.Client: getting client out of cache: Client-13d7b2110f204c45b479e64949a5ec25
2025-03-26 04:31:29,907 DEBUG [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl is started
2025-03-26 04:31:29,908 INFO [main] org.apache.spark.deploy.yarn.YarnRMClient: Registering the ApplicationMaster
2025-03-26 04:31:29,931 DEBUG [main] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:31:29,932 DEBUG [main] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.14:8030
2025-03-26 04:31:29,932 DEBUG [main] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.14:8030
2025-03-26 04:31:29,934 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@3ebff828]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy31.registerApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.registerApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:108)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy32.registerApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:247)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:234)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:214)
	at org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:72)
	at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:440)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:518)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-26 04:31:29,984 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:31:29,988 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB info:org.apache.hadoop.yarn.security.SchedulerSecurityInfo$1@2a76b80a
2025-03-26 04:31:29,989 DEBUG [main] org.apache.hadoop.yarn.security.AMRMTokenSelector: Looking for a token with service 172.20.1.14:8030
2025-03-26 04:31:29,989 DEBUG [main] org.apache.hadoop.yarn.security.AMRMTokenSelector: Token kind is YARN_AM_RM_TOKEN and the token's service name is 172.20.1.14:8030
2025-03-26 04:31:29,991 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-26 04:31:29,991 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ApplicationMasterProtocolPB
2025-03-26 04:31:29,992 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEPfy3YXdMhABEOeXvZcH
2025-03-26 04:31:29,992 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-26 04:31:29,993 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-26 04:31:29,994 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEPfy3YXdMhABEOeXvZcH\",realm=\"default\",nonce=\"vAs4RQpZ6hhJ/u7fr4yIPABTKmpNoZC0uW6F27qm\",nc=00000001,cnonce=\"pwk5Lh7SUDDbo6b0VBjE59ZmBF8ayG0tIAewE8tl\",digest-uri=\"/default\",maxbuf=65536,response=0e22a370b39d4524247d32d86127cc4b,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-26 04:31:29,996 DEBUG [main] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-26 04:31:30,000 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root: starting, having connections 1
2025-03-26 04:31:30,001 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #0 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster
2025-03-26 04:31:30,019 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #0
2025-03-26 04:31:30,019 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: registerApplicationMaster took 107ms
2025-03-26 04:31:30,030 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Preparing Local resources
2025-03-26 04:31:30,067 DEBUG [main] org.apache.hadoop.fs.FileSystem: Starting: Acquiring creator semaphore for hdfs://master:9000/user/root/.sparkStaging/application_1742963439991_0001/__spark_conf__.zip
2025-03-26 04:31:30,067 DEBUG [main] org.apache.hadoop.fs.FileSystem: Acquiring creator semaphore for hdfs://master:9000/user/root/.sparkStaging/application_1742963439991_0001/__spark_conf__.zip: duration 0:00.000s
2025-03-26 04:31:30,068 DEBUG [main] org.apache.hadoop.fs.FileSystem: Starting: Creating FS hdfs://master:9000/user/root/.sparkStaging/application_1742963439991_0001/__spark_conf__.zip
2025-03-26 04:31:30,068 DEBUG [main] org.apache.hadoop.fs.FileSystem: Loading filesystems
2025-03-26 04:31:30,074 DEBUG [main] org.apache.hadoop.fs.FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:31:30,076 DEBUG [main] org.apache.hadoop.fs.FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:31:30,077 DEBUG [main] org.apache.hadoop.fs.FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:31:30,078 DEBUG [main] org.apache.hadoop.fs.FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:31:30,078 DEBUG [main] org.apache.hadoop.fs.FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:31:30,082 DEBUG [main] org.apache.hadoop.fs.FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:31:30,085 DEBUG [main] org.apache.hadoop.fs.FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:31:30,085 DEBUG [main] org.apache.hadoop.fs.FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:31:30,086 DEBUG [main] org.apache.hadoop.fs.FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hive-exec-2.3.9-core.jar
2025-03-26 04:31:30,086 DEBUG [main] org.apache.hadoop.fs.FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__1383911700865684827.zip/hive-exec-2.3.9-core.jar
2025-03-26 04:31:30,087 DEBUG [main] org.apache.hadoop.fs.FileSystem: Looking for FS supporting hdfs
2025-03-26 04:31:30,087 DEBUG [main] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.hdfs.impl
2025-03-26 04:31:30,098 DEBUG [main] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:31:30,098 DEBUG [main] org.apache.hadoop.fs.FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem
2025-03-26 04:31:30,109 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.use.legacy.blockreader.local = false
2025-03-26 04:31:30,109 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.read.shortcircuit = false
2025-03-26 04:31:30,109 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.domain.socket.data.traffic = false
2025-03-26 04:31:30,109 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.domain.socket.path = 
2025-03-26 04:31:30,112 DEBUG [main] org.apache.hadoop.hdfs.DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2025-03-26 04:31:30,114 DEBUG [main] org.apache.hadoop.io.retry.RetryUtils: multipleLinearRandomRetry = null
2025-03-26 04:31:30,116 DEBUG [main] org.apache.hadoop.ipc.Client: getting client out of cache: Client-13d7b2110f204c45b479e64949a5ec25
2025-03-26 04:31:30,263 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
2025-03-26 04:31:30,266 DEBUG [main] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
2025-03-26 04:31:30,267 DEBUG [main] org.apache.hadoop.fs.FileSystem: Creating FS hdfs://master:9000/user/root/.sparkStaging/application_1742963439991_0001/__spark_conf__.zip: duration 0:00.199s
2025-03-26 04:31:30,269 DEBUG [main] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:31:30,269 DEBUG [main] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.14:9000
2025-03-26 04:31:30,269 DEBUG [main] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.14:9000
2025-03-26 04:31:30,270 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@4b2a30d]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy36.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy37.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$prepareLocalResources$4(ApplicationMaster.scala:200)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$prepareLocalResources$4$adapted(ApplicationMaster.scala:197)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.deploy.yarn.ApplicationMaster.prepareLocalResources(ApplicationMaster.scala:197)
	at org.apache.spark.deploy.yarn.ApplicationMaster.createAllocator(ApplicationMaster.scala:463)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:523)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-26 04:31:30,270 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:31:30,271 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
2025-03-26 04:31:30,271 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one
2025-03-26 04:31:30,271 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Use SIMPLE authentication for protocol ClientNamenodeProtocolPB
2025-03-26 04:31:30,271 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
auths {
  method: "SIMPLE"
  mechanism: ""
}

2025-03-26 04:31:30,273 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:9000 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2025-03-26 04:31:30,273 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:9000 from root: starting, having connections 2
2025-03-26 04:31:30,274 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:9000 from root got value #1
2025-03-26 04:31:30,274 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: getFileInfo took 5ms
2025-03-26 04:31:30,293 DEBUG [main] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:31:30,314 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: 
===============================================================================
Default YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
    SPARK_YARN_STAGING_DIR -> hdfs://master:9000/user/root/.sparkStaging/application_1742963439991_0001
    SPARK_USER -> root

  command:
    {{JAVA_HOME}}/bin/java \ 
      -server \ 
      -Xmx2048m \ 
      '-XX:+IgnoreUnrecognizedVMOptions' \ 
      '--add-opens=java.base/java.lang=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.io=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.net=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.nio=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.security.action=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED' \ 
      '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED' \ 
      -Djava.io.tmpdir={{PWD}}/tmp \ 
      '-Dspark.ui.port=0' \ 
      '-Dspark.driver.port=36275' \ 
      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
      -XX:OnOutOfMemoryError='kill %p' \ 
      org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
      --driver-url \ 
      spark://CoarseGrainedScheduler@slave0:36275 \ 
      --executor-id \ 
      <executorId> \ 
      --hostname \ 
      <hostname> \ 
      --cores \ 
      2 \ 
      --app-id \ 
      application_1742963439991_0001 \ 
      --resourceProfileId \ 
      0 \ 
      1><LOG_DIR>/stdout \ 
      2><LOG_DIR>/stderr

  resources:
    __app__.jar -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742963439991_0001/scopt_2.12-3.7.1.jar" } size: 78803 timestamp: 1742963485056 type: FILE visibility: PRIVATE
    __spark_libs__ -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742963439991_0001/__spark_libs__1383911700865684827.zip" } size: 301733843 timestamp: 1742963485005 type: ARCHIVE visibility: PRIVATE
    __spark_conf__ -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742963439991_0001/__spark_conf__.zip" } size: 947014 timestamp: 1742963485176 type: ARCHIVE visibility: PRIVATE
    spark-examples_2.12-3.3.2.jar -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742963439991_0001/spark-examples_2.12-3.3.2.jar" } size: 1567446 timestamp: 1742963485079 type: FILE visibility: PRIVATE

===============================================================================
2025-03-26 04:31:30,334 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Resource profile 0 doesn't exist, adding it
2025-03-26 04:31:30,359 INFO [main] org.apache.hadoop.conf.Configuration: resource-types.xml not found
2025-03-26 04:31:30,359 INFO [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
2025-03-26 04:31:30,363 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
2025-03-26 04:31:30,363 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
2025-03-26 04:31:30,363 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.memory-mb.minimum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.minimum-allocation-mb'
2025-03-26 04:31:30,363 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.memory-mb.maximum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.maximum-allocation-mb'
2025-03-26 04:31:30,363 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.vcores.minimum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.minimum-allocation-vcores'
2025-03-26 04:31:30,363 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.vcores.maximum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.maximum-allocation-vcores'
2025-03-26 04:31:30,364 DEBUG [main] org.apache.spark.deploy.yarn.ResourceRequestHelper: Custom resources requested: Map()
2025-03-26 04:31:30,364 DEBUG [main] org.apache.spark.deploy.yarn.YarnAllocator: Created resource capability: <memory:2432, vCores:2>
2025-03-26 04:31:30,366 INFO [dispatcher-event-loop-0] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@slave0:36275)
2025-03-26 04:31:30,368 DEBUG [main] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 0, executorsStarting: 0
2025-03-26 04:31:30,372 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Will request 3 executor container(s) for  ResourceProfile Id: 0, each with 2 core(s) and 2432 MB memory. with custom resources: <memory:2432, vCores:2>
2025-03-26 04:31:30,380 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added priority=0
2025-03-26 04:31:30,380 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added resourceName=*
2025-03-26 04:31:30,380 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added Execution Type=GUARANTEED
2025-03-26 04:31:30,380 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 1, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:31:30,380 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 1, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:31:30,380 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=1 #asks=1
2025-03-26 04:31:30,381 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 2, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:31:30,381 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 2, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:31:30,381 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=2 #asks=1
2025-03-26 04:31:30,381 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:31:30,381 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:31:30,381 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=3 #asks=1
2025-03-26 04:31:30,382 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Submitted 3 unlocalized container requests.
2025-03-26 04:31:30,391 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #2 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:31:30,398 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #2
2025-03-26 04:31:30,398 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 7ms
2025-03-26 04:31:30,405 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
2025-03-26 04:31:30,406 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:31:30,407 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 3, running: 0, executorsStarting: 0
2025-03-26 04:31:30,408 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #3 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:31:30,409 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #3
2025-03-26 04:31:30,409 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 2ms
2025-03-26 04:31:30,611 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 3. Slept for 200105893/200.
2025-03-26 04:31:30,611 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:31:30,611 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 3, running: 0, executorsStarting: 0
2025-03-26 04:31:30,611 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #4 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:31:30,613 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #4
2025-03-26 04:31:30,613 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 2ms
2025-03-26 04:31:31,014 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 3. Slept for 400181504/400.
2025-03-26 04:31:31,014 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:31:31,015 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 3, running: 0, executorsStarting: 0
2025-03-26 04:31:31,016 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #5 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:31:31,023 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #5
2025-03-26 04:31:31,023 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 8ms
2025-03-26 04:31:31,032 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave0:36369
2025-03-26 04:31:31,037 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Allocated containers: 1. Current executor count: 0. Launching executor count: 0. Cluster resources: <memory:16384, vCores:22>.
2025-03-26 04:31:31,038 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave0, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,041 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,042 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,042 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-26 04:31:31,042 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=3
2025-03-26 04:31:31,042 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=2 #asks=1
2025-03-26 04:31:31,043 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742963439991_0001_01_000002 on host slave0 for executor with ID 1 for ResourceProfile Id 0
2025-03-26 04:31:31,044 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
2025-03-26 04:31:31,044 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:31:31,045 DEBUG [ContainerLauncher-0] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-26 04:31:31,046 DEBUG [ContainerLauncher-0] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-26 04:31:31,047 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-26 04:31:31,047 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:31:31,047 DEBUG [ContainerLauncher-0] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-26 04:31:31,056 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave0:36369
2025-03-26 04:31:31,066 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.15:36369, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963439991 } attemptId: 1 } nodeId { host: "slave0" port: 36369 } appSubmitter: "root" keyId: 1722873515)
2025-03-26 04:31:31,067 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963439991_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@5693813b]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:31:31,067 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-26 04:31:31,070 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: getting client out of cache: Client-13d7b2110f204c45b479e64949a5ec25
2025-03-26 04:31:31,154 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:31:31,154 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Connecting to slave0/172.20.1.15:36369
2025-03-26 04:31:31,154 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Setup connection to slave0/172.20.1.15:36369
2025-03-26 04:31:31,154 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963439991_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@2947ad44]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:31:31,154 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:31:31,155 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@65e77023
2025-03-26 04:31:31,155 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.15:36369. Current token is Kind: NMToken, Service: 172.20.1.15:36369, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963439991 } attemptId: 1 } nodeId { host: "slave0" port: 36369 } appSubmitter: "root" keyId: 1722873515)
2025-03-26 04:31:31,156 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-26 04:31:31,156 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-26 04:31:31,156 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEPfy3YXdMhABEgwKBnNsYXZlMBCRnAIaBHJvb3Qgq+3DtQY=
2025-03-26 04:31:31,156 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-26 04:31:31,156 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-26 04:31:31,156 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEPfy3YXdMhABEgwKBnNsYXZlMBCRnAIaBHJvb3Qgq+3DtQY=\",realm=\"default\",nonce=\"c8kApzaO29XfVwLPrM8n8ai+umh0ZBZnPr+ZICK6\",nc=00000001,cnonce=\"drv/LbXT5vjb2KU09Z1LLg5BJtsDMqoeMnUdRAKD\",digest-uri=\"/default\",maxbuf=65536,response=70683225fde90bfc431a262f1b777bff,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-26 04:31:31,158 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-26 04:31:31,158 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001 sending #6 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-26 04:31:31,159 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001: starting, having connections 3
2025-03-26 04:31:31,178 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001 got value #6
2025-03-26 04:31:31,178 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 25ms
2025-03-26 04:31:31,181 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001: closed
2025-03-26 04:31:31,181 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.15:36369 from appattempt_1742963439991_0001_000001: stopped, remaining connections 2
2025-03-26 04:31:31,845 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 2. Slept for 800066405/800.
2025-03-26 04:31:31,845 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:31:31,845 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 2, running: 1, executorsStarting: 0
2025-03-26 04:31:31,846 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #7 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:31:31,849 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #7
2025-03-26 04:31:31,849 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 4ms
2025-03-26 04:31:31,849 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave1:44687
2025-03-26 04:31:31,849 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave2:44391
2025-03-26 04:31:31,849 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Allocated containers: 2. Current executor count: 1. Launching executor count: 0. Cluster resources: <memory:10240, vCores:20>.
2025-03-26 04:31:31,849 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave2, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,849 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave1, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,863 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,863 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,863 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,864 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-26 04:31:31,864 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=2
2025-03-26 04:31:31,864 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=1 #asks=1
2025-03-26 04:31:31,864 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-26 04:31:31,864 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-26 04:31:31,864 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=1
2025-03-26 04:31:31,864 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=0 #asks=1
2025-03-26 04:31:31,864 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742963439991_0001_01_000003 on host slave2 for executor with ID 2 for ResourceProfile Id 0
2025-03-26 04:31:31,868 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742963439991_0001_01_000004 on host slave1 for executor with ID 3 for ResourceProfile Id 0
2025-03-26 04:31:31,870 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.
2025-03-26 04:31:31,871 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:31:31,874 DEBUG [ContainerLauncher-2] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-26 04:31:31,874 DEBUG [ContainerLauncher-2] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-26 04:31:31,872 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:31:31,875 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-26 04:31:31,875 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:31:31,875 DEBUG [ContainerLauncher-2] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-26 04:31:31,878 DEBUG [ContainerLauncher-1] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-26 04:31:31,878 DEBUG [ContainerLauncher-1] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-26 04:31:31,879 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-26 04:31:31,879 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:31:31,879 DEBUG [ContainerLauncher-1] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-26 04:31:31,887 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave2:44391
2025-03-26 04:31:31,887 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.17:44391, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963439991 } attemptId: 1 } nodeId { host: "slave2" port: 44391 } appSubmitter: "root" keyId: 1722873515)
2025-03-26 04:31:31,888 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963439991_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@5eaef0f6]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:31:31,888 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-26 04:31:31,888 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: getting client out of cache: Client-13d7b2110f204c45b479e64949a5ec25
2025-03-26 04:31:31,889 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:31:31,889 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Connecting to slave2/172.20.1.17:44391
2025-03-26 04:31:31,889 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Setup connection to slave2/172.20.1.17:44391
2025-03-26 04:31:31,889 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963439991_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@7b838a08]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:31:31,889 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:31:31,896 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave1:44687
2025-03-26 04:31:31,896 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@18f91fef
2025-03-26 04:31:31,896 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.17:44391. Current token is Kind: NMToken, Service: 172.20.1.17:44391, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963439991 } attemptId: 1 } nodeId { host: "slave2" port: 44391 } appSubmitter: "root" keyId: 1722873515)
2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.16:44687, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963439991 } attemptId: 1 } nodeId { host: "slave1" port: 44687 } appSubmitter: "root" keyId: 1722873515)
2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEPfy3YXdMhABEgwKBnNsYXZlMhDn2gIaBHJvb3Qgq+3DtQY=
2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEPfy3YXdMhABEgwKBnNsYXZlMhDn2gIaBHJvb3Qgq+3DtQY=\",realm=\"default\",nonce=\"PclYeObuzhg1QbcXjbi+txY4hMutCfkPxw2BtKQS\",nc=00000001,cnonce=\"+4oC2rlPUF8W0tsaT+VS8Ib5Xwstskzq+VOso4Ph\",digest-uri=\"/default\",maxbuf=65536,response=0f655a543ebb6d897d45af470d67209f,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963439991_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@768dd61]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:31:31,897 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-26 04:31:31,898 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: getting client out of cache: Client-13d7b2110f204c45b479e64949a5ec25
2025-03-26 04:31:31,898 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:31:31,898 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Connecting to slave1/172.20.1.16:44687
2025-03-26 04:31:31,898 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Setup connection to slave1/172.20.1.16:44687
2025-03-26 04:31:31,898 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963439991_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@2245aca7]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:31:31,899 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:31:31,907 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@29a4d344
2025-03-26 04:31:31,907 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.16:44687. Current token is Kind: NMToken, Service: 172.20.1.16:44687, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963439991 } attemptId: 1 } nodeId { host: "slave1" port: 44687 } appSubmitter: "root" keyId: 1722873515)
2025-03-26 04:31:31,907 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-26 04:31:31,908 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-26 04:31:31,908 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEPfy3YXdMhABEgwKBnNsYXZlMRCP3QIaBHJvb3Qgq+3DtQY=
2025-03-26 04:31:31,908 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-26 04:31:31,908 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-26 04:31:31,908 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEPfy3YXdMhABEgwKBnNsYXZlMRCP3QIaBHJvb3Qgq+3DtQY=\",realm=\"default\",nonce=\"cxPYTM68dCoc2G8OyBC+xM3P4VI/4xH7awSW41Ir\",nc=00000001,cnonce=\"k3IMRq8PJ4WznBsSkMcN4CIlLKSD0/Ryvl3ydLzF\",digest-uri=\"/default\",maxbuf=65536,response=062c79928f8e68ee97576c7bac10d3ba,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-26 04:31:31,910 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-26 04:31:31,931 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001 sending #8 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-26 04:31:31,932 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001: starting, having connections 4
2025-03-26 04:31:31,933 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-26 04:31:31,950 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001: starting, having connections 4
2025-03-26 04:31:31,951 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001 sending #9 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-26 04:31:32,033 INFO [main] org.apache.spark.executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1577@slave0
2025-03-26 04:31:32,040 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for TERM
2025-03-26 04:31:32,041 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for HUP
2025-03-26 04:31:32,041 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for INT
2025-03-26 04:31:32,095 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001 got value #8
2025-03-26 04:31:32,095 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001: closed
2025-03-26 04:31:32,095 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.17:44391 from appattempt_1742963439991_0001_000001: stopped, remaining connections 3
2025-03-26 04:31:32,095 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 206ms
2025-03-26 04:31:32,138 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001 got value #9
2025-03-26 04:31:32,138 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001: closed
2025-03-26 04:31:32,138 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.16:44687 from appattempt_1742963439991_0001_000001: stopped, remaining connections 2
2025-03-26 04:31:32,138 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 240ms
2025-03-26 04:31:32,349 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
2025-03-26 04:31:32,355 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
2025-03-26 04:31:32,355 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
2025-03-26 04:31:32,356 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
2025-03-26 04:31:32,356 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
2025-03-26 04:31:32,356 DEBUG [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
2025-03-26 04:31:32,375 DEBUG [main] org.apache.hadoop.util.Shell: setsid exited with exit code 0
2025-03-26 04:31:32,376 DEBUG [main] org.apache.hadoop.security.SecurityUtil: Setting hadoop.security.token.service.use_ip to true
2025-03-26 04:31:32,380 DEBUG [main] org.apache.hadoop.security.Groups:  Creating new Groups object
2025-03-26 04:31:32,381 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
2025-03-26 04:31:32,381 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2025-03-26 04:31:32,381 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-26 04:31:32,381 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-03-26 04:31:32,382 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Falling back to shell based
2025-03-26 04:31:32,382 DEBUG [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2025-03-26 04:31:32,431 DEBUG [main] org.apache.hadoop.security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2025-03-26 04:31:32,433 DEBUG [main] org.apache.spark.deploy.SparkHadoopUtil: creating UGI for user: root
2025-03-26 04:31:32,436 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Hadoop login
2025-03-26 04:31:32,437 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: hadoop login commit
2025-03-26 04:31:32,438 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using local user: UnixPrincipal: root
2025-03-26 04:31:32,440 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using user: "UnixPrincipal: root" with name: root
2025-03-26 04:31:32,440 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: User entry: "root"
2025-03-26 04:31:32,440 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Reading credentials from location /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000002/container_tokens
2025-03-26 04:31:32,446 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Loaded 1 tokens from /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000002/container_tokens
2025-03-26 04:31:32,446 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: UGI loginUser: root (auth:SIMPLE)
2025-03-26 04:31:32,446 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.spark.deploy.SparkHadoopUtil$$anon$1@59d2400d]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:427)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend$.main(YarnCoarseGrainedExecutorBackend.scala:83)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main(YarnCoarseGrainedExecutorBackend.scala)
2025-03-26 04:31:32,456 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-26 04:31:32,456 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-26 04:31:32,456 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-26 04:31:32,457 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-26 04:31:32,457 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-26 04:31:32,543 DEBUG [main] io.netty.util.internal.logging.InternalLoggerFactory: Using SLF4J as the default logging framework
2025-03-26 04:31:32,547 DEBUG [main] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2025-03-26 04:31:32,547 DEBUG [main] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2025-03-26 04:31:32,559 DEBUG [main] io.netty.channel.MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
2025-03-26 04:31:32,578 DEBUG [main] io.netty.util.internal.PlatformDependent0: -Dio.netty.noUnsafe: false
2025-03-26 04:31:32,578 DEBUG [main] io.netty.util.internal.PlatformDependent0: Java version: 8
2025-03-26 04:31:32,579 DEBUG [main] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
2025-03-26 04:31:32,579 DEBUG [main] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.copyMemory: available
2025-03-26 04:31:32,580 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.Buffer.address: available
2025-03-26 04:31:32,580 DEBUG [main] io.netty.util.internal.PlatformDependent0: direct buffer constructor: available
2025-03-26 04:31:32,580 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.Bits.unaligned: available, true
2025-03-26 04:31:32,581 DEBUG [main] io.netty.util.internal.PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2025-03-26 04:31:32,581 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
2025-03-26 04:31:32,581 DEBUG [main] io.netty.util.internal.PlatformDependent: sun.misc.Unsafe: available
2025-03-26 04:31:32,581 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.tmpdir: /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000002/tmp (java.io.tmpdir)
2025-03-26 04:31:32,581 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
2025-03-26 04:31:32,581 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.maxDirectMemory: 1908932608 bytes
2025-03-26 04:31:32,581 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
2025-03-26 04:31:32,582 DEBUG [main] io.netty.util.internal.CleanerJava6: java.nio.ByteBuffer.cleaner(): available
2025-03-26 04:31:32,582 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.noPreferDirect: false
2025-03-26 04:31:32,582 DEBUG [main] io.netty.channel.nio.NioEventLoop: -Dio.netty.noKeySetOptimization: false
2025-03-26 04:31:32,582 DEBUG [main] io.netty.channel.nio.NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
2025-03-26 04:31:32,586 DEBUG [main] io.netty.util.internal.PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
2025-03-26 04:31:32,598 DEBUG [main] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
2025-03-26 04:31:32,598 DEBUG [main] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
2025-03-26 04:31:32,600 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
2025-03-26 04:31:32,600 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
2025-03-26 04:31:32,600 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
2025-03-26 04:31:32,600 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
2025-03-26 04:31:32,600 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
2025-03-26 04:31:32,600 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
2025-03-26 04:31:32,601 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
2025-03-26 04:31:32,601 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2025-03-26 04:31:32,601 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
2025-03-26 04:31:32,601 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2025-03-26 04:31:32,601 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
2025-03-26 04:31:32,601 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2025-03-26 04:31:32,646 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave0/172.20.1.15:36275
2025-03-26 04:31:32,659 DEBUG [netty-rpc-connection-0] io.netty.channel.DefaultChannelId: -Dio.netty.processId: 1577 (auto-detected)
2025-03-26 04:31:32,662 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: -Djava.net.preferIPv4Stack: false
2025-03-26 04:31:32,662 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: -Djava.net.preferIPv6Addresses: false
2025-03-26 04:31:32,664 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
2025-03-26 04:31:32,665 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: /proc/sys/net/core/somaxconn: 4096
2025-03-26 04:31:32,665 DEBUG [netty-rpc-connection-0] io.netty.channel.DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:14:01:0f (auto-detected)
2025-03-26 04:31:32,686 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.allocator.type: pooled
2025-03-26 04:31:32,686 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0
2025-03-26 04:31:32,686 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
2025-03-26 04:31:32,707 DEBUG [rpc-client-1-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkAccessible: true
2025-03-26 04:31:32,708 DEBUG [rpc-client-1-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkBounds: true
2025-03-26 04:31:32,708 DEBUG [rpc-client-1-1] io.netty.util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@ebc48ad
2025-03-26 04:31:32,721 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94] REGISTERED
2025-03-26 04:31:32,721 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94] CONNECT: slave0/172.20.1.15:36275
2025-03-26 04:31:32,738 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Connection to slave0/172.20.1.15:36275 successful, running bootstraps...
2025-03-26 04:31:32,738 INFO [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave0/172.20.1.15:36275 after 83 ms (0 ms spent in bootstraps)
2025-03-26 04:31:32,742 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096
2025-03-26 04:31:32,742 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.ratio: 8
2025-03-26 04:31:32,742 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.chunkSize: 32
2025-03-26 04:31:32,742 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.blocking: false
2025-03-26 04:31:32,743 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.15:60408.
2025-03-26 04:31:32,743 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] ACTIVE
2025-03-26 04:31:32,751 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 168]
2025-03-26 04:31:32,752 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:32,761 DEBUG [rpc-server-4-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkAccessible: true
2025-03-26 04:31:32,761 DEBUG [rpc-server-4-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkBounds: true
2025-03-26 04:31:32,761 DEBUG [rpc-server-4-1] io.netty.util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@1ec638df
2025-03-26 04:31:32,769 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] REGISTERED
2025-03-26 04:31:32,770 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] ACTIVE
2025-03-26 04:31:32,773 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096
2025-03-26 04:31:32,773 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.ratio: 8
2025-03-26 04:31:32,773 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.chunkSize: 32
2025-03-26 04:31:32,773 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.blocking: false
2025-03-26 04:31:32,777 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] READ 189B
2025-03-26 04:31:32,787 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] READ COMPLETE
2025-03-26 04:31:32,790 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:32,790 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] FLUSH
2025-03-26 04:31:32,792 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] READ 68B
2025-03-26 04:31:32,799 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:32,799 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 169]
2025-03-26 04:31:32,800 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:32,800 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] READ 190B
2025-03-26 04:31:32,802 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] READ COMPLETE
2025-03-26 04:31:32,828 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4386]
2025-03-26 04:31:32,828 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] FLUSH
2025-03-26 04:31:32,828 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] READ 2048B
2025-03-26 04:31:32,830 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] READ 2359B
2025-03-26 04:31:32,855 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:32,856 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 - R:slave0/172.20.1.15:36275] CLOSE
2025-03-26 04:31:32,856 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 - R:/172.20.1.15:60408] READ COMPLETE
2025-03-26 04:31:32,857 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 ! R:slave0/172.20.1.15:36275] INACTIVE
2025-03-26 04:31:32,857 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 ! R:/172.20.1.15:60408] INACTIVE
2025-03-26 04:31:32,857 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x2189eb94, L:/172.20.1.15:60408 ! R:slave0/172.20.1.15:36275] UNREGISTERED
2025-03-26 04:31:32,858 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x00794007, L:/172.20.1.15:36275 ! R:/172.20.1.15:60408] UNREGISTERED
2025-03-26 04:31:32,868 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-26 04:31:32,869 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-26 04:31:32,869 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-26 04:31:32,869 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-26 04:31:32,869 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-26 04:31:32,888 DEBUG [main] org.apache.spark.SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
2025-03-26 04:31:32,901 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave0/172.20.1.15:36275
2025-03-26 04:31:32,903 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac] REGISTERED
2025-03-26 04:31:32,903 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac] CONNECT: slave0/172.20.1.15:36275
2025-03-26 04:31:32,903 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Connection to slave0/172.20.1.15:36275 successful, running bootstraps...
2025-03-26 04:31:32,903 INFO [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave0/172.20.1.15:36275 after 1 ms (0 ms spent in bootstraps)
2025-03-26 04:31:32,903 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] ACTIVE
2025-03-26 04:31:32,903 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 162]
2025-03-26 04:31:32,903 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:32,904 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.15:60410.
2025-03-26 04:31:32,905 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] REGISTERED
2025-03-26 04:31:32,905 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] ACTIVE
2025-03-26 04:31:32,905 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 183B
2025-03-26 04:31:32,906 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:32,906 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:32,906 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:32,906 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 21B
2025-03-26 04:31:32,906 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:32,906 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 47B
2025-03-26 04:31:32,910 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:32,931 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 164]
2025-03-26 04:31:32,931 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:32,931 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 185B
2025-03-26 04:31:32,932 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:32,932 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:32,932 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:32,933 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 68B
2025-03-26 04:31:32,933 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:32,936 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 173]
2025-03-26 04:31:32,936 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:32,936 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 194B
2025-03-26 04:31:32,937 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:32,937 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:32,937 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:32,937 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 68B
2025-03-26 04:31:32,937 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:32,952 INFO [main] org.apache.spark.storage.DiskBlockManager: Created local directory at /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/blockmgr-418c3f40-87d2-46a9-87ac-3e232942eb10
2025-03-26 04:31:32,953 DEBUG [main] org.apache.spark.storage.DiskBlockManager: Adding shutdown hook
2025-03-26 04:31:32,954 DEBUG [main] org.apache.spark.util.ShutdownHookManager: Adding shutdown hook
2025-03-26 04:31:32,976 INFO [main] org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 912.3 MiB
2025-03-26 04:31:33,158 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 169]
2025-03-26 04:31:33,158 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:33,158 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 190B
2025-03-26 04:31:33,159 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:33,160 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:33,160 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:33,160 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 68B
2025-03-26 04:31:33,161 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:33,189 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@slave0:36275
2025-03-26 04:31:33,231 DEBUG [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Resource profile id is: 0
2025-03-26 04:31:33,239 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-26 04:31:33,239 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: No custom resources configured for spark.executor.
2025-03-26 04:31:33,240 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-26 04:31:33,241 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 168]
2025-03-26 04:31:33,241 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:33,242 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 189B
2025-03-26 04:31:33,242 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:33,243 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:33,243 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:33,243 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 68B
2025-03-26 04:31:33,244 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:33,286 DEBUG [dispatcher-Executor] org.apache.spark.util.YarnContainerInfoHelper: Base URL for logs: http://slave0:8042/node/containerlogs/container_1742963439991_0001_01_000002/root
2025-03-26 04:31:33,314 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 1916]
2025-03-26 04:31:33,314 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:33,315 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 512B
2025-03-26 04:31:33,317 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 1425B
2025-03-26 04:31:33,334 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:33,335 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.15:60410) with ID 1,  ResourceProfileId 0
2025-03-26 04:31:33,342 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:33,347 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:33,347 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 21B
2025-03-26 04:31:33,347 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:33,347 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 47B
2025-03-26 04:31:33,349 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:33,350 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver
2025-03-26 04:31:33,354 INFO [dispatcher-Executor] org.apache.spark.executor.Executor: Starting executor ID 1 on host slave0
2025-03-26 04:31:33,378 DEBUG [dispatcher-Executor] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 38599
2025-03-26 04:31:33,379 INFO [dispatcher-Executor] org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38599.
2025-03-26 04:31:33,379 INFO [dispatcher-Executor] org.apache.spark.network.netty.NettyBlockTransferService: Server created on slave0:38599
2025-03-26 04:31:33,381 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-03-26 04:31:33,387 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, slave0, 38599, None)
2025-03-26 04:31:33,390 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 1456]
2025-03-26 04:31:33,390 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:33,390 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 1477B
2025-03-26 04:31:33,393 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:33,400 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave0
2025-03-26 04:31:33,400 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave0:38599 with 912.3 MiB RAM, BlockManagerId(1, slave0, 38599, None)
2025-03-26 04:31:33,403 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-26 04:31:33,403 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:33,403 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 100B
2025-03-26 04:31:33,404 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:33,405 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, slave0, 38599, None)
2025-03-26 04:31:33,406 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(1, slave0, 38599, None)
2025-03-26 04:31:33,413 INFO [dispatcher-Executor] org.apache.spark.executor.Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000002/__app__.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000002/spark-examples_2.12-3.3.2.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000002/__app__.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000002/spark-examples_2.12-3.3.2.jar'
2025-03-26 04:31:33,418 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 163]
2025-03-26 04:31:33,418 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:33,418 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 184B
2025-03-26 04:31:33,419 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:33,419 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:33,419 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:33,420 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 68B
2025-03-26 04:31:33,420 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:33,445 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 177]
2025-03-26 04:31:33,445 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:33,446 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 190B
2025-03-26 04:31:33,448 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:34,871 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000070916/3000.
2025-03-26 04:31:34,871 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:31:34,871 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-26 04:31:34,872 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #10 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:31:34,876 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #10
2025-03-26 04:31:34,876 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 4ms
2025-03-26 04:31:34,877 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Allocated containers: 2. Current executor count: 3. Launching executor count: 0. Cluster resources: <memory:4096, vCores:18>.
2025-03-26 04:31:34,877 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave1, resource: <memory:2432, vCores:2>
2025-03-26 04:31:34,877 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave2, resource: <memory:2432, vCores:2>
2025-03-26 04:31:34,877 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-26 04:31:34,877 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-26 04:31:34,877 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-26 04:31:34,877 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-26 04:31:34,878 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Releasing 2 unneeded containers that were allocated to us
2025-03-26 04:31:34,878 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 0 of them.
2025-03-26 04:31:36,682 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.17:55580.
2025-03-26 04:31:36,682 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] REGISTERED
2025-03-26 04:31:36,682 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] ACTIVE
2025-03-26 04:31:36,696 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] READ 189B
2025-03-26 04:31:36,696 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] READ COMPLETE
2025-03-26 04:31:36,696 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:36,697 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] FLUSH
2025-03-26 04:31:36,708 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] READ 190B
2025-03-26 04:31:36,708 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] READ COMPLETE
2025-03-26 04:31:36,709 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4386]
2025-03-26 04:31:36,709 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] FLUSH
2025-03-26 04:31:36,751 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 - R:/172.20.1.17:55580] READ COMPLETE
2025-03-26 04:31:36,751 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 ! R:/172.20.1.17:55580] INACTIVE
2025-03-26 04:31:36,751 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xee512e1e, L:/172.20.1.15:36275 ! R:/172.20.1.17:55580] UNREGISTERED
2025-03-26 04:31:36,821 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.17:55596.
2025-03-26 04:31:36,821 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] REGISTERED
2025-03-26 04:31:36,821 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] ACTIVE
2025-03-26 04:31:36,821 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 183B
2025-03-26 04:31:36,822 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:36,822 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:36,822 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:36,825 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.16:50332.
2025-03-26 04:31:36,825 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] REGISTERED
2025-03-26 04:31:36,826 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] ACTIVE
2025-03-26 04:31:36,842 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] READ 189B
2025-03-26 04:31:36,842 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] READ COMPLETE
2025-03-26 04:31:36,843 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:36,843 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] FLUSH
2025-03-26 04:31:36,855 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] READ 190B
2025-03-26 04:31:36,857 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] READ COMPLETE
2025-03-26 04:31:36,858 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4386]
2025-03-26 04:31:36,858 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] FLUSH
2025-03-26 04:31:36,864 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 185B
2025-03-26 04:31:36,865 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:36,865 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:36,865 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:36,869 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 194B
2025-03-26 04:31:36,870 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:36,870 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:36,870 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:36,891 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 - R:/172.20.1.16:50332] READ COMPLETE
2025-03-26 04:31:36,891 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 ! R:/172.20.1.16:50332] INACTIVE
2025-03-26 04:31:36,891 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x950e81a7, L:/172.20.1.15:36275 ! R:/172.20.1.16:50332] UNREGISTERED
2025-03-26 04:31:36,956 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.16:50336.
2025-03-26 04:31:36,957 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] REGISTERED
2025-03-26 04:31:36,957 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] ACTIVE
2025-03-26 04:31:36,957 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 183B
2025-03-26 04:31:36,957 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:36,959 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:36,959 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:36,997 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 185B
2025-03-26 04:31:36,998 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:36,998 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:36,998 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:37,003 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 194B
2025-03-26 04:31:37,003 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:37,004 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,004 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:37,196 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 190B
2025-03-26 04:31:37,197 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:37,197 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,197 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:37,270 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 189B
2025-03-26 04:31:37,271 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:37,271 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,271 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:37,279 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 190B
2025-03-26 04:31:37,280 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:37,280 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,280 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:37,363 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 512B
2025-03-26 04:31:37,363 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 1425B
2025-03-26 04:31:37,364 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:37,364 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.17:55596) with ID 2,  ResourceProfileId 0
2025-03-26 04:31:37,365 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,365 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:37,372 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 189B
2025-03-26 04:31:37,373 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:37,373 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,373 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:37,433 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 1477B
2025-03-26 04:31:37,434 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave2
2025-03-26 04:31:37,434 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave2:33207 with 912.3 MiB RAM, BlockManagerId(2, slave2, 33207, None)
2025-03-26 04:31:37,434 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:37,435 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-26 04:31:37,435 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:37,452 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 512B
2025-03-26 04:31:37,452 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 1425B
2025-03-26 04:31:37,453 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.16:50336) with ID 3,  ResourceProfileId 0
2025-03-26 04:31:37,453 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:37,453 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,453 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:37,456 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 184B
2025-03-26 04:31:37,457 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:37,457 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,457 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:37,459 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2025-03-26 04:31:37,459 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
2025-03-26 04:31:37,502 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 190B
2025-03-26 04:31:37,503 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:37,523 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 1477B
2025-03-26 04:31:37,524 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:37,525 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave1
2025-03-26 04:31:37,525 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave1:35803 with 912.3 MiB RAM, BlockManagerId(3, slave1, 35803, None)
2025-03-26 04:31:37,526 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-26 04:31:37,527 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:37,543 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 184B
2025-03-26 04:31:37,543 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:37,544 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:37,544 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:37,563 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 190B
2025-03-26 04:31:37,564 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:37,878 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000061358/3000.
2025-03-26 04:31:37,879 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:31:37,879 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-26 04:31:37,879 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #11 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:31:37,885 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #11
2025-03-26 04:31:37,885 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 6ms
2025-03-26 04:31:37,888 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Completed 2 containers
2025-03-26 04:31:37,890 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Finished processing 2 completed containers. Current running executor count: 3.
2025-03-26 04:31:38,234 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-26 04:31:38,234 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-26 04:31:38,234 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:31:38,234 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-26 04:31:38,235 INFO [Driver] org.apache.spark.sql.internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-03-26 04:31:38,235 DEBUG [Driver] org.apache.spark.sql.internal.SharedState: Applying other initial session options to HadoopConf: spark.app.name -> DeveloperApiExample
2025-03-26 04:31:38,236 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Starting: Acquiring creator semaphore for file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000001/spark-warehouse
2025-03-26 04:31:38,236 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Acquiring creator semaphore for file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000001/spark-warehouse: duration 0:00.000s
2025-03-26 04:31:38,236 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Starting: Creating FS file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000001/spark-warehouse
2025-03-26 04:31:38,236 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-26 04:31:38,236 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-26 04:31:38,236 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:31:38,236 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-26 04:31:38,236 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Creating FS file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000001/spark-warehouse: duration 0:00.000s
2025-03-26 04:31:38,236 INFO [Driver] org.apache.spark.sql.internal.SharedState: Warehouse path is 'file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/container_1742963439991_0001_01_000001/spark-warehouse'.
2025-03-26 04:31:38,243 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:38,244 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol http
2025-03-26 04:31:38,244 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Unknown protocol http, delegating to default implementation
2025-03-26 04:31:38,244 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:38,245 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:38,245 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:38,246 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol jar
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting jar
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.jar.impl
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol file
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Found implementation of file: class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-26 04:31:38,247 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Using handler for protocol file
2025-03-26 04:31:38,661 DEBUG [Driver] org.apache.spark.sql.catalyst.parser.CatalystSqlParser: Parsing command: spark_grouping_id
2025-03-26 04:31:39,125 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegression: Input schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}}]}
2025-03-26 04:31:39,128 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegression: Expected output schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{}},{"name":"rawPrediction","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":false,"metadata":{}}]}
2025-03-26 04:31:39,154 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#0
2025-03-26 04:31:39,170 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#7
2025-03-26 04:31:39,171 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#1
2025-03-26 04:31:39,581 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, double, false],input[1, vector, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     double value_0 = i.getDouble(0);
/* 036 */     mutableStateArray_0[0].write(0, value_0);
/* 037 */
/* 038 */     boolean isNull_1 = i.isNullAt(1);
/* 039 */     InternalRow value_1 = isNull_1 ?
/* 040 */     null : (i.getStruct(1, 4));
/* 041 */     if (isNull_1) {
/* 042 */       mutableStateArray_0[0].setNullAt(1);
/* 043 */     } else {
/* 044 */       final InternalRow tmpInput_0 = value_1;
/* 045 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 046 */         mutableStateArray_0[0].write(1, (UnsafeRow) tmpInput_0);
/* 047 */       } else {
/* 048 */         // Remember the current cursor so that we can calculate how many bytes are
/* 049 */         // written later.
/* 050 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 051 */
/* 052 */         mutableStateArray_0[1].resetRowWriter();
/* 053 */
/* 054 */
/* 055 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 056 */
/* 057 */
/* 058 */         if ((tmpInput_0.isNullAt(1))) {
/* 059 */           mutableStateArray_0[1].setNullAt(1);
/* 060 */         } else {
/* 061 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 062 */         }
/* 063 */
/* 064 */
/* 065 */         if ((tmpInput_0.isNullAt(2))) {
/* 066 */           mutableStateArray_0[1].setNullAt(2);
/* 067 */         } else {
/* 068 */           // Remember the current cursor so that we can calculate how many bytes are
/* 069 */           // written later.
/* 070 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 071 */
/* 072 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 073 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 074 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 075 */           } else {
/* 076 */             final int numElements_0 = tmpInput_1.numElements();
/* 077 */             mutableStateArray_1[0].initialize(numElements_0);
/* 078 */
/* 079 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 080 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 081 */             }
/* 082 */           }
/* 083 */
/* 084 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 085 */         }
/* 086 */
/* 087 */
/* 088 */         if ((tmpInput_0.isNullAt(3))) {
/* 089 */           mutableStateArray_0[1].setNullAt(3);
/* 090 */         } else {
/* 091 */           // Remember the current cursor so that we can calculate how many bytes are
/* 092 */           // written later.
/* 093 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 094 */
/* 095 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 096 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 097 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 098 */           } else {
/* 099 */             final int numElements_1 = tmpInput_2.numElements();
/* 100 */             mutableStateArray_1[1].initialize(numElements_1);
/* 101 */
/* 102 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 103 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 104 */             }
/* 105 */           }
/* 106 */
/* 107 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 108 */         }
/* 109 */
/* 110 */
/* 111 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, previousCursor_0);
/* 112 */       }
/* 113 */     }
/* 114 */     return (mutableStateArray_0[0].getRow());
/* 115 */   }
/* 116 */
/* 117 */
/* 118 */ }

2025-03-26 04:31:39,593 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     double value_0 = i.getDouble(0);
/* 036 */     mutableStateArray_0[0].write(0, value_0);
/* 037 */
/* 038 */     boolean isNull_1 = i.isNullAt(1);
/* 039 */     InternalRow value_1 = isNull_1 ?
/* 040 */     null : (i.getStruct(1, 4));
/* 041 */     if (isNull_1) {
/* 042 */       mutableStateArray_0[0].setNullAt(1);
/* 043 */     } else {
/* 044 */       final InternalRow tmpInput_0 = value_1;
/* 045 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 046 */         mutableStateArray_0[0].write(1, (UnsafeRow) tmpInput_0);
/* 047 */       } else {
/* 048 */         // Remember the current cursor so that we can calculate how many bytes are
/* 049 */         // written later.
/* 050 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 051 */
/* 052 */         mutableStateArray_0[1].resetRowWriter();
/* 053 */
/* 054 */
/* 055 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 056 */
/* 057 */
/* 058 */         if ((tmpInput_0.isNullAt(1))) {
/* 059 */           mutableStateArray_0[1].setNullAt(1);
/* 060 */         } else {
/* 061 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 062 */         }
/* 063 */
/* 064 */
/* 065 */         if ((tmpInput_0.isNullAt(2))) {
/* 066 */           mutableStateArray_0[1].setNullAt(2);
/* 067 */         } else {
/* 068 */           // Remember the current cursor so that we can calculate how many bytes are
/* 069 */           // written later.
/* 070 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 071 */
/* 072 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 073 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 074 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 075 */           } else {
/* 076 */             final int numElements_0 = tmpInput_1.numElements();
/* 077 */             mutableStateArray_1[0].initialize(numElements_0);
/* 078 */
/* 079 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 080 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 081 */             }
/* 082 */           }
/* 083 */
/* 084 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 085 */         }
/* 086 */
/* 087 */
/* 088 */         if ((tmpInput_0.isNullAt(3))) {
/* 089 */           mutableStateArray_0[1].setNullAt(3);
/* 090 */         } else {
/* 091 */           // Remember the current cursor so that we can calculate how many bytes are
/* 092 */           // written later.
/* 093 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 094 */
/* 095 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 096 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 097 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 098 */           } else {
/* 099 */             final int numElements_1 = tmpInput_2.numElements();
/* 100 */             mutableStateArray_1[1].initialize(numElements_1);
/* 101 */
/* 102 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 103 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 104 */             }
/* 105 */           }
/* 106 */
/* 107 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 108 */         }
/* 109 */
/* 110 */
/* 111 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, previousCursor_0);
/* 112 */       }
/* 113 */     }
/* 114 */     return (mutableStateArray_0[0].getRow());
/* 115 */   }
/* 116 */
/* 117 */
/* 118 */ }

2025-03-26 04:31:39,699 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 117.56312 ms
2025-03-26 04:31:39,712 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$1
2025-03-26 04:31:39,722 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$1) is now cleaned +++
2025-03-26 04:31:39,732 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1
2025-03-26 04:31:39,738 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
2025-03-26 04:31:39,754 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$extractLabeledPoints$1
2025-03-26 04:31:39,755 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$extractLabeledPoints$1) is now cleaned +++
2025-03-26 04:31:39,761 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$take$2
2025-03-26 04:31:39,765 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$take$2) is now cleaned +++
2025-03-26 04:31:39,817 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5
2025-03-26 04:31:39,821 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
2025-03-26 04:31:39,822 INFO [Driver] org.apache.spark.SparkContext: Starting job: take at DeveloperApiExample.scala:127
2025-03-26 04:31:39,824 DEBUG [Driver] org.apache.spark.scheduler.DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 5 took 0.000613 seconds
2025-03-26 04:31:39,826 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Merging stage rdd profiles: Set()
2025-03-26 04:31:39,833 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Got job 0 (take at DeveloperApiExample.scala:127) with 1 output partitions
2025-03-26 04:31:39,833 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (take at DeveloperApiExample.scala:127)
2025-03-26 04:31:39,833 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2025-03-26 04:31:39,833 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2025-03-26 04:31:39,849 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: submitStage(ResultStage 0 (name=take at DeveloperApiExample.scala:127;jobs=0))
2025-03-26 04:31:39,850 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: missing: List()
2025-03-26 04:31:39,850 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at Predictor.scala:185), which has no missing parents
2025-03-26 04:31:39,851 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: submitMissingTasks(ResultStage 0)
2025-03-26 04:31:39,905 INFO [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.0 KiB, free 2004.6 MiB)
2025-03-26 04:31:39,906 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Put block broadcast_0 locally took 15 ms
2025-03-26 04:31:39,907 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Putting block broadcast_0 without replication took 15 ms
2025-03-26 04:31:39,926 INFO [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 2004.6 MiB)
2025-03-26 04:31:39,927 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, slave0, 46565, None)
2025-03-26 04:31:39,927 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on slave0:46565 (size: 9.4 KiB, free: 2004.6 MiB)
2025-03-26 04:31:39,929 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
2025-03-26 04:31:39,929 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Told master about block broadcast_0_piece0
2025-03-26 04:31:39,930 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Put block broadcast_0_piece0 locally took 5 ms
2025-03-26 04:31:39,930 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Putting block broadcast_0_piece0 without replication took 5 ms
2025-03-26 04:31:39,930 INFO [dag-scheduler-event-loop] org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
2025-03-26 04:31:39,940 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at Predictor.scala:185) (first 15 tasks are for partitions Vector(0))
2025-03-26 04:31:39,941 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Adding task set 0.0 with 1 tasks resource profile 0
2025-03-26 04:31:39,952 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Epoch for TaskSet 0.0: 0
2025-03-26 04:31:39,954 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Adding pending tasks took 1 ms
2025-03-26 04:31:39,956 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2025-03-26 04:31:39,958 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnClusterScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0
2025-03-26 04:31:39,972 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (slave0, executor 1, partition 0, PROCESS_LOCAL, 4702 bytes) taskResourceAssignments Map()
2025-03-26 04:31:39,976 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
2025-03-26 04:31:39,981 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 0 on executor id: 1 hostname: slave0.
2025-03-26 04:31:39,985 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 5149]
2025-03-26 04:31:39,985 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:39,985 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 464B
2025-03-26 04:31:39,986 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 1024B
2025-03-26 04:31:39,986 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 3674B
2025-03-26 04:31:39,990 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:39,992 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0
2025-03-26 04:31:39,999 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2025-03-26 04:31:40,011 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 1629]
2025-03-26 04:31:40,011 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:40,011 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 1642B
2025-03-26 04:31:40,016 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:40,055 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: task 0.0 in stage 0.0 (TID 0)'s epoch is 0
2025-03-26 04:31:40,060 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1
2025-03-26 04:31:40,114 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting local block broadcast_0
2025-03-26 04:31:40,117 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Block broadcast_0 was not found
2025-03-26 04:31:40,118 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-03-26 04:31:40,126 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Reading piece broadcast_0_piece0 of broadcast_0
2025-03-26 04:31:40,126 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting local block broadcast_0_piece0 as bytes
2025-03-26 04:31:40,128 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting remote block broadcast_0_piece0
2025-03-26 04:31:40,130 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 317]
2025-03-26 04:31:40,130 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:40,131 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 338B
2025-03-26 04:31:40,132 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:40,137 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 760]
2025-03-26 04:31:40,138 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:40,138 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 781B
2025-03-26 04:31:40,153 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:40,156 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting remote block broadcast_0_piece0 from BlockManagerId(driver, slave0, 46565, None)
2025-03-26 04:31:40,161 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave0/172.20.1.15:46565
2025-03-26 04:31:40,168 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e] REGISTERED
2025-03-26 04:31:40,168 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e] CONNECT: slave0/172.20.1.15:46565
2025-03-26 04:31:40,169 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Connection to slave0/172.20.1.15:46565 successful, running bootstraps...
2025-03-26 04:31:40,169 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave0/172.20.1.15:46565 after 7 ms (0 ms spent in bootstraps)
2025-03-26 04:31:40,172 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] ACTIVE
2025-03-26 04:31:40,173 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 71]
2025-03-26 04:31:40,173 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] FLUSH
2025-03-26 04:31:40,174 DEBUG [shuffle-server-7-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.15:60102.
2025-03-26 04:31:40,175 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] REGISTERED
2025-03-26 04:31:40,175 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] ACTIVE
2025-03-26 04:31:40,176 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] READ 92B
2025-03-26 04:31:40,186 DEBUG [shuffle-server-7-1] org.apache.spark.storage.BlockManager: Getting local block broadcast_0_piece0 as bytes
2025-03-26 04:31:40,187 DEBUG [shuffle-server-7-1] org.apache.spark.storage.BlockManager: Level for block broadcast_0_piece0 is StorageLevel(disk, memory, 1 replicas)
2025-03-26 04:31:40,189 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 13]
2025-03-26 04:31:40,189 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] FLUSH
2025-03-26 04:31:40,189 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ 21B
2025-03-26 04:31:40,189 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ COMPLETE
2025-03-26 04:31:40,190 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ 13B
2025-03-26 04:31:40,194 DEBUG [shuffle-client-4-1] org.apache.spark.network.client.TransportClient: Sending fetch chunk request 0 to slave0/172.20.1.15:46565
2025-03-26 04:31:40,196 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] WRITE 21B
2025-03-26 04:31:40,196 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] FLUSH
2025-03-26 04:31:40,196 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ COMPLETE
2025-03-26 04:31:40,196 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] READ COMPLETE
2025-03-26 04:31:40,196 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] READ 21B
2025-03-26 04:31:40,198 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 9662]
2025-03-26 04:31:40,199 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] FLUSH
2025-03-26 04:31:40,199 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ 21B
2025-03-26 04:31:40,199 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ COMPLETE
2025-03-26 04:31:40,199 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ 1024B
2025-03-26 04:31:40,199 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ 8638B
2025-03-26 04:31:40,200 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ COMPLETE
2025-03-26 04:31:40,201 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 - R:/172.20.1.15:60102] READ COMPLETE
2025-03-26 04:31:40,213 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 912.3 MiB)
2025-03-26 04:31:40,217 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 173]
2025-03-26 04:31:40,218 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:40,218 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 194B
2025-03-26 04:31:40,219 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:40,220 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(1, slave0, 38599, None)
2025-03-26 04:31:40,220 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on slave0:38599 (size: 9.4 KiB, free: 912.3 MiB)
2025-03-26 04:31:40,221 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:40,221 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:40,221 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 68B
2025-03-26 04:31:40,221 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:40,223 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
2025-03-26 04:31:40,223 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Told master about block broadcast_0_piece0
2025-03-26 04:31:40,224 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Put block broadcast_0_piece0 locally took 13 ms
2025-03-26 04:31:40,226 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Putting block broadcast_0_piece0 without replication took 15 ms
2025-03-26 04:31:40,227 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Reading broadcast variable 0 took 109 ms
2025-03-26 04:31:40,273 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:9000 from root: closed
2025-03-26 04:31:40,273 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:9000 from root: stopped, remaining connections 1
2025-03-26 04:31:40,294 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.0 KiB, free 912.3 MiB)
2025-03-26 04:31:40,298 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Put block broadcast_0 locally took 55 ms
2025-03-26 04:31:40,298 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Putting block broadcast_0 without replication took 55 ms
2025-03-26 04:31:40,890 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000066190/3000.
2025-03-26 04:31:40,890 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:31:40,890 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-26 04:31:40,891 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #12 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:31:40,894 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #12
2025-03-26 04:31:40,894 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 4ms
2025-03-26 04:31:41,799 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for createexternalrow(input[0, double, false], newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, StructField(label,DoubleType,false), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_5 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_5);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[2];
/* 039 */
/* 040 */     double value_1 = i.getDouble(0);
/* 041 */     if (false) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.ml.linalg.VectorUDT value_3 = false ?
/* 048 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 049 */     boolean isNull_2 = true;
/* 050 */     org.apache.spark.ml.linalg.Vector value_2 = null;
/* 051 */     resultIsNull_0 = false;
/* 052 */     if (!resultIsNull_0) {
/* 053 */       boolean isNull_4 = i.isNullAt(1);
/* 054 */       InternalRow value_4 = isNull_4 ?
/* 055 */       null : (i.getStruct(1, 4));
/* 056 */       resultIsNull_0 = isNull_4;
/* 057 */       mutableStateArray_0[0] = value_4;
/* 058 */     }
/* 059 */
/* 060 */     isNull_2 = resultIsNull_0;
/* 061 */     if (!isNull_2) {
/* 062 */
/* 063 */       Object funcResult_0 = null;
/* 064 */       funcResult_0 = value_3.deserialize(mutableStateArray_0[0]);
/* 065 */
/* 066 */       if (funcResult_0 != null) {
/* 067 */         value_2 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 068 */       } else {
/* 069 */         isNull_2 = true;
/* 070 */       }
/* 071 */
/* 072 */
/* 073 */     }
/* 074 */     if (isNull_2) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_2;
/* 078 */     }
/* 079 */
/* 080 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 081 */
/* 082 */     return value_0;
/* 083 */   }
/* 084 */
/* 085 */ }

2025-03-26 04:31:41,818 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_5 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_5);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[2];
/* 039 */
/* 040 */     double value_1 = i.getDouble(0);
/* 041 */     if (false) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.ml.linalg.VectorUDT value_3 = false ?
/* 048 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 049 */     boolean isNull_2 = true;
/* 050 */     org.apache.spark.ml.linalg.Vector value_2 = null;
/* 051 */     resultIsNull_0 = false;
/* 052 */     if (!resultIsNull_0) {
/* 053 */       boolean isNull_4 = i.isNullAt(1);
/* 054 */       InternalRow value_4 = isNull_4 ?
/* 055 */       null : (i.getStruct(1, 4));
/* 056 */       resultIsNull_0 = isNull_4;
/* 057 */       mutableStateArray_0[0] = value_4;
/* 058 */     }
/* 059 */
/* 060 */     isNull_2 = resultIsNull_0;
/* 061 */     if (!isNull_2) {
/* 062 */
/* 063 */       Object funcResult_0 = null;
/* 064 */       funcResult_0 = value_3.deserialize(mutableStateArray_0[0]);
/* 065 */
/* 066 */       if (funcResult_0 != null) {
/* 067 */         value_2 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 068 */       } else {
/* 069 */         isNull_2 = true;
/* 070 */       }
/* 071 */
/* 072 */
/* 073 */     }
/* 074 */     if (isNull_2) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_2;
/* 078 */     }
/* 079 */
/* 080 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 081 */
/* 082 */     return value_0;
/* 083 */   }
/* 084 */
/* 085 */ }

2025-03-26 04:31:41,931 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 130.512687 ms
2025-03-26 04:31:41,948 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1419 bytes result sent to driver
2025-03-26 04:31:41,950 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 3056]
2025-03-26 04:31:41,950 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:41,950 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.ExecutorMetricsPoller: stageTCMP: (0, 0) -> 0
2025-03-26 04:31:41,950 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 1024B
2025-03-26 04:31:41,950 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 2045B
2025-03-26 04:31:41,951 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:41,963 INFO [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2003 ms on slave0 (executor 1) (1/1)
2025-03-26 04:31:41,966 INFO [task-result-getter-0] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-03-26 04:31:41,968 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (take at DeveloperApiExample.scala:127) finished in 2.098 s
2025-03-26 04:31:41,970 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: After removal of stage 0, remaining stages = 0
2025-03-26 04:31:41,979 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-03-26 04:31:41,979 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Killing all running tasks in stage 0: Stage finished
2025-03-26 04:31:41,987 INFO [Driver] org.apache.spark.scheduler.DAGScheduler: Job 0 finished: take at DeveloperApiExample.scala:127, took 2.164577 s
2025-03-26 04:31:42,014 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegressionModel: Input schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}}]}
2025-03-26 04:31:42,023 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegressionModel: Expected output schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{"ml_attr":{"type":"nominal","num_vals":2}}},{"name":"rawPrediction","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":false,"metadata":{"ml_attr":{"num_attrs":2}}}]}
2025-03-26 04:31:42,053 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#14
2025-03-26 04:31:42,074 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'rawPrediction to rawPrediction#19
2025-03-26 04:31:42,094 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#14
2025-03-26 04:31:42,095 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#13
2025-03-26 04:31:42,095 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'prediction to prediction#26
2025-03-26 04:31:42,133 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for newInstance(class org.apache.spark.ml.linalg.VectorUDT).serialize:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private boolean resultIsNull_0;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private org.apache.spark.ml.linalg.Vector[] mutableStateArray_0 = new org.apache.spark.ml.linalg.Vector[1];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */
/* 016 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_1[0], 4);
/* 018 */     mutableStateArray_2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 4);
/* 019 */     mutableStateArray_2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 8);
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public void initialize(int partitionIndex) {
/* 024 */
/* 025 */   }
/* 026 */
/* 027 */   // Scala.Function1 need this
/* 028 */   public java.lang.Object apply(java.lang.Object row) {
/* 029 */     return apply((InternalRow) row);
/* 030 */   }
/* 031 */
/* 032 */   public UnsafeRow apply(InternalRow i) {
/* 033 */     mutableStateArray_1[0].reset();
/* 034 */
/* 035 */
/* 036 */     mutableStateArray_1[0].zeroOutNullBytes();
/* 037 */
/* 038 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 039 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 040 */     boolean isNull_0 = true;
/* 041 */     InternalRow value_0 = null;
/* 042 */     resultIsNull_0 = false;
/* 043 */     if (!resultIsNull_0) {
/* 044 */       boolean isNull_2 = i.isNullAt(0);
/* 045 */       org.apache.spark.ml.linalg.Vector value_2 = isNull_2 ?
/* 046 */       null : ((org.apache.spark.ml.linalg.Vector)i.get(0, null));
/* 047 */       resultIsNull_0 = isNull_2;
/* 048 */       mutableStateArray_0[0] = value_2;
/* 049 */     }
/* 050 */
/* 051 */     isNull_0 = resultIsNull_0;
/* 052 */     if (!isNull_0) {
/* 053 */
/* 054 */       Object funcResult_0 = null;
/* 055 */       funcResult_0 = value_1.serialize(mutableStateArray_0[0]);
/* 056 */
/* 057 */       if (funcResult_0 != null) {
/* 058 */         value_0 = (InternalRow) funcResult_0;
/* 059 */       } else {
/* 060 */         isNull_0 = true;
/* 061 */       }
/* 062 */
/* 063 */
/* 064 */     }
/* 065 */     if (isNull_0) {
/* 066 */       mutableStateArray_1[0].setNullAt(0);
/* 067 */     } else {
/* 068 */       final InternalRow tmpInput_0 = value_0;
/* 069 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 070 */         mutableStateArray_1[0].write(0, (UnsafeRow) tmpInput_0);
/* 071 */       } else {
/* 072 */         // Remember the current cursor so that we can calculate how many bytes are
/* 073 */         // written later.
/* 074 */         final int previousCursor_0 = mutableStateArray_1[0].cursor();
/* 075 */
/* 076 */         mutableStateArray_1[1].resetRowWriter();
/* 077 */
/* 078 */
/* 079 */         mutableStateArray_1[1].write(0, (tmpInput_0.getByte(0)));
/* 080 */
/* 081 */
/* 082 */         if ((tmpInput_0.isNullAt(1))) {
/* 083 */           mutableStateArray_1[1].setNullAt(1);
/* 084 */         } else {
/* 085 */           mutableStateArray_1[1].write(1, (tmpInput_0.getInt(1)));
/* 086 */         }
/* 087 */
/* 088 */
/* 089 */         if ((tmpInput_0.isNullAt(2))) {
/* 090 */           mutableStateArray_1[1].setNullAt(2);
/* 091 */         } else {
/* 092 */           // Remember the current cursor so that we can calculate how many bytes are
/* 093 */           // written later.
/* 094 */           final int previousCursor_1 = mutableStateArray_1[1].cursor();
/* 095 */
/* 096 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 097 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 098 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_1);
/* 099 */           } else {
/* 100 */             final int numElements_0 = tmpInput_1.numElements();
/* 101 */             mutableStateArray_2[0].initialize(numElements_0);
/* 102 */
/* 103 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 104 */               mutableStateArray_2[0].write(index_0, tmpInput_1.getInt(index_0));
/* 105 */             }
/* 106 */           }
/* 107 */
/* 108 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 109 */         }
/* 110 */
/* 111 */
/* 112 */         if ((tmpInput_0.isNullAt(3))) {
/* 113 */           mutableStateArray_1[1].setNullAt(3);
/* 114 */         } else {
/* 115 */           // Remember the current cursor so that we can calculate how many bytes are
/* 116 */           // written later.
/* 117 */           final int previousCursor_2 = mutableStateArray_1[1].cursor();
/* 118 */
/* 119 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 120 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 121 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_2);
/* 122 */           } else {
/* 123 */             final int numElements_1 = tmpInput_2.numElements();
/* 124 */             mutableStateArray_2[1].initialize(numElements_1);
/* 125 */
/* 126 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 127 */               mutableStateArray_2[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 128 */             }
/* 129 */           }
/* 130 */
/* 131 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 132 */         }
/* 133 */
/* 134 */
/* 135 */         mutableStateArray_1[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 136 */       }
/* 137 */     }
/* 138 */     return (mutableStateArray_1[0].getRow());
/* 139 */   }
/* 140 */
/* 141 */
/* 142 */ }

2025-03-26 04:31:42,135 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private boolean resultIsNull_0;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private org.apache.spark.ml.linalg.Vector[] mutableStateArray_0 = new org.apache.spark.ml.linalg.Vector[1];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */
/* 016 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_1[0], 4);
/* 018 */     mutableStateArray_2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 4);
/* 019 */     mutableStateArray_2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 8);
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public void initialize(int partitionIndex) {
/* 024 */
/* 025 */   }
/* 026 */
/* 027 */   // Scala.Function1 need this
/* 028 */   public java.lang.Object apply(java.lang.Object row) {
/* 029 */     return apply((InternalRow) row);
/* 030 */   }
/* 031 */
/* 032 */   public UnsafeRow apply(InternalRow i) {
/* 033 */     mutableStateArray_1[0].reset();
/* 034 */
/* 035 */
/* 036 */     mutableStateArray_1[0].zeroOutNullBytes();
/* 037 */
/* 038 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 039 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 040 */     boolean isNull_0 = true;
/* 041 */     InternalRow value_0 = null;
/* 042 */     resultIsNull_0 = false;
/* 043 */     if (!resultIsNull_0) {
/* 044 */       boolean isNull_2 = i.isNullAt(0);
/* 045 */       org.apache.spark.ml.linalg.Vector value_2 = isNull_2 ?
/* 046 */       null : ((org.apache.spark.ml.linalg.Vector)i.get(0, null));
/* 047 */       resultIsNull_0 = isNull_2;
/* 048 */       mutableStateArray_0[0] = value_2;
/* 049 */     }
/* 050 */
/* 051 */     isNull_0 = resultIsNull_0;
/* 052 */     if (!isNull_0) {
/* 053 */
/* 054 */       Object funcResult_0 = null;
/* 055 */       funcResult_0 = value_1.serialize(mutableStateArray_0[0]);
/* 056 */
/* 057 */       if (funcResult_0 != null) {
/* 058 */         value_0 = (InternalRow) funcResult_0;
/* 059 */       } else {
/* 060 */         isNull_0 = true;
/* 061 */       }
/* 062 */
/* 063 */
/* 064 */     }
/* 065 */     if (isNull_0) {
/* 066 */       mutableStateArray_1[0].setNullAt(0);
/* 067 */     } else {
/* 068 */       final InternalRow tmpInput_0 = value_0;
/* 069 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 070 */         mutableStateArray_1[0].write(0, (UnsafeRow) tmpInput_0);
/* 071 */       } else {
/* 072 */         // Remember the current cursor so that we can calculate how many bytes are
/* 073 */         // written later.
/* 074 */         final int previousCursor_0 = mutableStateArray_1[0].cursor();
/* 075 */
/* 076 */         mutableStateArray_1[1].resetRowWriter();
/* 077 */
/* 078 */
/* 079 */         mutableStateArray_1[1].write(0, (tmpInput_0.getByte(0)));
/* 080 */
/* 081 */
/* 082 */         if ((tmpInput_0.isNullAt(1))) {
/* 083 */           mutableStateArray_1[1].setNullAt(1);
/* 084 */         } else {
/* 085 */           mutableStateArray_1[1].write(1, (tmpInput_0.getInt(1)));
/* 086 */         }
/* 087 */
/* 088 */
/* 089 */         if ((tmpInput_0.isNullAt(2))) {
/* 090 */           mutableStateArray_1[1].setNullAt(2);
/* 091 */         } else {
/* 092 */           // Remember the current cursor so that we can calculate how many bytes are
/* 093 */           // written later.
/* 094 */           final int previousCursor_1 = mutableStateArray_1[1].cursor();
/* 095 */
/* 096 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 097 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 098 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_1);
/* 099 */           } else {
/* 100 */             final int numElements_0 = tmpInput_1.numElements();
/* 101 */             mutableStateArray_2[0].initialize(numElements_0);
/* 102 */
/* 103 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 104 */               mutableStateArray_2[0].write(index_0, tmpInput_1.getInt(index_0));
/* 105 */             }
/* 106 */           }
/* 107 */
/* 108 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 109 */         }
/* 110 */
/* 111 */
/* 112 */         if ((tmpInput_0.isNullAt(3))) {
/* 113 */           mutableStateArray_1[1].setNullAt(3);
/* 114 */         } else {
/* 115 */           // Remember the current cursor so that we can calculate how many bytes are
/* 116 */           // written later.
/* 117 */           final int previousCursor_2 = mutableStateArray_1[1].cursor();
/* 118 */
/* 119 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 120 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 121 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_2);
/* 122 */           } else {
/* 123 */             final int numElements_1 = tmpInput_2.numElements();
/* 124 */             mutableStateArray_2[1].initialize(numElements_1);
/* 125 */
/* 126 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 127 */               mutableStateArray_2[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 128 */             }
/* 129 */           }
/* 130 */
/* 131 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 132 */         }
/* 133 */
/* 134 */
/* 135 */         mutableStateArray_1[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 136 */       }
/* 137 */     }
/* 138 */     return (mutableStateArray_1[0].getRow());
/* 139 */   }
/* 140 */
/* 141 */
/* 142 */ }

2025-03-26 04:31:42,150 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 17.419101 ms
2025-03-26 04:31:42,160 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 026 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 027 */     boolean isNull_0 = true;
/* 028 */     org.apache.spark.ml.linalg.Vector value_0 = null;
/* 029 */     resultIsNull_0 = false;
/* 030 */     if (!resultIsNull_0) {
/* 031 */       boolean isNull_2 = i.isNullAt(0);
/* 032 */       InternalRow value_2 = isNull_2 ?
/* 033 */       null : (i.getStruct(0, 4));
/* 034 */       resultIsNull_0 = isNull_2;
/* 035 */       mutableStateArray_0[0] = value_2;
/* 036 */     }
/* 037 */
/* 038 */     isNull_0 = resultIsNull_0;
/* 039 */     if (!isNull_0) {
/* 040 */
/* 041 */       Object funcResult_0 = null;
/* 042 */       funcResult_0 = value_1.deserialize(mutableStateArray_0[0]);
/* 043 */
/* 044 */       if (funcResult_0 != null) {
/* 045 */         value_0 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 046 */       } else {
/* 047 */         isNull_0 = true;
/* 048 */       }
/* 049 */
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableRow.setNullAt(0);
/* 054 */     } else {
/* 055 */
/* 056 */       mutableRow.update(0, value_0);
/* 057 */     }
/* 058 */
/* 059 */     return mutableRow;
/* 060 */   }
/* 061 */
/* 062 */
/* 063 */ }

2025-03-26 04:31:42,161 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 026 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 027 */     boolean isNull_0 = true;
/* 028 */     org.apache.spark.ml.linalg.Vector value_0 = null;
/* 029 */     resultIsNull_0 = false;
/* 030 */     if (!resultIsNull_0) {
/* 031 */       boolean isNull_2 = i.isNullAt(0);
/* 032 */       InternalRow value_2 = isNull_2 ?
/* 033 */       null : (i.getStruct(0, 4));
/* 034 */       resultIsNull_0 = isNull_2;
/* 035 */       mutableStateArray_0[0] = value_2;
/* 036 */     }
/* 037 */
/* 038 */     isNull_0 = resultIsNull_0;
/* 039 */     if (!isNull_0) {
/* 040 */
/* 041 */       Object funcResult_0 = null;
/* 042 */       funcResult_0 = value_1.deserialize(mutableStateArray_0[0]);
/* 043 */
/* 044 */       if (funcResult_0 != null) {
/* 045 */         value_0 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 046 */       } else {
/* 047 */         isNull_0 = true;
/* 048 */       }
/* 049 */
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableRow.setNullAt(0);
/* 054 */     } else {
/* 055 */
/* 056 */       mutableRow.update(0, value_0);
/* 057 */     }
/* 058 */
/* 059 */     return mutableRow;
/* 060 */   }
/* 061 */
/* 062 */
/* 063 */ }

2025-03-26 04:31:42,168 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 7.325605 ms
2025-03-26 04:31:42,173 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, double, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     double value_0 = i.getDouble(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

2025-03-26 04:31:42,174 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     double value_0 = i.getDouble(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

2025-03-26 04:31:42,178 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 5.233432 ms
2025-03-26 04:31:42,220 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, vector, true],input[1, double, false],input[2, double, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */     writeFields_0_0(i);
/* 035 */     writeFields_0_1(i);
/* 036 */     return (mutableStateArray_0[0].getRow());
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */   private void writeFields_0_1(InternalRow i) {
/* 041 */
/* 042 */     double value_1 = i.getDouble(1);
/* 043 */     mutableStateArray_0[0].write(1, value_1);
/* 044 */
/* 045 */     double value_2 = i.getDouble(2);
/* 046 */     mutableStateArray_0[0].write(2, value_2);
/* 047 */
/* 048 */   }
/* 049 */
/* 050 */
/* 051 */   private void writeFields_0_0(InternalRow i) {
/* 052 */
/* 053 */     boolean isNull_0 = i.isNullAt(0);
/* 054 */     InternalRow value_0 = isNull_0 ?
/* 055 */     null : (i.getStruct(0, 4));
/* 056 */     if (isNull_0) {
/* 057 */       mutableStateArray_0[0].setNullAt(0);
/* 058 */     } else {
/* 059 */       final InternalRow tmpInput_0 = value_0;
/* 060 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 061 */         mutableStateArray_0[0].write(0, (UnsafeRow) tmpInput_0);
/* 062 */       } else {
/* 063 */         // Remember the current cursor so that we can calculate how many bytes are
/* 064 */         // written later.
/* 065 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 066 */
/* 067 */         mutableStateArray_0[1].resetRowWriter();
/* 068 */
/* 069 */
/* 070 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 071 */
/* 072 */
/* 073 */         if ((tmpInput_0.isNullAt(1))) {
/* 074 */           mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 077 */         }
/* 078 */
/* 079 */
/* 080 */         if ((tmpInput_0.isNullAt(2))) {
/* 081 */           mutableStateArray_0[1].setNullAt(2);
/* 082 */         } else {
/* 083 */           // Remember the current cursor so that we can calculate how many bytes are
/* 084 */           // written later.
/* 085 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 086 */
/* 087 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 088 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 089 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 090 */           } else {
/* 091 */             final int numElements_0 = tmpInput_1.numElements();
/* 092 */             mutableStateArray_1[0].initialize(numElements_0);
/* 093 */
/* 094 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 095 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 096 */             }
/* 097 */           }
/* 098 */
/* 099 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 100 */         }
/* 101 */
/* 102 */
/* 103 */         if ((tmpInput_0.isNullAt(3))) {
/* 104 */           mutableStateArray_0[1].setNullAt(3);
/* 105 */         } else {
/* 106 */           // Remember the current cursor so that we can calculate how many bytes are
/* 107 */           // written later.
/* 108 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 109 */
/* 110 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 111 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 112 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 113 */           } else {
/* 114 */             final int numElements_1 = tmpInput_2.numElements();
/* 115 */             mutableStateArray_1[1].initialize(numElements_1);
/* 116 */
/* 117 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 118 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 119 */             }
/* 120 */           }
/* 121 */
/* 122 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 123 */         }
/* 124 */
/* 125 */
/* 126 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 127 */       }
/* 128 */     }
/* 129 */
/* 130 */   }
/* 131 */
/* 132 */ }

2025-03-26 04:31:42,222 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */     writeFields_0_0(i);
/* 035 */     writeFields_0_1(i);
/* 036 */     return (mutableStateArray_0[0].getRow());
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */   private void writeFields_0_1(InternalRow i) {
/* 041 */
/* 042 */     double value_1 = i.getDouble(1);
/* 043 */     mutableStateArray_0[0].write(1, value_1);
/* 044 */
/* 045 */     double value_2 = i.getDouble(2);
/* 046 */     mutableStateArray_0[0].write(2, value_2);
/* 047 */
/* 048 */   }
/* 049 */
/* 050 */
/* 051 */   private void writeFields_0_0(InternalRow i) {
/* 052 */
/* 053 */     boolean isNull_0 = i.isNullAt(0);
/* 054 */     InternalRow value_0 = isNull_0 ?
/* 055 */     null : (i.getStruct(0, 4));
/* 056 */     if (isNull_0) {
/* 057 */       mutableStateArray_0[0].setNullAt(0);
/* 058 */     } else {
/* 059 */       final InternalRow tmpInput_0 = value_0;
/* 060 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 061 */         mutableStateArray_0[0].write(0, (UnsafeRow) tmpInput_0);
/* 062 */       } else {
/* 063 */         // Remember the current cursor so that we can calculate how many bytes are
/* 064 */         // written later.
/* 065 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 066 */
/* 067 */         mutableStateArray_0[1].resetRowWriter();
/* 068 */
/* 069 */
/* 070 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 071 */
/* 072 */
/* 073 */         if ((tmpInput_0.isNullAt(1))) {
/* 074 */           mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 077 */         }
/* 078 */
/* 079 */
/* 080 */         if ((tmpInput_0.isNullAt(2))) {
/* 081 */           mutableStateArray_0[1].setNullAt(2);
/* 082 */         } else {
/* 083 */           // Remember the current cursor so that we can calculate how many bytes are
/* 084 */           // written later.
/* 085 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 086 */
/* 087 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 088 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 089 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 090 */           } else {
/* 091 */             final int numElements_0 = tmpInput_1.numElements();
/* 092 */             mutableStateArray_1[0].initialize(numElements_0);
/* 093 */
/* 094 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 095 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 096 */             }
/* 097 */           }
/* 098 */
/* 099 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 100 */         }
/* 101 */
/* 102 */
/* 103 */         if ((tmpInput_0.isNullAt(3))) {
/* 104 */           mutableStateArray_0[1].setNullAt(3);
/* 105 */         } else {
/* 106 */           // Remember the current cursor so that we can calculate how many bytes are
/* 107 */           // written later.
/* 108 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 109 */
/* 110 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 111 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 112 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 113 */           } else {
/* 114 */             final int numElements_1 = tmpInput_2.numElements();
/* 115 */             mutableStateArray_1[1].initialize(numElements_1);
/* 116 */
/* 117 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 118 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 119 */             }
/* 120 */           }
/* 121 */
/* 122 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 123 */         }
/* 124 */
/* 125 */
/* 126 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 127 */       }
/* 128 */     }
/* 129 */
/* 130 */   }
/* 131 */
/* 132 */ }

2025-03-26 04:31:42,281 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 60.983195 ms
2025-03-26 04:31:42,283 DEBUG [Spark Context Cleaner] org.apache.spark.ContextCleaner: Got cleaning task CleanBroadcast(0)
2025-03-26 04:31:42,284 DEBUG [Spark Context Cleaner] org.apache.spark.ContextCleaner: Cleaning broadcast 0
2025-03-26 04:31:42,284 DEBUG [Spark Context Cleaner] org.apache.spark.broadcast.TorrentBroadcast: Unpersisting TorrentBroadcast 0
2025-03-26 04:31:42,287 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for createexternalrow(newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, input[1, double, false], input[2, double, false], StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true), StructField(label,DoubleType,false), StructField(prediction,DoubleType,false)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_6 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_6);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[3];
/* 039 */
/* 040 */     final org.apache.spark.ml.linalg.VectorUDT value_2 = false ?
/* 041 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 042 */     boolean isNull_1 = true;
/* 043 */     org.apache.spark.ml.linalg.Vector value_1 = null;
/* 044 */     resultIsNull_0 = false;
/* 045 */     if (!resultIsNull_0) {
/* 046 */       boolean isNull_3 = i.isNullAt(0);
/* 047 */       InternalRow value_3 = isNull_3 ?
/* 048 */       null : (i.getStruct(0, 4));
/* 049 */       resultIsNull_0 = isNull_3;
/* 050 */       mutableStateArray_0[0] = value_3;
/* 051 */     }
/* 052 */
/* 053 */     isNull_1 = resultIsNull_0;
/* 054 */     if (!isNull_1) {
/* 055 */
/* 056 */       Object funcResult_0 = null;
/* 057 */       funcResult_0 = value_2.deserialize(mutableStateArray_0[0]);
/* 058 */
/* 059 */       if (funcResult_0 != null) {
/* 060 */         value_1 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 061 */       } else {
/* 062 */         isNull_1 = true;
/* 063 */       }
/* 064 */
/* 065 */
/* 066 */     }
/* 067 */     if (isNull_1) {
/* 068 */       values_0[0] = null;
/* 069 */     } else {
/* 070 */       values_0[0] = value_1;
/* 071 */     }
/* 072 */
/* 073 */     double value_4 = i.getDouble(1);
/* 074 */     if (false) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_4;
/* 078 */     }
/* 079 */
/* 080 */     double value_5 = i.getDouble(2);
/* 081 */     if (false) {
/* 082 */       values_0[2] = null;
/* 083 */     } else {
/* 084 */       values_0[2] = value_5;
/* 085 */     }
/* 086 */
/* 087 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 088 */
/* 089 */     return value_0;
/* 090 */   }
/* 091 */
/* 092 */ }

2025-03-26 04:31:42,288 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_6 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_6);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[3];
/* 039 */
/* 040 */     final org.apache.spark.ml.linalg.VectorUDT value_2 = false ?
/* 041 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 042 */     boolean isNull_1 = true;
/* 043 */     org.apache.spark.ml.linalg.Vector value_1 = null;
/* 044 */     resultIsNull_0 = false;
/* 045 */     if (!resultIsNull_0) {
/* 046 */       boolean isNull_3 = i.isNullAt(0);
/* 047 */       InternalRow value_3 = isNull_3 ?
/* 048 */       null : (i.getStruct(0, 4));
/* 049 */       resultIsNull_0 = isNull_3;
/* 050 */       mutableStateArray_0[0] = value_3;
/* 051 */     }
/* 052 */
/* 053 */     isNull_1 = resultIsNull_0;
/* 054 */     if (!isNull_1) {
/* 055 */
/* 056 */       Object funcResult_0 = null;
/* 057 */       funcResult_0 = value_2.deserialize(mutableStateArray_0[0]);
/* 058 */
/* 059 */       if (funcResult_0 != null) {
/* 060 */         value_1 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 061 */       } else {
/* 062 */         isNull_1 = true;
/* 063 */       }
/* 064 */
/* 065 */
/* 066 */     }
/* 067 */     if (isNull_1) {
/* 068 */       values_0[0] = null;
/* 069 */     } else {
/* 070 */       values_0[0] = value_1;
/* 071 */     }
/* 072 */
/* 073 */     double value_4 = i.getDouble(1);
/* 074 */     if (false) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_4;
/* 078 */     }
/* 079 */
/* 080 */     double value_5 = i.getDouble(2);
/* 081 */     if (false) {
/* 082 */       values_0[2] = null;
/* 083 */     } else {
/* 084 */       values_0[2] = value_5;
/* 085 */     }
/* 086 */
/* 087 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 088 */
/* 089 */     return value_0;
/* 090 */   }
/* 091 */
/* 092 */ }

2025-03-26 04:31:42,294 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 6.534923 ms
2025-03-26 04:31:42,296 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 161]
2025-03-26 04:31:42,296 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:42,311 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 161]
2025-03-26 04:31:42,311 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:42,312 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 161]
2025-03-26 04:31:42,312 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:42,312 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 182B
2025-03-26 04:31:42,316 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:42,317 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ 102B
2025-03-26 04:31:42,321 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManagerStorageEndpoint: removing broadcast 0
2025-03-26 04:31:42,321 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManager: Removing broadcast 0
2025-03-26 04:31:42,323 INFO [Driver] org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://slave0:43989
2025-03-26 04:31:42,323 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManager: Removing block broadcast_0_piece0
2025-03-26 04:31:42,324 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:42,330 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 of size 9662 dropped from memory (free 2101954945)
2025-03-26 04:31:42,331 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, slave0, 46565, None)
2025-03-26 04:31:42,331 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ 102B
2025-03-26 04:31:42,332 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on slave0:46565 in memory (size: 9.4 KiB, free: 2004.6 MiB)
2025-03-26 04:31:42,333 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManagerStorageEndpoint: removing broadcast 0
2025-03-26 04:31:42,333 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManager: Removing broadcast 0
2025-03-26 04:31:42,335 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:42,336 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
2025-03-26 04:31:42,336 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManager: Told master about block broadcast_0_piece0
2025-03-26 04:31:42,336 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManager: Removing block broadcast_0
2025-03-26 04:31:42,337 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 of size 20504 dropped from memory (free 2101975449)
2025-03-26 04:31:42,337 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManager: Removing block broadcast_0_piece0
2025-03-26 04:31:42,339 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 of size 9662 dropped from memory (free 956595380)
2025-03-26 04:31:42,339 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 173]
2025-03-26 04:31:42,339 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:42,340 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 194B
2025-03-26 04:31:42,340 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:42,340 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(1, slave0, 38599, None)
2025-03-26 04:31:42,340 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on slave0:38599 in memory (size: 9.4 KiB, free: 912.3 MiB)
2025-03-26 04:31:42,346 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:31:42,346 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:42,346 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 68B
2025-03-26 04:31:42,346 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
2025-03-26 04:31:42,346 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManager: Told master about block broadcast_0_piece0
2025-03-26 04:31:42,346 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:42,347 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.BlockManager: Removing block broadcast_0
2025-03-26 04:31:42,347 DEBUG [block-manager-storage-async-thread-pool-0] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 of size 20504 dropped from memory (free 956615884)
2025-03-26 04:31:42,349 DEBUG [block-manager-storage-async-thread-pool-2] org.apache.spark.storage.BlockManagerStorageEndpoint: Done removing broadcast 0, response is 0
2025-03-26 04:31:42,350 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 81]
2025-03-26 04:31:42,350 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] FLUSH
2025-03-26 04:31:42,350 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ 102B
2025-03-26 04:31:42,352 DEBUG [block-manager-storage-async-thread-pool-2] org.apache.spark.storage.BlockManagerStorageEndpoint: Sent response: 0 to slave0:36275
2025-03-26 04:31:42,355 DEBUG [block-manager-storage-async-thread-pool-2] org.apache.spark.storage.BlockManagerStorageEndpoint: Done removing broadcast 0, response is 0
2025-03-26 04:31:42,358 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:42,359 DEBUG [block-manager-storage-async-thread-pool-2] org.apache.spark.storage.BlockManagerStorageEndpoint: Sent response: 0 to slave0:36275
2025-03-26 04:31:42,360 DEBUG [Spark Context Cleaner] org.apache.spark.ContextCleaner: Cleaned broadcast 0
2025-03-26 04:31:42,367 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend: Shutting down all executors
2025-03-26 04:31:42,368 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
2025-03-26 04:31:42,369 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-26 04:31:42,369 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] FLUSH
2025-03-26 04:31:42,370 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-26 04:31:42,370 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] FLUSH
2025-03-26 04:31:42,370 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-26 04:31:42,370 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] FLUSH
2025-03-26 04:31:42,370 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ 135B
2025-03-26 04:31:42,372 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] READ COMPLETE
2025-03-26 04:31:42,373 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Driver commanded a shutdown
2025-03-26 04:31:42,392 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 - R:/172.20.1.16:50336] READ COMPLETE
2025-03-26 04:31:42,392 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 ! R:/172.20.1.16:50336] INACTIVE
2025-03-26 04:31:42,392 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2bd43c68, L:/172.20.1.15:36275 ! R:/172.20.1.16:50336] UNREGISTERED
2025-03-26 04:31:42,392 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 - R:/172.20.1.17:55596] READ COMPLETE
2025-03-26 04:31:42,392 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 ! R:/172.20.1.17:55596] INACTIVE
2025-03-26 04:31:42,392 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x98f5d6d3, L:/172.20.1.15:36275 ! R:/172.20.1.17:55596] UNREGISTERED
2025-03-26 04:31:42,395 INFO [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-03-26 04:31:42,400 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 ! R:/172.20.1.15:60102] INACTIVE
2025-03-26 04:31:42,400 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0x02d2346a, L:/172.20.1.15:46565 ! R:/172.20.1.15:60102] UNREGISTERED
2025-03-26 04:31:42,401 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 - R:slave0/172.20.1.15:46565] READ COMPLETE
2025-03-26 04:31:42,401 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 ! R:slave0/172.20.1.15:46565] INACTIVE
2025-03-26 04:31:42,401 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x7c93128e, L:/172.20.1.15:60102 ! R:slave0/172.20.1.15:46565] UNREGISTERED
2025-03-26 04:31:42,442 INFO [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2025-03-26 04:31:42,442 INFO [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.storage.BlockManager: BlockManager stopped
2025-03-26 04:31:42,445 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 - R:slave0/172.20.1.15:36275] CLOSE
2025-03-26 04:31:42,446 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 ! R:slave0/172.20.1.15:36275] INACTIVE
2025-03-26 04:31:42,446 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x1cb85fac, L:/172.20.1.15:60410 ! R:slave0/172.20.1.15:36275] UNREGISTERED
2025-03-26 04:31:42,446 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 - R:/172.20.1.15:60410] READ COMPLETE
2025-03-26 04:31:42,446 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 ! R:/172.20.1.15:60410] INACTIVE
2025-03-26 04:31:42,446 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x55cc06bb, L:/172.20.1.15:36275 ! R:/172.20.1.15:60410] UNREGISTERED
2025-03-26 04:31:42,452 INFO [Driver] org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2025-03-26 04:31:42,452 INFO [Driver] org.apache.spark.storage.BlockManager: BlockManager stopped
2025-03-26 04:31:42,456 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2025-03-26 04:31:42,459 INFO [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-03-26 04:31:42,461 DEBUG [rpc-server-4-1] io.netty.buffer.PoolThreadCache: Freed 3 thread-local buffer(s) from thread: rpc-server-4-1
2025-03-26 04:31:42,472 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2025-03-26 04:31:42,473 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: Completed shutdown in 0.010 seconds; Timeouts: 0
2025-03-26 04:31:42,494 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: ShutdownHookManager completed shutdown.
2025-03-26 04:31:42,510 INFO [Driver] org.apache.spark.SparkContext: Successfully stopped SparkContext
2025-03-26 04:31:42,511 INFO [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
2025-03-26 04:31:42,514 DEBUG [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: shutting down reporter thread
2025-03-26 04:31:42,515 DEBUG [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: Done running user class
2025-03-26 04:31:42,528 INFO [shutdown-hook-0] org.apache.spark.deploy.yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
2025-03-26 04:31:42,531 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #13 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.finishApplicationMaster
2025-03-26 04:31:42,538 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #13
2025-03-26 04:31:42,542 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: finishApplicationMaster took 11ms
2025-03-26 04:31:42,543 INFO [shutdown-hook-0] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
2025-03-26 04:31:42,644 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root sending #14 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.finishApplicationMaster
2025-03-26 04:31:42,644 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:8030 from root got value #14
2025-03-26 04:31:42,645 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: finishApplicationMaster took 2ms
2025-03-26 04:31:42,645 DEBUG [shutdown-hook-0] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl entered state STOPPED
2025-03-26 04:31:42,645 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: stopping client from cache: Client-13d7b2110f204c45b479e64949a5ec25
2025-03-26 04:31:42,645 INFO [shutdown-hook-0] org.apache.spark.deploy.yarn.ApplicationMaster: Deleting staging directory hdfs://master:9000/user/root/.sparkStaging/application_1742963439991_0001
2025-03-26 04:31:42,647 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:31:42,647 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.14:9000
2025-03-26 04:31:42,647 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.14:9000
2025-03-26 04:31:42,647 DEBUG [shutdown-hook-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@4f593297]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy36.delete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:655)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy37.delete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1662)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:992)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:989)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:999)
	at org.apache.spark.deploy.yarn.ApplicationMaster.cleanupStagingDir(ApplicationMaster.scala:686)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$run$2(ApplicationMaster.scala:265)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:31:42,647 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:31:42,648 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
2025-03-26 04:31:42,648 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one
2025-03-26 04:31:42,648 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Use SIMPLE authentication for protocol ClientNamenodeProtocolPB
2025-03-26 04:31:42,648 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
auths {
  method: "SIMPLE"
  mechanism: ""
}

2025-03-26 04:31:42,648 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:9000 from root: starting, having connections 2
2025-03-26 04:31:42,648 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:9000 from root sending #15 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete
2025-03-26 04:31:42,649 DEBUG [IPC Client (1244880808) connection to master/172.20.1.14:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.14:9000 from root got value #15
2025-03-26 04:31:42,649 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: delete took 3ms
2025-03-26 04:31:42,650 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2025-03-26 04:31:42,651 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Deleting directory /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963439991_0001/spark-976d4481-360f-4848-a612-f42508dd3946
2025-03-26 04:31:42,652 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.fs.FilterFileSystem.close(FilterFileSystem.java:529)); Key: (root (auth:SIMPLE))@file://; URI: file:///; Object Identity Hash: 8537e7
2025-03-26 04:31:42,652 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.fs.RawLocalFileSystem.close(RawLocalFileSystem.java:759)); Key: null; URI: file:///; Object Identity Hash: 22c831db
2025-03-26 04:31:42,652 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1518)); Key: (root (auth:SIMPLE))@hdfs://master:9000; URI: hdfs://master:9000; Object Identity Hash: 37a494f7
2025-03-26 04:31:42,652 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: stopping client from cache: Client-13d7b2110f204c45b479e64949a5ec25
2025-03-26 04:31:42,652 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: Completed shutdown in 0.136 seconds; Timeouts: 0
2025-03-26 04:31:42,661 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: ShutdownHookManager completed shutdown.
