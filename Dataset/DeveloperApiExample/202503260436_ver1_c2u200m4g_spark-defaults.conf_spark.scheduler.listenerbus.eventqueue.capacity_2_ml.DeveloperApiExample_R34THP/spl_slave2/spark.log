program=ml.DeveloperApiExample
SPARKLORD_MODE=CONFIG_INJECTION
cpu_cores=2
cpu_util=200
memory=4g
config_file_name=spark-defaults.conf
config_key=spark.scheduler.listenerbus.eventqueue.capacity
config_value=2

2025-03-26 04:37:59,314 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for TERM
2025-03-26 04:37:59,319 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for HUP
2025-03-26 04:37:59,319 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for INT
2025-03-26 04:37:59,532 DEBUG [main] org.apache.hadoop.util.Shell: setsid exited with exit code 0
2025-03-26 04:37:59,679 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-26 04:37:59,680 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-26 04:37:59,682 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-26 04:37:59,682 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-26 04:37:59,683 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-26 04:37:59,743 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
2025-03-26 04:37:59,749 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
2025-03-26 04:37:59,749 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
2025-03-26 04:37:59,749 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
2025-03-26 04:37:59,750 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
2025-03-26 04:37:59,750 DEBUG [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
2025-03-26 04:37:59,764 DEBUG [main] org.apache.hadoop.security.SecurityUtil: Setting hadoop.security.token.service.use_ip to true
2025-03-26 04:37:59,768 DEBUG [main] org.apache.hadoop.security.Groups:  Creating new Groups object
2025-03-26 04:37:59,769 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
2025-03-26 04:37:59,769 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2025-03-26 04:37:59,769 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-26 04:37:59,769 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-03-26 04:37:59,769 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Falling back to shell based
2025-03-26 04:37:59,770 DEBUG [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2025-03-26 04:37:59,811 DEBUG [main] org.apache.hadoop.security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2025-03-26 04:37:59,887 DEBUG [main] org.apache.spark.deploy.SparkHadoopUtil: creating UGI for user: root
2025-03-26 04:37:59,890 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Hadoop login
2025-03-26 04:37:59,891 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: hadoop login commit
2025-03-26 04:37:59,892 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using local user: UnixPrincipal: root
2025-03-26 04:37:59,893 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using user: "UnixPrincipal: root" with name: root
2025-03-26 04:37:59,893 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: User entry: "root"
2025-03-26 04:37:59,894 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Reading credentials from location /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000001/container_tokens
2025-03-26 04:37:59,898 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Loaded 1 tokens from /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000001/container_tokens
2025-03-26 04:37:59,898 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: UGI loginUser: root (auth:SIMPLE)
2025-03-26 04:37:59,898 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3@1522d8a0]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-26 04:37:59,905 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1742963823806_0001_000001
2025-03-26 04:37:59,907 DEBUG [main] org.apache.spark.util.ShutdownHookManager: Adding shutdown hook
2025-03-26 04:37:59,927 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Starting the user application in a separate Thread
2025-03-26 04:37:59,928 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Waiting for spark context initialization...
2025-03-26 04:37:59,973 INFO [Driver] org.apache.spark.SparkContext: Running Spark version 3.3.2
2025-03-26 04:37:59,992 INFO [Driver] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-26 04:37:59,992 INFO [Driver] org.apache.spark.resource.ResourceUtils: No custom resources configured for spark.driver.
2025-03-26 04:37:59,992 INFO [Driver] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-26 04:37:59,992 INFO [Driver] org.apache.spark.SparkContext: Submitted application: DeveloperApiExample
2025-03-26 04:38:00,006 INFO [Driver] org.apache.spark.resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-03-26 04:38:00,010 INFO [Driver] org.apache.spark.resource.ResourceProfile: Limiting resource is cpus at 2 tasks per executor
2025-03-26 04:38:00,012 INFO [Driver] org.apache.spark.resource.ResourceProfileManager: Added ResourceProfile id: 0
2025-03-26 04:38:00,040 INFO [Driver] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-26 04:38:00,040 INFO [Driver] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-26 04:38:00,040 INFO [Driver] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-26 04:38:00,040 INFO [Driver] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-26 04:38:00,040 INFO [Driver] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-26 04:38:00,072 DEBUG [Driver] io.netty.util.internal.logging.InternalLoggerFactory: Using SLF4J as the default logging framework
2025-03-26 04:38:00,076 DEBUG [Driver] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2025-03-26 04:38:00,076 DEBUG [Driver] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2025-03-26 04:38:00,084 DEBUG [Driver] io.netty.channel.MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
2025-03-26 04:38:00,111 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: -Dio.netty.noUnsafe: false
2025-03-26 04:38:00,111 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: Java version: 8
2025-03-26 04:38:00,113 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
2025-03-26 04:38:00,113 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.copyMemory: available
2025-03-26 04:38:00,114 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.Buffer.address: available
2025-03-26 04:38:00,114 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: direct buffer constructor: available
2025-03-26 04:38:00,114 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.Bits.unaligned: available, true
2025-03-26 04:38:00,114 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2025-03-26 04:38:00,114 DEBUG [Driver] io.netty.util.internal.PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
2025-03-26 04:38:00,115 DEBUG [Driver] io.netty.util.internal.PlatformDependent: sun.misc.Unsafe: available
2025-03-26 04:38:00,115 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.tmpdir: /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000001/tmp (java.io.tmpdir)
2025-03-26 04:38:00,115 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
2025-03-26 04:38:00,116 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.maxDirectMemory: 3817865216 bytes
2025-03-26 04:38:00,116 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
2025-03-26 04:38:00,116 DEBUG [Driver] io.netty.util.internal.CleanerJava6: java.nio.ByteBuffer.cleaner(): available
2025-03-26 04:38:00,117 DEBUG [Driver] io.netty.util.internal.PlatformDependent: -Dio.netty.noPreferDirect: false
2025-03-26 04:38:00,117 DEBUG [Driver] io.netty.channel.nio.NioEventLoop: -Dio.netty.noKeySetOptimization: false
2025-03-26 04:38:00,117 DEBUG [Driver] io.netty.channel.nio.NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
2025-03-26 04:38:00,122 DEBUG [Driver] io.netty.util.internal.PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
2025-03-26 04:38:00,132 DEBUG [Driver] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
2025-03-26 04:38:00,133 DEBUG [Driver] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
2025-03-26 04:38:00,134 DEBUG [Driver] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2025-03-26 04:38:00,150 DEBUG [Driver] io.netty.channel.DefaultChannelId: -Dio.netty.processId: 1503 (auto-detected)
2025-03-26 04:38:00,151 DEBUG [Driver] io.netty.util.NetUtil: -Djava.net.preferIPv4Stack: false
2025-03-26 04:38:00,151 DEBUG [Driver] io.netty.util.NetUtil: -Djava.net.preferIPv6Addresses: false
2025-03-26 04:38:00,152 DEBUG [Driver] io.netty.util.NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
2025-03-26 04:38:00,152 DEBUG [Driver] io.netty.util.NetUtil: /proc/sys/net/core/somaxconn: 4096
2025-03-26 04:38:00,153 DEBUG [Driver] io.netty.channel.DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:14:01:0d (auto-detected)
2025-03-26 04:38:00,163 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.allocator.type: pooled
2025-03-26 04:38:00,163 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0
2025-03-26 04:38:00,163 DEBUG [Driver] io.netty.buffer.ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
2025-03-26 04:38:00,170 DEBUG [Driver] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 34623
2025-03-26 04:38:00,177 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'sparkDriver' on port 34623.
2025-03-26 04:38:00,178 DEBUG [Driver] org.apache.spark.SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
2025-03-26 04:38:00,195 INFO [Driver] org.apache.spark.SparkEnv: Registering MapOutputTracker
2025-03-26 04:38:00,196 DEBUG [Driver] org.apache.spark.MapOutputTrackerMasterEndpoint: init
2025-03-26 04:38:00,221 INFO [Driver] org.apache.spark.SparkEnv: Registering BlockManagerMaster
2025-03-26 04:38:00,233 INFO [Driver] org.apache.spark.storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-03-26 04:38:00,234 INFO [Driver] org.apache.spark.storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-03-26 04:38:00,269 INFO [Driver] org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
2025-03-26 04:38:00,296 INFO [Driver] org.apache.spark.storage.DiskBlockManager: Created local directory at /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/blockmgr-248d1c3b-72ee-471d-aab1-56c56aa705c6
2025-03-26 04:38:00,298 DEBUG [Driver] org.apache.spark.storage.DiskBlockManager: Adding shutdown hook
2025-03-26 04:38:00,318 INFO [Driver] org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 2004.6 MiB
2025-03-26 04:38:00,369 INFO [Driver] org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
2025-03-26 04:38:00,370 DEBUG [Driver] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: init
2025-03-26 04:38:00,380 DEBUG [Driver] org.apache.spark.SecurityManager: Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
2025-03-26 04:38:00,560 DEBUG [Driver] org.apache.spark.ui.JettyUtils: Using requestHeaderSize: 8192
2025-03-26 04:38:00,581 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 45105.
2025-03-26 04:38:00,583 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,648 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Created YarnClusterScheduler
2025-03-26 04:38:00,697 DEBUG [Driver] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 40525
2025-03-26 04:38:00,697 INFO [Driver] org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40525.
2025-03-26 04:38:00,697 INFO [Driver] org.apache.spark.network.netty.NettyBlockTransferService: Server created on slave2:40525
2025-03-26 04:38:00,698 INFO [Driver] org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-03-26 04:38:00,702 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, slave2, 40525, None)
2025-03-26 04:38:00,704 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave2
2025-03-26 04:38:00,704 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave2:40525 with 2004.6 MiB RAM, BlockManagerId(driver, slave2, 40525, None)
2025-03-26 04:38:00,706 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, slave2, 40525, None)
2025-03-26 04:38:00,707 INFO [Driver] org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, slave2, 40525, None)
2025-03-26 04:38:00,819 DEBUG [Driver] org.apache.spark.util.YarnContainerInfoHelper: Base URL for logs: http://slave2:8042/node/containerlogs/container_1742963823806_0001_01_000001/root
2025-03-26 04:38:00,837 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,838 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,839 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,841 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,842 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,843 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,844 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,846 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,847 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,848 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,849 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,849 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,850 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,851 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,852 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,853 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,853 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,854 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,855 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,856 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,857 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,868 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,869 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,873 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,874 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,878 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:00,879 DEBUG [Driver] org.apache.spark.SparkContext: Adding shutdown hook
2025-03-26 04:38:00,884 DEBUG [main] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl entered state INITED
2025-03-26 04:38:00,890 INFO [main] org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master/172.20.1.10:8030
2025-03-26 04:38:00,890 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.RMProxy$1@12dae582]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.RMProxy.getProxy(RMProxy.java:145)
	at org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider.init(DefaultNoHARMFailoverProxyProvider.java:65)
	at org.apache.hadoop.yarn.client.RMProxy.createNonHaRMFailoverProxyProvider(RMProxy.java:172)
	at org.apache.hadoop.yarn.client.RMProxy.newProxyInstance(RMProxy.java:132)
	at org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:103)
	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:73)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.serviceStart(AMRMClientImpl.java:193)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:63)
	at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:440)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:518)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-26 04:38:00,890 DEBUG [main] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:38:00,891 DEBUG [main] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ApplicationMasterProtocol
2025-03-26 04:38:00,900 DEBUG [main] org.apache.hadoop.ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker@2611b9a3
2025-03-26 04:38:00,904 DEBUG [main] org.apache.hadoop.ipc.Client: getting client out of cache: Client-dd5ed27233bf44d986fbe0731c03d648
2025-03-26 04:38:00,915 DEBUG [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl is started
2025-03-26 04:38:00,916 INFO [main] org.apache.spark.deploy.yarn.YarnRMClient: Registering the ApplicationMaster
2025-03-26 04:38:00,946 DEBUG [main] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:38:00,946 DEBUG [main] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.10:8030
2025-03-26 04:38:00,946 DEBUG [main] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.10:8030
2025-03-26 04:38:00,949 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@3ebff828]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy31.registerApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.registerApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:108)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy32.registerApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:247)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:234)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:214)
	at org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:72)
	at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:440)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:518)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-26 04:38:00,999 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:38:01,005 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB info:org.apache.hadoop.yarn.security.SchedulerSecurityInfo$1@2a76b80a
2025-03-26 04:38:01,006 DEBUG [main] org.apache.hadoop.yarn.security.AMRMTokenSelector: Looking for a token with service 172.20.1.10:8030
2025-03-26 04:38:01,006 DEBUG [main] org.apache.hadoop.yarn.security.AMRMTokenSelector: Token kind is YARN_AM_RM_TOKEN and the token's service name is 172.20.1.10:8030
2025-03-26 04:38:01,008 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-26 04:38:01,009 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ApplicationMasterProtocolPB
2025-03-26 04:38:01,010 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEL6p9YXdMhABEPaLsq0E
2025-03-26 04:38:01,010 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-26 04:38:01,010 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-26 04:38:01,012 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEL6p9YXdMhABEPaLsq0E\",realm=\"default\",nonce=\"1VkSrUiUAwVy9Yt+95yM4Hno7S4XaEfIxj9Lvv3D\",nc=00000001,cnonce=\"ezBok0MA0G5mcaRv6lmWoAQKgBpbgaLSfpAQc/F3\",digest-uri=\"/default\",maxbuf=65536,response=4af6444bb71d89296421a68559724226,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-26 04:38:01,014 DEBUG [main] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-26 04:38:01,017 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root: starting, having connections 1
2025-03-26 04:38:01,018 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #0 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster
2025-03-26 04:38:01,035 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #0
2025-03-26 04:38:01,035 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: registerApplicationMaster took 114ms
2025-03-26 04:38:01,046 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Preparing Local resources
2025-03-26 04:38:01,093 DEBUG [main] org.apache.hadoop.fs.FileSystem: Starting: Acquiring creator semaphore for hdfs://master:9000/user/root/.sparkStaging/application_1742963823806_0001/__spark_conf__.zip
2025-03-26 04:38:01,093 DEBUG [main] org.apache.hadoop.fs.FileSystem: Acquiring creator semaphore for hdfs://master:9000/user/root/.sparkStaging/application_1742963823806_0001/__spark_conf__.zip: duration 0:00.001s
2025-03-26 04:38:01,093 DEBUG [main] org.apache.hadoop.fs.FileSystem: Starting: Creating FS hdfs://master:9000/user/root/.sparkStaging/application_1742963823806_0001/__spark_conf__.zip
2025-03-26 04:38:01,093 DEBUG [main] org.apache.hadoop.fs.FileSystem: Loading filesystems
2025-03-26 04:38:01,099 DEBUG [main] org.apache.hadoop.fs.FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:38:01,101 DEBUG [main] org.apache.hadoop.fs.FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:38:01,102 DEBUG [main] org.apache.hadoop.fs.FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:38:01,103 DEBUG [main] org.apache.hadoop.fs.FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:38:01,103 DEBUG [main] org.apache.hadoop.fs.FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:38:01,108 DEBUG [main] org.apache.hadoop.fs.FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:38:01,112 DEBUG [main] org.apache.hadoop.fs.FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:38:01,112 DEBUG [main] org.apache.hadoop.fs.FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hadoop-client-api-3.3.2.jar
2025-03-26 04:38:01,113 DEBUG [main] org.apache.hadoop.fs.FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hive-exec-2.3.9-core.jar
2025-03-26 04:38:01,113 DEBUG [main] org.apache.hadoop.fs.FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /data/tmp/nm-local-dir/usercache/root/filecache/10/__spark_libs__9017572008495746107.zip/hive-exec-2.3.9-core.jar
2025-03-26 04:38:01,114 DEBUG [main] org.apache.hadoop.fs.FileSystem: Looking for FS supporting hdfs
2025-03-26 04:38:01,114 DEBUG [main] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.hdfs.impl
2025-03-26 04:38:01,123 DEBUG [main] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:38:01,123 DEBUG [main] org.apache.hadoop.fs.FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem
2025-03-26 04:38:01,135 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.use.legacy.blockreader.local = false
2025-03-26 04:38:01,135 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.read.shortcircuit = false
2025-03-26 04:38:01,135 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.domain.socket.data.traffic = false
2025-03-26 04:38:01,135 DEBUG [main] org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.domain.socket.path = 
2025-03-26 04:38:01,138 DEBUG [main] org.apache.hadoop.hdfs.DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2025-03-26 04:38:01,141 DEBUG [main] org.apache.hadoop.io.retry.RetryUtils: multipleLinearRandomRetry = null
2025-03-26 04:38:01,143 DEBUG [main] org.apache.hadoop.ipc.Client: getting client out of cache: Client-dd5ed27233bf44d986fbe0731c03d648
2025-03-26 04:38:01,290 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
2025-03-26 04:38:01,293 DEBUG [main] org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
2025-03-26 04:38:01,295 DEBUG [main] org.apache.hadoop.fs.FileSystem: Creating FS hdfs://master:9000/user/root/.sparkStaging/application_1742963823806_0001/__spark_conf__.zip: duration 0:00.202s
2025-03-26 04:38:01,297 DEBUG [main] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:38:01,298 DEBUG [main] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.10:9000
2025-03-26 04:38:01,298 DEBUG [main] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.10:9000
2025-03-26 04:38:01,298 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@322803db]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy36.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy37.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$prepareLocalResources$4(ApplicationMaster.scala:200)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$prepareLocalResources$4$adapted(ApplicationMaster.scala:197)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.deploy.yarn.ApplicationMaster.prepareLocalResources(ApplicationMaster.scala:197)
	at org.apache.spark.deploy.yarn.ApplicationMaster.createAllocator(ApplicationMaster.scala:463)
	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:523)
	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:275)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:926)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:925)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:925)
	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)
2025-03-26 04:38:01,298 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:38:01,299 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
2025-03-26 04:38:01,299 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one
2025-03-26 04:38:01,299 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Use SIMPLE authentication for protocol ClientNamenodeProtocolPB
2025-03-26 04:38:01,299 DEBUG [main] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
auths {
  method: "SIMPLE"
  mechanism: ""
}

2025-03-26 04:38:01,300 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2025-03-26 04:38:01,300 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root: starting, having connections 2
2025-03-26 04:38:01,301 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root got value #1
2025-03-26 04:38:01,301 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: getFileInfo took 4ms
2025-03-26 04:38:01,321 DEBUG [main] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:38:01,343 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: 
===============================================================================
Default YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
    SPARK_YARN_STAGING_DIR -> hdfs://master:9000/user/root/.sparkStaging/application_1742963823806_0001
    SPARK_USER -> root

  command:
    {{JAVA_HOME}}/bin/java \ 
      -server \ 
      -Xmx2048m \ 
      '-XX:+IgnoreUnrecognizedVMOptions' \ 
      '--add-opens=java.base/java.lang=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.io=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.net=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.nio=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' \ 
      '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.security.action=ALL-UNNAMED' \ 
      '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED' \ 
      '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED' \ 
      -Djava.io.tmpdir={{PWD}}/tmp \ 
      '-Dspark.ui.port=0' \ 
      '-Dspark.driver.port=34623' \ 
      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
      -XX:OnOutOfMemoryError='kill %p' \ 
      org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \ 
      --driver-url \ 
      spark://CoarseGrainedScheduler@slave2:34623 \ 
      --executor-id \ 
      <executorId> \ 
      --hostname \ 
      <hostname> \ 
      --cores \ 
      2 \ 
      --app-id \ 
      application_1742963823806_0001 \ 
      --resourceProfileId \ 
      0 \ 
      1><LOG_DIR>/stdout \ 
      2><LOG_DIR>/stderr

  resources:
    __app__.jar -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742963823806_0001/scopt_2.12-3.7.1.jar" } size: 78803 timestamp: 1742963876044 type: FILE visibility: PRIVATE
    __spark_libs__ -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742963823806_0001/__spark_libs__9017572008495746107.zip" } size: 301733843 timestamp: 1742963875968 type: ARCHIVE visibility: PRIVATE
    __spark_conf__ -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742963823806_0001/__spark_conf__.zip" } size: 947004 timestamp: 1742963876198 type: ARCHIVE visibility: PRIVATE
    spark-examples_2.12-3.3.2.jar -> resource { scheme: "hdfs" host: "master" port: 9000 file: "/user/root/.sparkStaging/application_1742963823806_0001/spark-examples_2.12-3.3.2.jar" } size: 1567446 timestamp: 1742963876075 type: FILE visibility: PRIVATE

===============================================================================
2025-03-26 04:38:01,367 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Resource profile 0 doesn't exist, adding it
2025-03-26 04:38:01,388 INFO [main] org.apache.hadoop.conf.Configuration: resource-types.xml not found
2025-03-26 04:38:01,388 INFO [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
2025-03-26 04:38:01,392 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE
2025-03-26 04:38:01,392 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE
2025-03-26 04:38:01,392 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.memory-mb.minimum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.minimum-allocation-mb'
2025-03-26 04:38:01,392 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.memory-mb.maximum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.maximum-allocation-mb'
2025-03-26 04:38:01,392 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.vcores.minimum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.minimum-allocation-vcores'
2025-03-26 04:38:01,392 DEBUG [main] org.apache.hadoop.yarn.util.resource.ResourceUtils: Mandatory Resource 'yarn.resource-types.vcores.maximum-allocation' is not configured in resource-types config file. Setting allocation specified using 'yarn.scheduler.maximum-allocation-vcores'
2025-03-26 04:38:01,393 DEBUG [main] org.apache.spark.deploy.yarn.ResourceRequestHelper: Custom resources requested: Map()
2025-03-26 04:38:01,394 DEBUG [main] org.apache.spark.deploy.yarn.YarnAllocator: Created resource capability: <memory:2432, vCores:2>
2025-03-26 04:38:01,396 INFO [dispatcher-event-loop-0] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@slave2:34623)
2025-03-26 04:38:01,397 DEBUG [main] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 0, executorsStarting: 0
2025-03-26 04:38:01,401 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Will request 3 executor container(s) for  ResourceProfile Id: 0, each with 2 core(s) and 2432 MB memory. with custom resources: <memory:2432, vCores:2>
2025-03-26 04:38:01,410 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added priority=0
2025-03-26 04:38:01,410 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added resourceName=*
2025-03-26 04:38:01,410 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Added Execution Type=GUARANTEED
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 1, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 1, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=1 #asks=1
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 2, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 2, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=2 #asks=1
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Adding request to ask {AllocationRequestId: 0, Priority: 0, Capability: <memory:2432, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: {Execution Type: GUARANTEED, Enforce Execution Type: false}, Node Label Expression: null}
2025-03-26 04:38:01,411 DEBUG [main] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: addResourceRequest: applicationId= priority=0 resourceName=* numContainers=3 #asks=1
2025-03-26 04:38:01,412 INFO [main] org.apache.spark.deploy.yarn.YarnAllocator: Submitted 3 unlocalized container requests.
2025-03-26 04:38:01,420 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #2 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:38:01,427 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #2
2025-03-26 04:38:01,427 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 7ms
2025-03-26 04:38:01,437 INFO [main] org.apache.spark.deploy.yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
2025-03-26 04:38:01,437 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:38:01,438 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 3, running: 0, executorsStarting: 0
2025-03-26 04:38:01,438 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #3 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:38:01,440 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #3
2025-03-26 04:38:01,441 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 3ms
2025-03-26 04:38:01,642 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 3. Slept for 200103831/200.
2025-03-26 04:38:01,642 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:38:01,642 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 3, running: 0, executorsStarting: 0
2025-03-26 04:38:01,643 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #4 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:38:01,648 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #4
2025-03-26 04:38:01,648 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 5ms
2025-03-26 04:38:01,651 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave1:39565
2025-03-26 04:38:01,651 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave2:36613
2025-03-26 04:38:01,651 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Received new token for : slave0:40541
2025-03-26 04:38:01,653 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Allocated containers: 3. Current executor count: 0. Launching executor count: 0. Cluster resources: <memory:10240, vCores:20>.
2025-03-26 04:38:01,654 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave1, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,654 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave0, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,654 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: slave2, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,656 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,656 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,656 DEBUG [spark-rack-resolver] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: /default-rack, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=3
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=2 #asks=1
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=2
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=1 #asks=1
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Calling amClient.getMatchingRequests with parameters: priority: 0, location: *, resource: <memory:2432, vCores:2>
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Removing container request via AM client: Capability[<memory:2432, vCores:2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null]
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: BEFORE decResourceRequest: applicationId= priority=0 resourceName=* numContainers=1
2025-03-26 04:38:01,657 DEBUG [Reporter] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: AFTER decResourceRequest: allocationRequestId=0 priority=0 resourceName=* numContainers=0 #asks=1
2025-03-26 04:38:01,658 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742963823806_0001_01_000002 on host slave1 for executor with ID 1 for ResourceProfile Id 0
2025-03-26 04:38:01,659 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742963823806_0001_01_000003 on host slave0 for executor with ID 2 for ResourceProfile Id 0
2025-03-26 04:38:01,660 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Launching container container_1742963823806_0001_01_000004 on host slave2 for executor with ID 3 for ResourceProfile Id 0
2025-03-26 04:38:01,660 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:38:01,660 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:38:01,660 DEBUG [ContainerLauncher-0] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-26 04:38:01,662 INFO [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Received 3 containers from YARN, launching executors on 3 of them.
2025-03-26 04:38:01,662 DEBUG [ContainerLauncher-0] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-26 04:38:01,662 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:38:01,663 DEBUG [ContainerLauncher-2] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-26 04:38:01,663 DEBUG [ContainerLauncher-2] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-26 04:38:01,663 DEBUG [ContainerLauncher-1] org.apache.spark.deploy.yarn.ExecutorRunnable: Starting Executor Container
2025-03-26 04:38:01,663 DEBUG [ContainerLauncher-1] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
2025-03-26 04:38:01,663 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-26 04:38:01,663 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:38:01,664 DEBUG [ContainerLauncher-0] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-26 04:38:01,667 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-26 04:38:01,667 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:38:01,667 DEBUG [ContainerLauncher-2] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-26 04:38:01,668 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2025-03-26 04:38:01,668 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2025-03-26 04:38:01,668 DEBUG [ContainerLauncher-1] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
2025-03-26 04:38:01,677 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave2:36613
2025-03-26 04:38:01,679 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave0:40541
2025-03-26 04:38:01,679 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : slave1:39565
2025-03-26 04:38:01,690 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.13:36613, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963823806 } attemptId: 1 } nodeId { host: "slave2" port: 36613 } appSubmitter: "root" keyId: -911002869)
2025-03-26 04:38:01,691 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.11:40541, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963823806 } attemptId: 1 } nodeId { host: "slave0" port: 40541 } appSubmitter: "root" keyId: -911002869)
2025-03-26 04:38:01,692 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963823806_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@6b8ae6e3]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:38:01,692 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-26 04:38:01,692 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963823806_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@6ed52b7b]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:38:01,692 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-26 04:38:01,692 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.20.1.12:39565, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963823806 } attemptId: 1 } nodeId { host: "slave1" port: 39565 } appSubmitter: "root" keyId: -911002869)
2025-03-26 04:38:01,693 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963823806_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.yarn.client.ServerProxy$1@7d22a92f]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1852)
	at org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:99)
	at org.apache.hadoop.yarn.client.NMProxy.createNMProxy(NMProxy.java:51)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:270)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:244)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:133)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:202)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:38:01,693 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
2025-03-26 04:38:01,696 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: getting client out of cache: Client-dd5ed27233bf44d986fbe0731c03d648
2025-03-26 04:38:01,696 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: getting client out of cache: Client-dd5ed27233bf44d986fbe0731c03d648
2025-03-26 04:38:01,770 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: getting client out of cache: Client-dd5ed27233bf44d986fbe0731c03d648
2025-03-26 04:38:01,794 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:38:01,794 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Connecting to slave2/172.20.1.13:36613
2025-03-26 04:38:01,794 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Setup connection to slave2/172.20.1.13:36613
2025-03-26 04:38:01,794 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Connecting to slave1/172.20.1.12:39565
2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Setup connection to slave1/172.20.1.12:39565
2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963823806_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@2c04b8f7]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Connecting to slave0/172.20.1.11:40541
2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Setup connection to slave0/172.20.1.11:40541
2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963823806_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@2b0cc5e4]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:38:01,795 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:38:01,796 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@34305149
2025-03-26 04:38:01,797 DEBUG [ContainerLauncher-2] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.13:36613. Current token is Kind: NMToken, Service: 172.20.1.13:36613, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963823806 } attemptId: 1 } nodeId { host: "slave2" port: 36613 } appSubmitter: "root" keyId: -911002869)
2025-03-26 04:38:01,797 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-26 04:38:01,797 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-26 04:38:01,797 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEL6p9YXdMhABEgwKBnNsYXZlMhCFngIaBHJvb3Qgi+bMzfz/////AQ==
2025-03-26 04:38:01,797 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-26 04:38:01,797 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-26 04:38:01,798 DEBUG [ContainerLauncher-2] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEL6p9YXdMhABEgwKBnNsYXZlMhCFngIaBHJvb3Qgi+bMzfz/////AQ==\",realm=\"default\",nonce=\"LDkVXwYAsHbughbckJ1xBA38z/EkPKox8NlpxDaD\",nc=00000001,cnonce=\"CcfrJNjFZbc9g6YjBTG88gu365IFxIHxmPCGTdqe\",digest-uri=\"/default\",maxbuf=65536,response=0c7b28c75b5f7ed5f99cddaa9792a701,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-26 04:38:01,800 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-26 04:38:01,802 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@2c8b0d9e
2025-03-26 04:38:01,802 DEBUG [ContainerLauncher-1] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.11:40541. Current token is Kind: NMToken, Service: 172.20.1.11:40541, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963823806 } attemptId: 1 } nodeId { host: "slave0" port: 40541 } appSubmitter: "root" keyId: -911002869)
2025-03-26 04:38:01,803 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-26 04:38:01,803 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-26 04:38:01,803 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEL6p9YXdMhABEgwKBnNsYXZlMBDdvAIaBHJvb3Qgi+bMzfz/////AQ==
2025-03-26 04:38:01,803 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-26 04:38:01,803 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-26 04:38:01,803 DEBUG [ContainerLauncher-1] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEL6p9YXdMhABEgwKBnNsYXZlMBDdvAIaBHJvb3Qgi+bMzfz/////AQ==\",realm=\"default\",nonce=\"3hggKR1RuRfeP+ftsZ7u79iNB+OBQhG++f0yOR8w\",nc=00000001,cnonce=\"1QxwRL15c7Z1ZPSByLYH3YrZ1UQZqo5/XiwDPQYd\",digest-uri=\"/default\",maxbuf=65536,response=cbe4b4f0afaad57deb5cab19d9c6d831,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-26 04:38:01,804 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: appattempt_1742963823806_0001_000001 (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@27c41b70]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy38.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:213)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:124)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator.$anonfun$runAllocatedContainers$7(YarnAllocator.scala:731)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:38:01,804 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:38:01,805 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001 sending #5 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-26 04:38:01,808 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@43ddb719
2025-03-26 04:38:01,808 DEBUG [ContainerLauncher-0] org.apache.hadoop.yarn.security.NMTokenSelector: Looking for service: 172.20.1.12:39565. Current token is Kind: NMToken, Service: 172.20.1.12:39565, Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1742963823806 } attemptId: 1 } nodeId { host: "slave1" port: 39565 } appSubmitter: "root" keyId: -911002869)
2025-03-26 04:38:01,809 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
2025-03-26 04:38:01,809 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
2025-03-26 04:38:01,809 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting username: Cg0KCQgBEL6p9YXdMhABEgwKBnNsYXZlMRCNtQIaBHJvb3Qgi+bMzfz/////AQ==
2025-03-26 04:38:01,809 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting userPassword
2025-03-26 04:38:01,809 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: SASL client callback: setting realm: default
2025-03-26 04:38:01,809 DEBUG [ContainerLauncher-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg0KCQgBEL6p9YXdMhABEgwKBnNsYXZlMRCNtQIaBHJvb3Qgi+bMzfz/////AQ==\",realm=\"default\",nonce=\"rqEAHaGHtUjsRx8BfiPvEzNAHBPQo+sfA0hrWB8H\",nc=00000001,cnonce=\"nh6+LoUhZBhIqPomrrMHByHKovi46pG74aV7uDi+\",digest-uri=\"/default\",maxbuf=65536,response=f29dbe4468a8de86371c9ea3b9f0123a,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

2025-03-26 04:38:01,812 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001: starting, having connections 5
2025-03-26 04:38:01,812 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001 got value #5
2025-03-26 04:38:01,813 DEBUG [ContainerLauncher-2] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 19ms
2025-03-26 04:38:01,815 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-26 04:38:01,817 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001: closed
2025-03-26 04:38:01,817 DEBUG [IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave2/172.20.1.13:36613 from appattempt_1742963823806_0001_000001: stopped, remaining connections 4
2025-03-26 04:38:01,817 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001: starting, having connections 4
2025-03-26 04:38:01,818 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001 sending #6 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-26 04:38:01,821 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.Client: Negotiated QOP is :auth
2025-03-26 04:38:01,830 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001 sending #7 org.apache.hadoop.yarn.api.ContainerManagementProtocolPB.startContainers
2025-03-26 04:38:01,831 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001: starting, having connections 4
2025-03-26 04:38:01,955 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001 got value #6
2025-03-26 04:38:01,955 DEBUG [ContainerLauncher-1] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 161ms
2025-03-26 04:38:01,956 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001: closed
2025-03-26 04:38:01,956 DEBUG [IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave0/172.20.1.11:40541 from appattempt_1742963823806_0001_000001: stopped, remaining connections 3
2025-03-26 04:38:01,958 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001 got value #7
2025-03-26 04:38:01,958 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001: closed
2025-03-26 04:38:01,958 DEBUG [IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to slave1/172.20.1.12:39565 from appattempt_1742963823806_0001_000001: stopped, remaining connections 2
2025-03-26 04:38:01,959 DEBUG [ContainerLauncher-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: startContainers took 165ms
2025-03-26 04:38:02,785 INFO [main] org.apache.spark.executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1583@slave2
2025-03-26 04:38:02,794 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for TERM
2025-03-26 04:38:02,794 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for HUP
2025-03-26 04:38:02,795 INFO [main] org.apache.spark.util.SignalUtils: Registering signal handler for INT
2025-03-26 04:38:03,101 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])
2025-03-26 04:38:03,106 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])
2025-03-26 04:38:03,106 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])
2025-03-26 04:38:03,106 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])
2025-03-26 04:38:03,106 DEBUG [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])
2025-03-26 04:38:03,107 DEBUG [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: UgiMetrics, User and group related metrics
2025-03-26 04:38:03,127 DEBUG [main] org.apache.hadoop.util.Shell: setsid exited with exit code 0
2025-03-26 04:38:03,127 DEBUG [main] org.apache.hadoop.security.SecurityUtil: Setting hadoop.security.token.service.use_ip to true
2025-03-26 04:38:03,130 DEBUG [main] org.apache.hadoop.security.Groups:  Creating new Groups object
2025-03-26 04:38:03,131 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
2025-03-26 04:38:03,131 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2025-03-26 04:38:03,131 DEBUG [main] org.apache.hadoop.util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2025-03-26 04:38:03,131 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-03-26 04:38:03,131 DEBUG [main] org.apache.hadoop.util.PerformanceAdvisory: Falling back to shell based
2025-03-26 04:38:03,132 DEBUG [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2025-03-26 04:38:03,178 DEBUG [main] org.apache.hadoop.security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2025-03-26 04:38:03,180 DEBUG [main] org.apache.spark.deploy.SparkHadoopUtil: creating UGI for user: root
2025-03-26 04:38:03,183 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Hadoop login
2025-03-26 04:38:03,183 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: hadoop login commit
2025-03-26 04:38:03,185 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using local user: UnixPrincipal: root
2025-03-26 04:38:03,186 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Using user: "UnixPrincipal: root" with name: root
2025-03-26 04:38:03,186 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: User entry: "root"
2025-03-26 04:38:03,186 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Reading credentials from location /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000004/container_tokens
2025-03-26 04:38:03,192 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: Loaded 1 tokens from /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000004/container_tokens
2025-03-26 04:38:03,192 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: UGI loginUser: root (auth:SIMPLE)
2025-03-26 04:38:03,193 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.spark.deploy.SparkHadoopUtil$$anon$1@59d2400d]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:427)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend$.main(YarnCoarseGrainedExecutorBackend.scala:83)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main(YarnCoarseGrainedExecutorBackend.scala)
2025-03-26 04:38:03,206 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-26 04:38:03,207 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-26 04:38:03,207 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-26 04:38:03,208 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-26 04:38:03,208 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-26 04:38:03,279 DEBUG [main] io.netty.util.internal.logging.InternalLoggerFactory: Using SLF4J as the default logging framework
2025-03-26 04:38:03,283 DEBUG [main] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2025-03-26 04:38:03,283 DEBUG [main] io.netty.util.internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2025-03-26 04:38:03,290 DEBUG [main] io.netty.channel.MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 4
2025-03-26 04:38:03,311 DEBUG [main] io.netty.util.internal.PlatformDependent0: -Dio.netty.noUnsafe: false
2025-03-26 04:38:03,311 DEBUG [main] io.netty.util.internal.PlatformDependent0: Java version: 8
2025-03-26 04:38:03,311 DEBUG [main] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
2025-03-26 04:38:03,312 DEBUG [main] io.netty.util.internal.PlatformDependent0: sun.misc.Unsafe.copyMemory: available
2025-03-26 04:38:03,312 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.Buffer.address: available
2025-03-26 04:38:03,312 DEBUG [main] io.netty.util.internal.PlatformDependent0: direct buffer constructor: available
2025-03-26 04:38:03,312 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.Bits.unaligned: available, true
2025-03-26 04:38:03,313 DEBUG [main] io.netty.util.internal.PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2025-03-26 04:38:03,313 DEBUG [main] io.netty.util.internal.PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
2025-03-26 04:38:03,313 DEBUG [main] io.netty.util.internal.PlatformDependent: sun.misc.Unsafe: available
2025-03-26 04:38:03,313 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.tmpdir: /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000004/tmp (java.io.tmpdir)
2025-03-26 04:38:03,313 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
2025-03-26 04:38:03,313 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.maxDirectMemory: 1908932608 bytes
2025-03-26 04:38:03,314 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
2025-03-26 04:38:03,314 DEBUG [main] io.netty.util.internal.CleanerJava6: java.nio.ByteBuffer.cleaner(): available
2025-03-26 04:38:03,314 DEBUG [main] io.netty.util.internal.PlatformDependent: -Dio.netty.noPreferDirect: false
2025-03-26 04:38:03,315 DEBUG [main] io.netty.channel.nio.NioEventLoop: -Dio.netty.noKeySetOptimization: false
2025-03-26 04:38:03,315 DEBUG [main] io.netty.channel.nio.NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
2025-03-26 04:38:03,318 DEBUG [main] io.netty.util.internal.PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
2025-03-26 04:38:03,328 DEBUG [main] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
2025-03-26 04:38:03,328 DEBUG [main] io.netty.util.ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 4
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 4
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
2025-03-26 04:38:03,330 DEBUG [main] io.netty.buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2025-03-26 04:38:03,373 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave2/172.20.1.13:34623
2025-03-26 04:38:03,384 DEBUG [netty-rpc-connection-0] io.netty.channel.DefaultChannelId: -Dio.netty.processId: 1583 (auto-detected)
2025-03-26 04:38:03,386 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: -Djava.net.preferIPv4Stack: false
2025-03-26 04:38:03,386 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: -Djava.net.preferIPv6Addresses: false
2025-03-26 04:38:03,389 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
2025-03-26 04:38:03,390 DEBUG [netty-rpc-connection-0] io.netty.util.NetUtil: /proc/sys/net/core/somaxconn: 4096
2025-03-26 04:38:03,391 DEBUG [netty-rpc-connection-0] io.netty.channel.DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:14:01:0d (auto-detected)
2025-03-26 04:38:03,408 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.allocator.type: pooled
2025-03-26 04:38:03,408 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0
2025-03-26 04:38:03,408 DEBUG [netty-rpc-connection-0] io.netty.buffer.ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
2025-03-26 04:38:03,425 DEBUG [rpc-client-1-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkAccessible: true
2025-03-26 04:38:03,425 DEBUG [rpc-client-1-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkBounds: true
2025-03-26 04:38:03,425 DEBUG [rpc-client-1-1] io.netty.util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@31491a42
2025-03-26 04:38:03,436 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e] REGISTERED
2025-03-26 04:38:03,436 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e] CONNECT: slave2/172.20.1.13:34623
2025-03-26 04:38:03,437 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Connection to slave2/172.20.1.13:34623 successful, running bootstraps...
2025-03-26 04:38:03,437 INFO [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave2/172.20.1.13:34623 after 56 ms (0 ms spent in bootstraps)
2025-03-26 04:38:03,440 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096
2025-03-26 04:38:03,440 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.ratio: 8
2025-03-26 04:38:03,441 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.chunkSize: 32
2025-03-26 04:38:03,441 DEBUG [netty-rpc-connection-0] io.netty.util.Recycler: -Dio.netty.recycler.blocking: false
2025-03-26 04:38:03,442 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] ACTIVE
2025-03-26 04:38:03,449 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 168]
2025-03-26 04:38:03,449 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:03,451 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.13:38314.
2025-03-26 04:38:03,462 DEBUG [rpc-server-4-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkAccessible: true
2025-03-26 04:38:03,462 DEBUG [rpc-server-4-1] io.netty.buffer.AbstractByteBuf: -Dio.netty.buffer.checkBounds: true
2025-03-26 04:38:03,462 DEBUG [rpc-server-4-1] io.netty.util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@5d19f382
2025-03-26 04:38:03,472 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] REGISTERED
2025-03-26 04:38:03,472 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] ACTIVE
2025-03-26 04:38:03,474 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096
2025-03-26 04:38:03,474 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.ratio: 8
2025-03-26 04:38:03,474 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.chunkSize: 32
2025-03-26 04:38:03,474 DEBUG [rpc-server-4-1] io.netty.util.Recycler: -Dio.netty.recycler.blocking: false
2025-03-26 04:38:03,477 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] READ 189B
2025-03-26 04:38:03,487 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] READ COMPLETE
2025-03-26 04:38:03,488 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:03,489 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] FLUSH
2025-03-26 04:38:03,490 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] READ 21B
2025-03-26 04:38:03,492 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:03,493 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] READ 47B
2025-03-26 04:38:03,503 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:03,503 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 169]
2025-03-26 04:38:03,503 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:03,510 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] READ 190B
2025-03-26 04:38:03,511 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] READ COMPLETE
2025-03-26 04:38:03,531 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4376]
2025-03-26 04:38:03,532 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] FLUSH
2025-03-26 04:38:03,532 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] READ 1024B
2025-03-26 04:38:03,532 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] READ 3373B
2025-03-26 04:38:03,552 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:03,552 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 - R:slave2/172.20.1.13:34623] CLOSE
2025-03-26 04:38:03,553 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 - R:/172.20.1.13:38314] READ COMPLETE
2025-03-26 04:38:03,554 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 ! R:/172.20.1.13:38314] INACTIVE
2025-03-26 04:38:03,554 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0xa4ee42fd, L:/172.20.1.13:34623 ! R:/172.20.1.13:38314] UNREGISTERED
2025-03-26 04:38:03,556 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 ! R:slave2/172.20.1.13:34623] INACTIVE
2025-03-26 04:38:03,563 DEBUG [rpc-client-1-1] org.apache.spark.network.util.NettyLogger: [id: 0x686fe94e, L:/172.20.1.13:38314 ! R:slave2/172.20.1.13:34623] UNREGISTERED
2025-03-26 04:38:03,566 INFO [main] org.apache.spark.SecurityManager: Changing view acls to: root
2025-03-26 04:38:03,566 INFO [main] org.apache.spark.SecurityManager: Changing modify acls to: root
2025-03-26 04:38:03,567 INFO [main] org.apache.spark.SecurityManager: Changing view acls groups to: 
2025-03-26 04:38:03,567 INFO [main] org.apache.spark.SecurityManager: Changing modify acls groups to: 
2025-03-26 04:38:03,567 INFO [main] org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2025-03-26 04:38:03,587 DEBUG [main] org.apache.spark.SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
2025-03-26 04:38:03,605 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave2/172.20.1.13:34623
2025-03-26 04:38:03,606 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa] REGISTERED
2025-03-26 04:38:03,606 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa] CONNECT: slave2/172.20.1.13:34623
2025-03-26 04:38:03,606 DEBUG [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Connection to slave2/172.20.1.13:34623 successful, running bootstraps...
2025-03-26 04:38:03,607 INFO [netty-rpc-connection-0] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave2/172.20.1.13:34623 after 1 ms (0 ms spent in bootstraps)
2025-03-26 04:38:03,607 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] ACTIVE
2025-03-26 04:38:03,607 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 162]
2025-03-26 04:38:03,607 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:03,608 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.13:38318.
2025-03-26 04:38:03,609 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] REGISTERED
2025-03-26 04:38:03,609 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] ACTIVE
2025-03-26 04:38:03,609 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 183B
2025-03-26 04:38:03,610 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:03,610 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:03,610 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:03,611 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 21B
2025-03-26 04:38:03,611 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:03,611 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 47B
2025-03-26 04:38:03,614 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:03,637 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 164]
2025-03-26 04:38:03,637 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:03,637 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 21B
2025-03-26 04:38:03,640 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:03,640 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 164B
2025-03-26 04:38:03,642 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:03,642 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:03,642 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:03,643 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 68B
2025-03-26 04:38:03,643 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:03,645 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 173]
2025-03-26 04:38:03,645 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:03,645 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 194B
2025-03-26 04:38:03,646 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:03,646 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:03,646 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:03,647 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 68B
2025-03-26 04:38:03,647 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:03,663 INFO [main] org.apache.spark.storage.DiskBlockManager: Created local directory at /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/blockmgr-22a37763-73aa-4575-828b-03d33c81cc2d
2025-03-26 04:38:03,664 DEBUG [main] org.apache.spark.storage.DiskBlockManager: Adding shutdown hook
2025-03-26 04:38:03,666 DEBUG [main] org.apache.spark.util.ShutdownHookManager: Adding shutdown hook
2025-03-26 04:38:03,688 INFO [main] org.apache.spark.storage.memory.MemoryStore: MemoryStore started with capacity 912.3 MiB
2025-03-26 04:38:03,910 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 169]
2025-03-26 04:38:03,911 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:03,911 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 190B
2025-03-26 04:38:03,911 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:03,911 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:03,912 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:03,912 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 68B
2025-03-26 04:38:03,912 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:03,927 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@slave2:34623
2025-03-26 04:38:03,955 DEBUG [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Resource profile id is: 0
2025-03-26 04:38:03,960 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-26 04:38:03,960 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: No custom resources configured for spark.executor.
2025-03-26 04:38:03,960 INFO [dispatcher-Executor] org.apache.spark.resource.ResourceUtils: ==============================================================
2025-03-26 04:38:03,961 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 168]
2025-03-26 04:38:03,961 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:03,961 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 189B
2025-03-26 04:38:03,962 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:03,962 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:03,962 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:03,963 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 68B
2025-03-26 04:38:03,992 DEBUG [rpc-client-3-1] org.apache.spark.util.YarnContainerInfoHelper: Base URL for logs: http://slave2:8042/node/containerlogs/container_1742963823806_0001_01_000004/root
2025-03-26 04:38:04,017 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 1916]
2025-03-26 04:38:04,017 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:04,017 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 21B
2025-03-26 04:38:04,018 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:04,018 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 496B
2025-03-26 04:38:04,018 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 1420B
2025-03-26 04:38:04,022 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:04,038 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:04,039 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.13:38318) with ID 3,  ResourceProfileId 0
2025-03-26 04:38:04,042 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:04,042 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:04,042 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 68B
2025-03-26 04:38:04,044 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:04,046 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver
2025-03-26 04:38:04,052 INFO [dispatcher-Executor] org.apache.spark.executor.Executor: Starting executor ID 3 on host slave2
2025-03-26 04:38:04,094 DEBUG [dispatcher-Executor] org.apache.spark.network.server.TransportServer: Shuffle server started on port: 44151
2025-03-26 04:38:04,095 INFO [dispatcher-Executor] org.apache.spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44151.
2025-03-26 04:38:04,095 INFO [dispatcher-Executor] org.apache.spark.network.netty.NettyBlockTransferService: Server created on slave2:44151
2025-03-26 04:38:04,096 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-03-26 04:38:04,102 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, slave2, 44151, None)
2025-03-26 04:38:04,108 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 1456]
2025-03-26 04:38:04,108 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:04,108 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 1477B
2025-03-26 04:38:04,112 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:04,113 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave2
2025-03-26 04:38:04,114 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave2:44151 with 912.3 MiB RAM, BlockManagerId(3, slave2, 44151, None)
2025-03-26 04:38:04,122 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-26 04:38:04,123 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:04,123 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 21B
2025-03-26 04:38:04,123 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:04,124 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 79B
2025-03-26 04:38:04,125 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:04,125 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, slave2, 44151, None)
2025-03-26 04:38:04,126 INFO [dispatcher-Executor] org.apache.spark.storage.BlockManager: Initialized BlockManager: BlockManagerId(3, slave2, 44151, None)
2025-03-26 04:38:04,133 INFO [dispatcher-Executor] org.apache.spark.executor.Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000004/__app__.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000004/spark-examples_2.12-3.3.2.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000004/__app__.jar,file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000004/spark-examples_2.12-3.3.2.jar'
2025-03-26 04:38:04,138 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 163]
2025-03-26 04:38:04,139 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:04,139 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 184B
2025-03-26 04:38:04,140 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:04,141 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:04,141 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:04,141 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 68B
2025-03-26 04:38:04,141 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:04,164 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 177]
2025-03-26 04:38:04,164 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:04,164 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 190B
2025-03-26 04:38:04,166 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:04,662 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000060172/3000.
2025-03-26 04:38:04,663 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:38:04,663 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-26 04:38:04,663 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #8 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:38:04,665 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #8
2025-03-26 04:38:04,665 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 2ms
2025-03-26 04:38:05,207 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.12:33176.
2025-03-26 04:38:05,207 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] REGISTERED
2025-03-26 04:38:05,207 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] ACTIVE
2025-03-26 04:38:05,220 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] READ 189B
2025-03-26 04:38:05,220 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] READ COMPLETE
2025-03-26 04:38:05,220 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,220 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] FLUSH
2025-03-26 04:38:05,229 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] READ 190B
2025-03-26 04:38:05,229 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] READ COMPLETE
2025-03-26 04:38:05,230 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4376]
2025-03-26 04:38:05,230 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] FLUSH
2025-03-26 04:38:05,260 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 - R:/172.20.1.12:33176] READ COMPLETE
2025-03-26 04:38:05,260 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 ! R:/172.20.1.12:33176] INACTIVE
2025-03-26 04:38:05,260 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x3569b590, L:/172.20.1.13:34623 ! R:/172.20.1.12:33176] UNREGISTERED
2025-03-26 04:38:05,294 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.11:43154.
2025-03-26 04:38:05,295 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] REGISTERED
2025-03-26 04:38:05,295 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] ACTIVE
2025-03-26 04:38:05,306 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] READ 189B
2025-03-26 04:38:05,307 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] READ COMPLETE
2025-03-26 04:38:05,307 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,307 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] FLUSH
2025-03-26 04:38:05,310 DEBUG [rpc-server-4-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.12:33186.
2025-03-26 04:38:05,310 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] REGISTERED
2025-03-26 04:38:05,310 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] ACTIVE
2025-03-26 04:38:05,311 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 183B
2025-03-26 04:38:05,311 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,311 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,312 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:05,318 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] READ 190B
2025-03-26 04:38:05,319 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] READ COMPLETE
2025-03-26 04:38:05,319 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 4376]
2025-03-26 04:38:05,319 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] FLUSH
2025-03-26 04:38:05,341 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 185B
2025-03-26 04:38:05,341 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,342 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,342 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:05,346 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 194B
2025-03-26 04:38:05,346 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,346 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,347 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:05,352 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 - R:/172.20.1.11:43154] READ COMPLETE
2025-03-26 04:38:05,352 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 ! R:/172.20.1.11:43154] INACTIVE
2025-03-26 04:38:05,352 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x2445c482, L:/172.20.1.13:34623 ! R:/172.20.1.11:43154] UNREGISTERED
2025-03-26 04:38:05,415 DEBUG [rpc-server-4-2] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.11:43168.
2025-03-26 04:38:05,415 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] REGISTERED
2025-03-26 04:38:05,415 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] ACTIVE
2025-03-26 04:38:05,415 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 183B
2025-03-26 04:38:05,415 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:05,416 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,418 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:05,445 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 185B
2025-03-26 04:38:05,446 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:05,446 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,447 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:05,450 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 194B
2025-03-26 04:38:05,451 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:05,451 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,451 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:05,610 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 190B
2025-03-26 04:38:05,611 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,611 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,611 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:05,707 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 189B
2025-03-26 04:38:05,708 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,708 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,708 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:05,724 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 190B
2025-03-26 04:38:05,724 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:05,725 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,725 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:05,796 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 512B
2025-03-26 04:38:05,796 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 1425B
2025-03-26 04:38:05,797 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,797 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.12:33186) with ID 1,  ResourceProfileId 0
2025-03-26 04:38:05,798 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,798 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:05,828 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 189B
2025-03-26 04:38:05,828 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:05,828 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,828 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:05,855 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 1477B
2025-03-26 04:38:05,856 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,856 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave1
2025-03-26 04:38:05,857 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave1:45177 with 912.3 MiB RAM, BlockManagerId(1, slave1, 45177, None)
2025-03-26 04:38:05,858 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-26 04:38:05,858 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:05,884 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 184B
2025-03-26 04:38:05,884 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,885 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,885 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:05,915 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ 190B
2025-03-26 04:38:05,916 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:05,927 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 512B
2025-03-26 04:38:05,927 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 1425B
2025-03-26 04:38:05,928 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.1.11:43168) with ID 2,  ResourceProfileId 0
2025-03-26 04:38:05,928 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:05,929 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:05,929 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:05,977 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2025-03-26 04:38:05,977 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
2025-03-26 04:38:06,012 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 1477B
2025-03-26 04:38:06,013 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:06,013 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.DefaultTopologyMapper: Got a request for slave0
2025-03-26 04:38:06,014 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Registering block manager slave0:39363 with 912.3 MiB RAM, BlockManagerId(2, slave0, 39363, None)
2025-03-26 04:38:06,015 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 79]
2025-03-26 04:38:06,015 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:06,041 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 184B
2025-03-26 04:38:06,042 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:06,042 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:06,042 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:06,057 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ 190B
2025-03-26 04:38:06,058 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:06,957 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-26 04:38:06,957 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-26 04:38:06,957 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:38:06,957 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-26 04:38:06,958 INFO [Driver] org.apache.spark.sql.internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.spark.sql.internal.SharedState: Applying other initial session options to HadoopConf: spark.app.name -> DeveloperApiExample
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Starting: Acquiring creator semaphore for file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000001/spark-warehouse
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Acquiring creator semaphore for file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000001/spark-warehouse: duration 0:00.000s
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Starting: Creating FS file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000001/spark-warehouse
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-26 04:38:06,960 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Creating FS file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000001/spark-warehouse: duration 0:00.000s
2025-03-26 04:38:06,961 INFO [Driver] org.apache.spark.sql.internal.SharedState: Warehouse path is 'file:/data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/container_1742963823806_0001_01_000001/spark-warehouse'.
2025-03-26 04:38:06,970 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:06,971 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol http
2025-03-26 04:38:06,971 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Unknown protocol http, delegating to default implementation
2025-03-26 04:38:06,972 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:06,973 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:06,973 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:06,977 INFO [Driver] org.apache.spark.ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol jar
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting jar
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.jar.impl
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Creating handler for protocol file
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking for FS supporting file
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: looking for configuration option fs.file.impl
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: Looking in service filesystems for implementation class
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Found implementation of file: class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2025-03-26 04:38:06,978 DEBUG [Driver] org.apache.hadoop.fs.FsUrlStreamHandlerFactory: Using handler for protocol file
2025-03-26 04:38:07,532 DEBUG [Driver] org.apache.spark.sql.catalyst.parser.CatalystSqlParser: Parsing command: spark_grouping_id
2025-03-26 04:38:07,666 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000067800/3000.
2025-03-26 04:38:07,666 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:38:07,666 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-26 04:38:07,667 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #9 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:38:07,669 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #9
2025-03-26 04:38:07,669 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 3ms
2025-03-26 04:38:08,122 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegression: Input schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}}]}
2025-03-26 04:38:08,125 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegression: Expected output schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{}},{"name":"rawPrediction","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":false,"metadata":{}}]}
2025-03-26 04:38:08,152 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#0
2025-03-26 04:38:08,172 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#7
2025-03-26 04:38:08,172 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#1
2025-03-26 04:38:08,730 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, double, false],input[1, vector, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     double value_0 = i.getDouble(0);
/* 036 */     mutableStateArray_0[0].write(0, value_0);
/* 037 */
/* 038 */     boolean isNull_1 = i.isNullAt(1);
/* 039 */     InternalRow value_1 = isNull_1 ?
/* 040 */     null : (i.getStruct(1, 4));
/* 041 */     if (isNull_1) {
/* 042 */       mutableStateArray_0[0].setNullAt(1);
/* 043 */     } else {
/* 044 */       final InternalRow tmpInput_0 = value_1;
/* 045 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 046 */         mutableStateArray_0[0].write(1, (UnsafeRow) tmpInput_0);
/* 047 */       } else {
/* 048 */         // Remember the current cursor so that we can calculate how many bytes are
/* 049 */         // written later.
/* 050 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 051 */
/* 052 */         mutableStateArray_0[1].resetRowWriter();
/* 053 */
/* 054 */
/* 055 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 056 */
/* 057 */
/* 058 */         if ((tmpInput_0.isNullAt(1))) {
/* 059 */           mutableStateArray_0[1].setNullAt(1);
/* 060 */         } else {
/* 061 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 062 */         }
/* 063 */
/* 064 */
/* 065 */         if ((tmpInput_0.isNullAt(2))) {
/* 066 */           mutableStateArray_0[1].setNullAt(2);
/* 067 */         } else {
/* 068 */           // Remember the current cursor so that we can calculate how many bytes are
/* 069 */           // written later.
/* 070 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 071 */
/* 072 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 073 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 074 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 075 */           } else {
/* 076 */             final int numElements_0 = tmpInput_1.numElements();
/* 077 */             mutableStateArray_1[0].initialize(numElements_0);
/* 078 */
/* 079 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 080 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 081 */             }
/* 082 */           }
/* 083 */
/* 084 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 085 */         }
/* 086 */
/* 087 */
/* 088 */         if ((tmpInput_0.isNullAt(3))) {
/* 089 */           mutableStateArray_0[1].setNullAt(3);
/* 090 */         } else {
/* 091 */           // Remember the current cursor so that we can calculate how many bytes are
/* 092 */           // written later.
/* 093 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 094 */
/* 095 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 096 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 097 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 098 */           } else {
/* 099 */             final int numElements_1 = tmpInput_2.numElements();
/* 100 */             mutableStateArray_1[1].initialize(numElements_1);
/* 101 */
/* 102 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 103 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 104 */             }
/* 105 */           }
/* 106 */
/* 107 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 108 */         }
/* 109 */
/* 110 */
/* 111 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, previousCursor_0);
/* 112 */       }
/* 113 */     }
/* 114 */     return (mutableStateArray_0[0].getRow());
/* 115 */   }
/* 116 */
/* 117 */
/* 118 */ }

2025-03-26 04:38:08,752 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     double value_0 = i.getDouble(0);
/* 036 */     mutableStateArray_0[0].write(0, value_0);
/* 037 */
/* 038 */     boolean isNull_1 = i.isNullAt(1);
/* 039 */     InternalRow value_1 = isNull_1 ?
/* 040 */     null : (i.getStruct(1, 4));
/* 041 */     if (isNull_1) {
/* 042 */       mutableStateArray_0[0].setNullAt(1);
/* 043 */     } else {
/* 044 */       final InternalRow tmpInput_0 = value_1;
/* 045 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 046 */         mutableStateArray_0[0].write(1, (UnsafeRow) tmpInput_0);
/* 047 */       } else {
/* 048 */         // Remember the current cursor so that we can calculate how many bytes are
/* 049 */         // written later.
/* 050 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 051 */
/* 052 */         mutableStateArray_0[1].resetRowWriter();
/* 053 */
/* 054 */
/* 055 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 056 */
/* 057 */
/* 058 */         if ((tmpInput_0.isNullAt(1))) {
/* 059 */           mutableStateArray_0[1].setNullAt(1);
/* 060 */         } else {
/* 061 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 062 */         }
/* 063 */
/* 064 */
/* 065 */         if ((tmpInput_0.isNullAt(2))) {
/* 066 */           mutableStateArray_0[1].setNullAt(2);
/* 067 */         } else {
/* 068 */           // Remember the current cursor so that we can calculate how many bytes are
/* 069 */           // written later.
/* 070 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 071 */
/* 072 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 073 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 074 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 075 */           } else {
/* 076 */             final int numElements_0 = tmpInput_1.numElements();
/* 077 */             mutableStateArray_1[0].initialize(numElements_0);
/* 078 */
/* 079 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 080 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 081 */             }
/* 082 */           }
/* 083 */
/* 084 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 085 */         }
/* 086 */
/* 087 */
/* 088 */         if ((tmpInput_0.isNullAt(3))) {
/* 089 */           mutableStateArray_0[1].setNullAt(3);
/* 090 */         } else {
/* 091 */           // Remember the current cursor so that we can calculate how many bytes are
/* 092 */           // written later.
/* 093 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 094 */
/* 095 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 096 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 097 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 098 */           } else {
/* 099 */             final int numElements_1 = tmpInput_2.numElements();
/* 100 */             mutableStateArray_1[1].initialize(numElements_1);
/* 101 */
/* 102 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 103 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 104 */             }
/* 105 */           }
/* 106 */
/* 107 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 108 */         }
/* 109 */
/* 110 */
/* 111 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, previousCursor_0);
/* 112 */       }
/* 113 */     }
/* 114 */     return (mutableStateArray_0[0].getRow());
/* 115 */   }
/* 116 */
/* 117 */
/* 118 */ }

2025-03-26 04:38:08,933 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 202.391025 ms
2025-03-26 04:38:08,958 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$1
2025-03-26 04:38:08,972 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$1) is now cleaned +++
2025-03-26 04:38:08,987 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1
2025-03-26 04:38:09,002 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
2025-03-26 04:38:09,025 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$extractLabeledPoints$1
2025-03-26 04:38:09,025 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$extractLabeledPoints$1) is now cleaned +++
2025-03-26 04:38:09,035 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$take$2
2025-03-26 04:38:09,038 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$take$2) is now cleaned +++
2025-03-26 04:38:09,118 DEBUG [Driver] org.apache.spark.util.ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5
2025-03-26 04:38:09,121 DEBUG [Driver] org.apache.spark.util.ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
2025-03-26 04:38:09,123 INFO [Driver] org.apache.spark.SparkContext: Starting job: take at DeveloperApiExample.scala:127
2025-03-26 04:38:09,125 DEBUG [Driver] org.apache.spark.scheduler.DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 5 took 0.000684 seconds
2025-03-26 04:38:09,128 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Merging stage rdd profiles: Set()
2025-03-26 04:38:09,146 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Got job 0 (take at DeveloperApiExample.scala:127) with 1 output partitions
2025-03-26 04:38:09,147 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (take at DeveloperApiExample.scala:127)
2025-03-26 04:38:09,147 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List()
2025-03-26 04:38:09,148 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Missing parents: List()
2025-03-26 04:38:09,167 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: submitStage(ResultStage 0 (name=take at DeveloperApiExample.scala:127;jobs=0))
2025-03-26 04:38:09,170 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: missing: List()
2025-03-26 04:38:09,171 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at Predictor.scala:185), which has no missing parents
2025-03-26 04:38:09,172 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: submitMissingTasks(ResultStage 0)
2025-03-26 04:38:09,235 INFO [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.0 KiB, free 2004.6 MiB)
2025-03-26 04:38:09,236 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Put block broadcast_0 locally took 16 ms
2025-03-26 04:38:09,238 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Putting block broadcast_0 without replication took 18 ms
2025-03-26 04:38:09,258 INFO [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 2004.6 MiB)
2025-03-26 04:38:09,260 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, slave2, 40525, None)
2025-03-26 04:38:09,261 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on slave2:40525 (size: 9.4 KiB, free: 2004.6 MiB)
2025-03-26 04:38:09,264 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
2025-03-26 04:38:09,264 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Told master about block broadcast_0_piece0
2025-03-26 04:38:09,264 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Put block broadcast_0_piece0 locally took 7 ms
2025-03-26 04:38:09,264 DEBUG [dag-scheduler-event-loop] org.apache.spark.storage.BlockManager: Putting block broadcast_0_piece0 without replication took 7 ms
2025-03-26 04:38:09,265 INFO [dag-scheduler-event-loop] org.apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
2025-03-26 04:38:09,277 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at Predictor.scala:185) (first 15 tasks are for partitions Vector(0))
2025-03-26 04:38:09,278 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Adding task set 0.0 with 1 tasks resource profile 0
2025-03-26 04:38:09,292 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Epoch for TaskSet 0.0: 0
2025-03-26 04:38:09,295 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Adding pending tasks took 2 ms
2025-03-26 04:38:09,297 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2025-03-26 04:38:09,300 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnClusterScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0
2025-03-26 04:38:09,316 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (slave2, executor 3, partition 0, PROCESS_LOCAL, 4702 bytes) taskResourceAssignments Map()
2025-03-26 04:38:09,326 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
2025-03-26 04:38:09,330 DEBUG [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 0 on executor id: 3 hostname: slave2.
2025-03-26 04:38:09,334 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 5149]
2025-03-26 04:38:09,334 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:09,334 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 464B
2025-03-26 04:38:09,334 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 1024B
2025-03-26 04:38:09,334 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 3674B
2025-03-26 04:38:09,338 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:09,340 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0
2025-03-26 04:38:09,350 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
2025-03-26 04:38:09,378 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 1629]
2025-03-26 04:38:09,378 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:09,378 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 1024B
2025-03-26 04:38:09,379 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 618B
2025-03-26 04:38:09,384 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:09,423 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: task 0.0 in stage 0.0 (TID 0)'s epoch is 0
2025-03-26 04:38:09,426 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1
2025-03-26 04:38:09,471 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting local block broadcast_0
2025-03-26 04:38:09,473 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Block broadcast_0 was not found
2025-03-26 04:38:09,474 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-03-26 04:38:09,479 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Reading piece broadcast_0_piece0 of broadcast_0
2025-03-26 04:38:09,479 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting local block broadcast_0_piece0 as bytes
2025-03-26 04:38:09,480 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting remote block broadcast_0_piece0
2025-03-26 04:38:09,481 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 317]
2025-03-26 04:38:09,481 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:09,482 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 338B
2025-03-26 04:38:09,483 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:09,487 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 760]
2025-03-26 04:38:09,487 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:09,488 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 781B
2025-03-26 04:38:09,492 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:09,494 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Getting remote block broadcast_0_piece0 from BlockManagerId(driver, slave2, 40525, None)
2025-03-26 04:38:09,498 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Creating new connection to slave2/172.20.1.13:40525
2025-03-26 04:38:09,499 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9] REGISTERED
2025-03-26 04:38:09,499 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9] CONNECT: slave2/172.20.1.13:40525
2025-03-26 04:38:09,500 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] ACTIVE
2025-03-26 04:38:09,500 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Connection to slave2/172.20.1.13:40525 successful, running bootstraps...
2025-03-26 04:38:09,500 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.network.client.TransportClientFactory: Successfully created connection to slave2/172.20.1.13:40525 after 2 ms (0 ms spent in bootstraps)
2025-03-26 04:38:09,500 DEBUG [shuffle-server-7-1] org.apache.spark.network.server.TransportServer: New connection accepted for remote address /172.20.1.13:56638.
2025-03-26 04:38:09,502 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] REGISTERED
2025-03-26 04:38:09,502 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] ACTIVE
2025-03-26 04:38:09,505 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 71]
2025-03-26 04:38:09,505 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] FLUSH
2025-03-26 04:38:09,505 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] READ 21B
2025-03-26 04:38:09,505 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] READ COMPLETE
2025-03-26 04:38:09,506 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] READ 71B
2025-03-26 04:38:09,524 DEBUG [shuffle-server-7-1] org.apache.spark.storage.BlockManager: Getting local block broadcast_0_piece0 as bytes
2025-03-26 04:38:09,526 DEBUG [shuffle-server-7-1] org.apache.spark.storage.BlockManager: Level for block broadcast_0_piece0 is StorageLevel(disk, memory, 1 replicas)
2025-03-26 04:38:09,527 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 13]
2025-03-26 04:38:09,527 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] FLUSH
2025-03-26 04:38:09,528 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] READ COMPLETE
2025-03-26 04:38:09,528 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] READ 34B
2025-03-26 04:38:09,531 DEBUG [shuffle-client-4-1] org.apache.spark.network.client.TransportClient: Sending fetch chunk request 0 to slave2/172.20.1.13:40525
2025-03-26 04:38:09,531 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] WRITE 21B
2025-03-26 04:38:09,531 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] FLUSH
2025-03-26 04:38:09,532 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] READ COMPLETE
2025-03-26 04:38:09,532 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] READ 21B
2025-03-26 04:38:09,534 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 9648]
2025-03-26 04:38:09,534 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] FLUSH
2025-03-26 04:38:09,535 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] READ COMPLETE
2025-03-26 04:38:09,534 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] READ 2048B
2025-03-26 04:38:09,535 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] READ 7621B
2025-03-26 04:38:09,536 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] READ COMPLETE
2025-03-26 04:38:09,547 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 912.3 MiB)
2025-03-26 04:38:09,549 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 173]
2025-03-26 04:38:09,549 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:09,550 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 194B
2025-03-26 04:38:09,552 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:09,553 DEBUG [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(3, slave2, 44151, None)
2025-03-26 04:38:09,553 INFO [dispatcher-BlockManagerMaster] org.apache.spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on slave2:44151 (size: 9.4 KiB, free: 912.3 MiB)
2025-03-26 04:38:09,553 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 21, bodyLength: 47]
2025-03-26 04:38:09,554 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:09,554 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 68B
2025-03-26 04:38:09,554 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:09,555 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
2025-03-26 04:38:09,555 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Told master about block broadcast_0_piece0
2025-03-26 04:38:09,558 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Put block broadcast_0_piece0 locally took 12 ms
2025-03-26 04:38:09,560 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Putting block broadcast_0_piece0 without replication took 14 ms
2025-03-26 04:38:09,560 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.broadcast.TorrentBroadcast: Reading broadcast variable 0 took 86 ms
2025-03-26 04:38:09,600 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 20.0 KiB, free 912.3 MiB)
2025-03-26 04:38:09,601 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Put block broadcast_0 locally took 25 ms
2025-03-26 04:38:09,601 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.storage.BlockManager: Putting block broadcast_0 without replication took 25 ms
2025-03-26 04:38:10,670 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Number of pending allocations is 0. Slept for 3000067442/3000.
2025-03-26 04:38:10,670 DEBUG [Reporter] org.apache.spark.deploy.yarn.ApplicationMaster: Sending progress
2025-03-26 04:38:10,670 DEBUG [Reporter] org.apache.spark.deploy.yarn.YarnAllocator: Updating resource requests for ResourceProfile id: 0, target: 3, pending: 0, running: 3, executorsStarting: 0
2025-03-26 04:38:10,671 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #10 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate
2025-03-26 04:38:10,672 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #10
2025-03-26 04:38:10,672 DEBUG [Reporter] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: allocate took 2ms
2025-03-26 04:38:11,121 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for createexternalrow(input[0, double, false], newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, StructField(label,DoubleType,false), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_5 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_5);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[2];
/* 039 */
/* 040 */     double value_1 = i.getDouble(0);
/* 041 */     if (false) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.ml.linalg.VectorUDT value_3 = false ?
/* 048 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 049 */     boolean isNull_2 = true;
/* 050 */     org.apache.spark.ml.linalg.Vector value_2 = null;
/* 051 */     resultIsNull_0 = false;
/* 052 */     if (!resultIsNull_0) {
/* 053 */       boolean isNull_4 = i.isNullAt(1);
/* 054 */       InternalRow value_4 = isNull_4 ?
/* 055 */       null : (i.getStruct(1, 4));
/* 056 */       resultIsNull_0 = isNull_4;
/* 057 */       mutableStateArray_0[0] = value_4;
/* 058 */     }
/* 059 */
/* 060 */     isNull_2 = resultIsNull_0;
/* 061 */     if (!isNull_2) {
/* 062 */
/* 063 */       Object funcResult_0 = null;
/* 064 */       funcResult_0 = value_3.deserialize(mutableStateArray_0[0]);
/* 065 */
/* 066 */       if (funcResult_0 != null) {
/* 067 */         value_2 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 068 */       } else {
/* 069 */         isNull_2 = true;
/* 070 */       }
/* 071 */
/* 072 */
/* 073 */     }
/* 074 */     if (isNull_2) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_2;
/* 078 */     }
/* 079 */
/* 080 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 081 */
/* 082 */     return value_0;
/* 083 */   }
/* 084 */
/* 085 */ }

2025-03-26 04:38:11,141 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_5 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_5);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[2];
/* 039 */
/* 040 */     double value_1 = i.getDouble(0);
/* 041 */     if (false) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.ml.linalg.VectorUDT value_3 = false ?
/* 048 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 049 */     boolean isNull_2 = true;
/* 050 */     org.apache.spark.ml.linalg.Vector value_2 = null;
/* 051 */     resultIsNull_0 = false;
/* 052 */     if (!resultIsNull_0) {
/* 053 */       boolean isNull_4 = i.isNullAt(1);
/* 054 */       InternalRow value_4 = isNull_4 ?
/* 055 */       null : (i.getStruct(1, 4));
/* 056 */       resultIsNull_0 = isNull_4;
/* 057 */       mutableStateArray_0[0] = value_4;
/* 058 */     }
/* 059 */
/* 060 */     isNull_2 = resultIsNull_0;
/* 061 */     if (!isNull_2) {
/* 062 */
/* 063 */       Object funcResult_0 = null;
/* 064 */       funcResult_0 = value_3.deserialize(mutableStateArray_0[0]);
/* 065 */
/* 066 */       if (funcResult_0 != null) {
/* 067 */         value_2 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 068 */       } else {
/* 069 */         isNull_2 = true;
/* 070 */       }
/* 071 */
/* 072 */
/* 073 */     }
/* 074 */     if (isNull_2) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_2;
/* 078 */     }
/* 079 */
/* 080 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 081 */
/* 082 */     return value_0;
/* 083 */   }
/* 084 */
/* 085 */ }

2025-03-26 04:38:11,243 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 120.933573 ms
2025-03-26 04:38:11,258 INFO [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1419 bytes result sent to driver
2025-03-26 04:38:11,259 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 3056]
2025-03-26 04:38:11,259 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] FLUSH
2025-03-26 04:38:11,259 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ 3069B
2025-03-26 04:38:11,260 DEBUG [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.ExecutorMetricsPoller: stageTCMP: (0, 0) -> 0
2025-03-26 04:38:11,261 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:11,272 INFO [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1970 ms on slave2 (executor 3) (1/1)
2025-03-26 04:38:11,273 INFO [task-result-getter-0] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-03-26 04:38:11,276 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: ResultStage 0 (take at DeveloperApiExample.scala:127) finished in 2.079 s
2025-03-26 04:38:11,278 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: After removal of stage 0, remaining stages = 0
2025-03-26 04:38:11,278 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-03-26 04:38:11,278 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.cluster.YarnClusterScheduler: Killing all running tasks in stage 0: Stage finished
2025-03-26 04:38:11,279 ERROR [dag-scheduler-event-loop] org.apache.spark.scheduler.AsyncEventQueue: Dropping event from queue executorManagement. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
2025-03-26 04:38:11,280 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.AsyncEventQueue: Dropped 1 events from executorManagement since the application started.
2025-03-26 04:38:11,280 ERROR [dag-scheduler-event-loop] org.apache.spark.scheduler.AsyncEventQueue: Dropping event from queue shared. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
2025-03-26 04:38:11,280 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.AsyncEventQueue: Dropped 1 events from shared since the application started.
2025-03-26 04:38:11,281 INFO [Driver] org.apache.spark.scheduler.DAGScheduler: Job 0 finished: take at DeveloperApiExample.scala:127, took 2.157624 s
2025-03-26 04:38:11,300 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root: closed
2025-03-26 04:38:11,300 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root: stopped, remaining connections 1
2025-03-26 04:38:11,304 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegressionModel: Input schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}}]}
2025-03-26 04:38:11,317 DEBUG [Driver] org.apache.spark.examples.ml.MyLogisticRegressionModel: Expected output schema: {"type":"struct","fields":[{"name":"label","type":"double","nullable":false,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{"ml_attr":{"type":"nominal","num_vals":2}}},{"name":"rawPrediction","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":false,"metadata":{"ml_attr":{"num_attrs":2}}}]}
2025-03-26 04:38:11,362 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#14
2025-03-26 04:38:11,398 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'rawPrediction to rawPrediction#19
2025-03-26 04:38:11,436 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'features to features#14
2025-03-26 04:38:11,437 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'label to label#13
2025-03-26 04:38:11,437 DEBUG [Driver] org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1: Resolving 'prediction to prediction#26
2025-03-26 04:38:11,483 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for newInstance(class org.apache.spark.ml.linalg.VectorUDT).serialize:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private boolean resultIsNull_0;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private org.apache.spark.ml.linalg.Vector[] mutableStateArray_0 = new org.apache.spark.ml.linalg.Vector[1];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */
/* 016 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_1[0], 4);
/* 018 */     mutableStateArray_2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 4);
/* 019 */     mutableStateArray_2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 8);
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public void initialize(int partitionIndex) {
/* 024 */
/* 025 */   }
/* 026 */
/* 027 */   // Scala.Function1 need this
/* 028 */   public java.lang.Object apply(java.lang.Object row) {
/* 029 */     return apply((InternalRow) row);
/* 030 */   }
/* 031 */
/* 032 */   public UnsafeRow apply(InternalRow i) {
/* 033 */     mutableStateArray_1[0].reset();
/* 034 */
/* 035 */
/* 036 */     mutableStateArray_1[0].zeroOutNullBytes();
/* 037 */
/* 038 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 039 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 040 */     boolean isNull_0 = true;
/* 041 */     InternalRow value_0 = null;
/* 042 */     resultIsNull_0 = false;
/* 043 */     if (!resultIsNull_0) {
/* 044 */       boolean isNull_2 = i.isNullAt(0);
/* 045 */       org.apache.spark.ml.linalg.Vector value_2 = isNull_2 ?
/* 046 */       null : ((org.apache.spark.ml.linalg.Vector)i.get(0, null));
/* 047 */       resultIsNull_0 = isNull_2;
/* 048 */       mutableStateArray_0[0] = value_2;
/* 049 */     }
/* 050 */
/* 051 */     isNull_0 = resultIsNull_0;
/* 052 */     if (!isNull_0) {
/* 053 */
/* 054 */       Object funcResult_0 = null;
/* 055 */       funcResult_0 = value_1.serialize(mutableStateArray_0[0]);
/* 056 */
/* 057 */       if (funcResult_0 != null) {
/* 058 */         value_0 = (InternalRow) funcResult_0;
/* 059 */       } else {
/* 060 */         isNull_0 = true;
/* 061 */       }
/* 062 */
/* 063 */
/* 064 */     }
/* 065 */     if (isNull_0) {
/* 066 */       mutableStateArray_1[0].setNullAt(0);
/* 067 */     } else {
/* 068 */       final InternalRow tmpInput_0 = value_0;
/* 069 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 070 */         mutableStateArray_1[0].write(0, (UnsafeRow) tmpInput_0);
/* 071 */       } else {
/* 072 */         // Remember the current cursor so that we can calculate how many bytes are
/* 073 */         // written later.
/* 074 */         final int previousCursor_0 = mutableStateArray_1[0].cursor();
/* 075 */
/* 076 */         mutableStateArray_1[1].resetRowWriter();
/* 077 */
/* 078 */
/* 079 */         mutableStateArray_1[1].write(0, (tmpInput_0.getByte(0)));
/* 080 */
/* 081 */
/* 082 */         if ((tmpInput_0.isNullAt(1))) {
/* 083 */           mutableStateArray_1[1].setNullAt(1);
/* 084 */         } else {
/* 085 */           mutableStateArray_1[1].write(1, (tmpInput_0.getInt(1)));
/* 086 */         }
/* 087 */
/* 088 */
/* 089 */         if ((tmpInput_0.isNullAt(2))) {
/* 090 */           mutableStateArray_1[1].setNullAt(2);
/* 091 */         } else {
/* 092 */           // Remember the current cursor so that we can calculate how many bytes are
/* 093 */           // written later.
/* 094 */           final int previousCursor_1 = mutableStateArray_1[1].cursor();
/* 095 */
/* 096 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 097 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 098 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_1);
/* 099 */           } else {
/* 100 */             final int numElements_0 = tmpInput_1.numElements();
/* 101 */             mutableStateArray_2[0].initialize(numElements_0);
/* 102 */
/* 103 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 104 */               mutableStateArray_2[0].write(index_0, tmpInput_1.getInt(index_0));
/* 105 */             }
/* 106 */           }
/* 107 */
/* 108 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 109 */         }
/* 110 */
/* 111 */
/* 112 */         if ((tmpInput_0.isNullAt(3))) {
/* 113 */           mutableStateArray_1[1].setNullAt(3);
/* 114 */         } else {
/* 115 */           // Remember the current cursor so that we can calculate how many bytes are
/* 116 */           // written later.
/* 117 */           final int previousCursor_2 = mutableStateArray_1[1].cursor();
/* 118 */
/* 119 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 120 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 121 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_2);
/* 122 */           } else {
/* 123 */             final int numElements_1 = tmpInput_2.numElements();
/* 124 */             mutableStateArray_2[1].initialize(numElements_1);
/* 125 */
/* 126 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 127 */               mutableStateArray_2[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 128 */             }
/* 129 */           }
/* 130 */
/* 131 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 132 */         }
/* 133 */
/* 134 */
/* 135 */         mutableStateArray_1[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 136 */       }
/* 137 */     }
/* 138 */     return (mutableStateArray_1[0].getRow());
/* 139 */   }
/* 140 */
/* 141 */
/* 142 */ }

2025-03-26 04:38:11,486 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private boolean resultIsNull_0;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private org.apache.spark.ml.linalg.Vector[] mutableStateArray_0 = new org.apache.spark.ml.linalg.Vector[1];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */
/* 016 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_1[0], 4);
/* 018 */     mutableStateArray_2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 4);
/* 019 */     mutableStateArray_2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_1[1], 8);
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public void initialize(int partitionIndex) {
/* 024 */
/* 025 */   }
/* 026 */
/* 027 */   // Scala.Function1 need this
/* 028 */   public java.lang.Object apply(java.lang.Object row) {
/* 029 */     return apply((InternalRow) row);
/* 030 */   }
/* 031 */
/* 032 */   public UnsafeRow apply(InternalRow i) {
/* 033 */     mutableStateArray_1[0].reset();
/* 034 */
/* 035 */
/* 036 */     mutableStateArray_1[0].zeroOutNullBytes();
/* 037 */
/* 038 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 039 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 040 */     boolean isNull_0 = true;
/* 041 */     InternalRow value_0 = null;
/* 042 */     resultIsNull_0 = false;
/* 043 */     if (!resultIsNull_0) {
/* 044 */       boolean isNull_2 = i.isNullAt(0);
/* 045 */       org.apache.spark.ml.linalg.Vector value_2 = isNull_2 ?
/* 046 */       null : ((org.apache.spark.ml.linalg.Vector)i.get(0, null));
/* 047 */       resultIsNull_0 = isNull_2;
/* 048 */       mutableStateArray_0[0] = value_2;
/* 049 */     }
/* 050 */
/* 051 */     isNull_0 = resultIsNull_0;
/* 052 */     if (!isNull_0) {
/* 053 */
/* 054 */       Object funcResult_0 = null;
/* 055 */       funcResult_0 = value_1.serialize(mutableStateArray_0[0]);
/* 056 */
/* 057 */       if (funcResult_0 != null) {
/* 058 */         value_0 = (InternalRow) funcResult_0;
/* 059 */       } else {
/* 060 */         isNull_0 = true;
/* 061 */       }
/* 062 */
/* 063 */
/* 064 */     }
/* 065 */     if (isNull_0) {
/* 066 */       mutableStateArray_1[0].setNullAt(0);
/* 067 */     } else {
/* 068 */       final InternalRow tmpInput_0 = value_0;
/* 069 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 070 */         mutableStateArray_1[0].write(0, (UnsafeRow) tmpInput_0);
/* 071 */       } else {
/* 072 */         // Remember the current cursor so that we can calculate how many bytes are
/* 073 */         // written later.
/* 074 */         final int previousCursor_0 = mutableStateArray_1[0].cursor();
/* 075 */
/* 076 */         mutableStateArray_1[1].resetRowWriter();
/* 077 */
/* 078 */
/* 079 */         mutableStateArray_1[1].write(0, (tmpInput_0.getByte(0)));
/* 080 */
/* 081 */
/* 082 */         if ((tmpInput_0.isNullAt(1))) {
/* 083 */           mutableStateArray_1[1].setNullAt(1);
/* 084 */         } else {
/* 085 */           mutableStateArray_1[1].write(1, (tmpInput_0.getInt(1)));
/* 086 */         }
/* 087 */
/* 088 */
/* 089 */         if ((tmpInput_0.isNullAt(2))) {
/* 090 */           mutableStateArray_1[1].setNullAt(2);
/* 091 */         } else {
/* 092 */           // Remember the current cursor so that we can calculate how many bytes are
/* 093 */           // written later.
/* 094 */           final int previousCursor_1 = mutableStateArray_1[1].cursor();
/* 095 */
/* 096 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 097 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 098 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_1);
/* 099 */           } else {
/* 100 */             final int numElements_0 = tmpInput_1.numElements();
/* 101 */             mutableStateArray_2[0].initialize(numElements_0);
/* 102 */
/* 103 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 104 */               mutableStateArray_2[0].write(index_0, tmpInput_1.getInt(index_0));
/* 105 */             }
/* 106 */           }
/* 107 */
/* 108 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 109 */         }
/* 110 */
/* 111 */
/* 112 */         if ((tmpInput_0.isNullAt(3))) {
/* 113 */           mutableStateArray_1[1].setNullAt(3);
/* 114 */         } else {
/* 115 */           // Remember the current cursor so that we can calculate how many bytes are
/* 116 */           // written later.
/* 117 */           final int previousCursor_2 = mutableStateArray_1[1].cursor();
/* 118 */
/* 119 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 120 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 121 */             mutableStateArray_1[1].write((UnsafeArrayData) tmpInput_2);
/* 122 */           } else {
/* 123 */             final int numElements_1 = tmpInput_2.numElements();
/* 124 */             mutableStateArray_2[1].initialize(numElements_1);
/* 125 */
/* 126 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 127 */               mutableStateArray_2[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 128 */             }
/* 129 */           }
/* 130 */
/* 131 */           mutableStateArray_1[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 132 */         }
/* 133 */
/* 134 */
/* 135 */         mutableStateArray_1[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 136 */       }
/* 137 */     }
/* 138 */     return (mutableStateArray_1[0].getRow());
/* 139 */   }
/* 140 */
/* 141 */
/* 142 */ }

2025-03-26 04:38:11,521 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 37.501658 ms
2025-03-26 04:38:11,531 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 026 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 027 */     boolean isNull_0 = true;
/* 028 */     org.apache.spark.ml.linalg.Vector value_0 = null;
/* 029 */     resultIsNull_0 = false;
/* 030 */     if (!resultIsNull_0) {
/* 031 */       boolean isNull_2 = i.isNullAt(0);
/* 032 */       InternalRow value_2 = isNull_2 ?
/* 033 */       null : (i.getStruct(0, 4));
/* 034 */       resultIsNull_0 = isNull_2;
/* 035 */       mutableStateArray_0[0] = value_2;
/* 036 */     }
/* 037 */
/* 038 */     isNull_0 = resultIsNull_0;
/* 039 */     if (!isNull_0) {
/* 040 */
/* 041 */       Object funcResult_0 = null;
/* 042 */       funcResult_0 = value_1.deserialize(mutableStateArray_0[0]);
/* 043 */
/* 044 */       if (funcResult_0 != null) {
/* 045 */         value_0 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 046 */       } else {
/* 047 */         isNull_0 = true;
/* 048 */       }
/* 049 */
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableRow.setNullAt(0);
/* 054 */     } else {
/* 055 */
/* 056 */       mutableRow.update(0, value_0);
/* 057 */     }
/* 058 */
/* 059 */     return mutableRow;
/* 060 */   }
/* 061 */
/* 062 */
/* 063 */ }

2025-03-26 04:38:11,532 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     final org.apache.spark.ml.linalg.VectorUDT value_1 = false ?
/* 026 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 027 */     boolean isNull_0 = true;
/* 028 */     org.apache.spark.ml.linalg.Vector value_0 = null;
/* 029 */     resultIsNull_0 = false;
/* 030 */     if (!resultIsNull_0) {
/* 031 */       boolean isNull_2 = i.isNullAt(0);
/* 032 */       InternalRow value_2 = isNull_2 ?
/* 033 */       null : (i.getStruct(0, 4));
/* 034 */       resultIsNull_0 = isNull_2;
/* 035 */       mutableStateArray_0[0] = value_2;
/* 036 */     }
/* 037 */
/* 038 */     isNull_0 = resultIsNull_0;
/* 039 */     if (!isNull_0) {
/* 040 */
/* 041 */       Object funcResult_0 = null;
/* 042 */       funcResult_0 = value_1.deserialize(mutableStateArray_0[0]);
/* 043 */
/* 044 */       if (funcResult_0 != null) {
/* 045 */         value_0 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 046 */       } else {
/* 047 */         isNull_0 = true;
/* 048 */       }
/* 049 */
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableRow.setNullAt(0);
/* 054 */     } else {
/* 055 */
/* 056 */       mutableRow.update(0, value_0);
/* 057 */     }
/* 058 */
/* 059 */     return mutableRow;
/* 060 */   }
/* 061 */
/* 062 */
/* 063 */ }

2025-03-26 04:38:11,548 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 16.597661 ms
2025-03-26 04:38:11,554 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, double, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     double value_0 = i.getDouble(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

2025-03-26 04:38:11,554 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     double value_0 = i.getDouble(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

2025-03-26 04:38:11,563 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 8.888873 ms
2025-03-26 04:38:11,598 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection: code for input[0, vector, true],input[1, double, false],input[2, double, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */     writeFields_0_0(i);
/* 035 */     writeFields_0_1(i);
/* 036 */     return (mutableStateArray_0[0].getRow());
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */   private void writeFields_0_1(InternalRow i) {
/* 041 */
/* 042 */     double value_1 = i.getDouble(1);
/* 043 */     mutableStateArray_0[0].write(1, value_1);
/* 044 */
/* 045 */     double value_2 = i.getDouble(2);
/* 046 */     mutableStateArray_0[0].write(2, value_2);
/* 047 */
/* 048 */   }
/* 049 */
/* 050 */
/* 051 */   private void writeFields_0_0(InternalRow i) {
/* 052 */
/* 053 */     boolean isNull_0 = i.isNullAt(0);
/* 054 */     InternalRow value_0 = isNull_0 ?
/* 055 */     null : (i.getStruct(0, 4));
/* 056 */     if (isNull_0) {
/* 057 */       mutableStateArray_0[0].setNullAt(0);
/* 058 */     } else {
/* 059 */       final InternalRow tmpInput_0 = value_0;
/* 060 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 061 */         mutableStateArray_0[0].write(0, (UnsafeRow) tmpInput_0);
/* 062 */       } else {
/* 063 */         // Remember the current cursor so that we can calculate how many bytes are
/* 064 */         // written later.
/* 065 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 066 */
/* 067 */         mutableStateArray_0[1].resetRowWriter();
/* 068 */
/* 069 */
/* 070 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 071 */
/* 072 */
/* 073 */         if ((tmpInput_0.isNullAt(1))) {
/* 074 */           mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 077 */         }
/* 078 */
/* 079 */
/* 080 */         if ((tmpInput_0.isNullAt(2))) {
/* 081 */           mutableStateArray_0[1].setNullAt(2);
/* 082 */         } else {
/* 083 */           // Remember the current cursor so that we can calculate how many bytes are
/* 084 */           // written later.
/* 085 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 086 */
/* 087 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 088 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 089 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 090 */           } else {
/* 091 */             final int numElements_0 = tmpInput_1.numElements();
/* 092 */             mutableStateArray_1[0].initialize(numElements_0);
/* 093 */
/* 094 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 095 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 096 */             }
/* 097 */           }
/* 098 */
/* 099 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 100 */         }
/* 101 */
/* 102 */
/* 103 */         if ((tmpInput_0.isNullAt(3))) {
/* 104 */           mutableStateArray_0[1].setNullAt(3);
/* 105 */         } else {
/* 106 */           // Remember the current cursor so that we can calculate how many bytes are
/* 107 */           // written later.
/* 108 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 109 */
/* 110 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 111 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 112 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 113 */           } else {
/* 114 */             final int numElements_1 = tmpInput_2.numElements();
/* 115 */             mutableStateArray_1[1].initialize(numElements_1);
/* 116 */
/* 117 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 118 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 119 */             }
/* 120 */           }
/* 121 */
/* 122 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 123 */         }
/* 124 */
/* 125 */
/* 126 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 127 */       }
/* 128 */     }
/* 129 */
/* 130 */   }
/* 131 */
/* 132 */ }

2025-03-26 04:38:11,599 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 014 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_0[0], 4);
/* 015 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 4);
/* 016 */     mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[1], 8);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */     writeFields_0_0(i);
/* 035 */     writeFields_0_1(i);
/* 036 */     return (mutableStateArray_0[0].getRow());
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */   private void writeFields_0_1(InternalRow i) {
/* 041 */
/* 042 */     double value_1 = i.getDouble(1);
/* 043 */     mutableStateArray_0[0].write(1, value_1);
/* 044 */
/* 045 */     double value_2 = i.getDouble(2);
/* 046 */     mutableStateArray_0[0].write(2, value_2);
/* 047 */
/* 048 */   }
/* 049 */
/* 050 */
/* 051 */   private void writeFields_0_0(InternalRow i) {
/* 052 */
/* 053 */     boolean isNull_0 = i.isNullAt(0);
/* 054 */     InternalRow value_0 = isNull_0 ?
/* 055 */     null : (i.getStruct(0, 4));
/* 056 */     if (isNull_0) {
/* 057 */       mutableStateArray_0[0].setNullAt(0);
/* 058 */     } else {
/* 059 */       final InternalRow tmpInput_0 = value_0;
/* 060 */       if (tmpInput_0 instanceof UnsafeRow) {
/* 061 */         mutableStateArray_0[0].write(0, (UnsafeRow) tmpInput_0);
/* 062 */       } else {
/* 063 */         // Remember the current cursor so that we can calculate how many bytes are
/* 064 */         // written later.
/* 065 */         final int previousCursor_0 = mutableStateArray_0[0].cursor();
/* 066 */
/* 067 */         mutableStateArray_0[1].resetRowWriter();
/* 068 */
/* 069 */
/* 070 */         mutableStateArray_0[1].write(0, (tmpInput_0.getByte(0)));
/* 071 */
/* 072 */
/* 073 */         if ((tmpInput_0.isNullAt(1))) {
/* 074 */           mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           mutableStateArray_0[1].write(1, (tmpInput_0.getInt(1)));
/* 077 */         }
/* 078 */
/* 079 */
/* 080 */         if ((tmpInput_0.isNullAt(2))) {
/* 081 */           mutableStateArray_0[1].setNullAt(2);
/* 082 */         } else {
/* 083 */           // Remember the current cursor so that we can calculate how many bytes are
/* 084 */           // written later.
/* 085 */           final int previousCursor_1 = mutableStateArray_0[1].cursor();
/* 086 */
/* 087 */           final ArrayData tmpInput_1 = (tmpInput_0.getArray(2));
/* 088 */           if (tmpInput_1 instanceof UnsafeArrayData) {
/* 089 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_1);
/* 090 */           } else {
/* 091 */             final int numElements_0 = tmpInput_1.numElements();
/* 092 */             mutableStateArray_1[0].initialize(numElements_0);
/* 093 */
/* 094 */             for (int index_0 = 0; index_0 < numElements_0; index_0++) {
/* 095 */               mutableStateArray_1[0].write(index_0, tmpInput_1.getInt(index_0));
/* 096 */             }
/* 097 */           }
/* 098 */
/* 099 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, previousCursor_1);
/* 100 */         }
/* 101 */
/* 102 */
/* 103 */         if ((tmpInput_0.isNullAt(3))) {
/* 104 */           mutableStateArray_0[1].setNullAt(3);
/* 105 */         } else {
/* 106 */           // Remember the current cursor so that we can calculate how many bytes are
/* 107 */           // written later.
/* 108 */           final int previousCursor_2 = mutableStateArray_0[1].cursor();
/* 109 */
/* 110 */           final ArrayData tmpInput_2 = (tmpInput_0.getArray(3));
/* 111 */           if (tmpInput_2 instanceof UnsafeArrayData) {
/* 112 */             mutableStateArray_0[1].write((UnsafeArrayData) tmpInput_2);
/* 113 */           } else {
/* 114 */             final int numElements_1 = tmpInput_2.numElements();
/* 115 */             mutableStateArray_1[1].initialize(numElements_1);
/* 116 */
/* 117 */             for (int index_1 = 0; index_1 < numElements_1; index_1++) {
/* 118 */               mutableStateArray_1[1].write(index_1, tmpInput_2.getDouble(index_1));
/* 119 */             }
/* 120 */           }
/* 121 */
/* 122 */           mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(3, previousCursor_2);
/* 123 */         }
/* 124 */
/* 125 */
/* 126 */         mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(0, previousCursor_0);
/* 127 */       }
/* 128 */     }
/* 129 */
/* 130 */   }
/* 131 */
/* 132 */ }

2025-03-26 04:38:11,623 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 24.61202 ms
2025-03-26 04:38:11,628 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection: code for createexternalrow(newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, input[1, double, false], input[2, double, false], StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true), StructField(label,DoubleType,false), StructField(prediction,DoubleType,false)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_6 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_6);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[3];
/* 039 */
/* 040 */     final org.apache.spark.ml.linalg.VectorUDT value_2 = false ?
/* 041 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 042 */     boolean isNull_1 = true;
/* 043 */     org.apache.spark.ml.linalg.Vector value_1 = null;
/* 044 */     resultIsNull_0 = false;
/* 045 */     if (!resultIsNull_0) {
/* 046 */       boolean isNull_3 = i.isNullAt(0);
/* 047 */       InternalRow value_3 = isNull_3 ?
/* 048 */       null : (i.getStruct(0, 4));
/* 049 */       resultIsNull_0 = isNull_3;
/* 050 */       mutableStateArray_0[0] = value_3;
/* 051 */     }
/* 052 */
/* 053 */     isNull_1 = resultIsNull_0;
/* 054 */     if (!isNull_1) {
/* 055 */
/* 056 */       Object funcResult_0 = null;
/* 057 */       funcResult_0 = value_2.deserialize(mutableStateArray_0[0]);
/* 058 */
/* 059 */       if (funcResult_0 != null) {
/* 060 */         value_1 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 061 */       } else {
/* 062 */         isNull_1 = true;
/* 063 */       }
/* 064 */
/* 065 */
/* 066 */     }
/* 067 */     if (isNull_1) {
/* 068 */       values_0[0] = null;
/* 069 */     } else {
/* 070 */       values_0[0] = value_1;
/* 071 */     }
/* 072 */
/* 073 */     double value_4 = i.getDouble(1);
/* 074 */     if (false) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_4;
/* 078 */     }
/* 079 */
/* 080 */     double value_5 = i.getDouble(2);
/* 081 */     if (false) {
/* 082 */       values_0[2] = null;
/* 083 */     } else {
/* 084 */       values_0[2] = value_5;
/* 085 */     }
/* 086 */
/* 087 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 088 */
/* 089 */     return value_0;
/* 090 */   }
/* 091 */
/* 092 */ }

2025-03-26 04:38:11,629 DEBUG [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private boolean resultIsNull_0;
/* 010 */   private InternalRow[] mutableStateArray_0 = new InternalRow[1];
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     org.apache.spark.sql.Row value_6 = CreateExternalRow_0(i);
/* 026 */     if (false) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_6);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 038 */     Object[] values_0 = new Object[3];
/* 039 */
/* 040 */     final org.apache.spark.ml.linalg.VectorUDT value_2 = false ?
/* 041 */     null : new org.apache.spark.ml.linalg.VectorUDT();
/* 042 */     boolean isNull_1 = true;
/* 043 */     org.apache.spark.ml.linalg.Vector value_1 = null;
/* 044 */     resultIsNull_0 = false;
/* 045 */     if (!resultIsNull_0) {
/* 046 */       boolean isNull_3 = i.isNullAt(0);
/* 047 */       InternalRow value_3 = isNull_3 ?
/* 048 */       null : (i.getStruct(0, 4));
/* 049 */       resultIsNull_0 = isNull_3;
/* 050 */       mutableStateArray_0[0] = value_3;
/* 051 */     }
/* 052 */
/* 053 */     isNull_1 = resultIsNull_0;
/* 054 */     if (!isNull_1) {
/* 055 */
/* 056 */       Object funcResult_0 = null;
/* 057 */       funcResult_0 = value_2.deserialize(mutableStateArray_0[0]);
/* 058 */
/* 059 */       if (funcResult_0 != null) {
/* 060 */         value_1 = (org.apache.spark.ml.linalg.Vector) funcResult_0;
/* 061 */       } else {
/* 062 */         isNull_1 = true;
/* 063 */       }
/* 064 */
/* 065 */
/* 066 */     }
/* 067 */     if (isNull_1) {
/* 068 */       values_0[0] = null;
/* 069 */     } else {
/* 070 */       values_0[0] = value_1;
/* 071 */     }
/* 072 */
/* 073 */     double value_4 = i.getDouble(1);
/* 074 */     if (false) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_4;
/* 078 */     }
/* 079 */
/* 080 */     double value_5 = i.getDouble(2);
/* 081 */     if (false) {
/* 082 */       values_0[2] = null;
/* 083 */     } else {
/* 084 */       values_0[2] = value_5;
/* 085 */     }
/* 086 */
/* 087 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 088 */
/* 089 */     return value_0;
/* 090 */   }
/* 091 */
/* 092 */ }

2025-03-26 04:38:11,634 INFO [Driver] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 6.167728 ms
2025-03-26 04:38:11,646 INFO [Driver] org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://slave2:45105
2025-03-26 04:38:11,650 INFO [Driver] org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend: Shutting down all executors
2025-03-26 04:38:11,650 INFO [dispatcher-CoarseGrainedScheduler] org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
2025-03-26 04:38:11,651 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-26 04:38:11,651 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] FLUSH
2025-03-26 04:38:11,652 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-26 04:38:11,652 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] FLUSH
2025-03-26 04:38:11,652 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] WRITE: MessageWithHeader [headerLength: 13, bodyLength: 122]
2025-03-26 04:38:11,652 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] FLUSH
2025-03-26 04:38:11,652 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ 135B
2025-03-26 04:38:11,654 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] READ COMPLETE
2025-03-26 04:38:11,655 INFO [dispatcher-Executor] org.apache.spark.executor.YarnCoarseGrainedExecutorBackend: Driver commanded a shutdown
2025-03-26 04:38:11,665 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 - R:slave2/172.20.1.13:40525] CLOSE
2025-03-26 04:38:11,665 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 ! R:slave2/172.20.1.13:40525] INACTIVE
2025-03-26 04:38:11,665 DEBUG [shuffle-client-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x20ab7aa9, L:/172.20.1.13:56638 ! R:slave2/172.20.1.13:40525] UNREGISTERED
2025-03-26 04:38:11,665 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 - R:/172.20.1.13:56638] READ COMPLETE
2025-03-26 04:38:11,665 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 ! R:/172.20.1.13:56638] INACTIVE
2025-03-26 04:38:11,666 DEBUG [shuffle-server-7-1] org.apache.spark.network.util.NettyLogger: [id: 0xd5215c65, L:/172.20.1.13:40525 ! R:/172.20.1.13:56638] UNREGISTERED
2025-03-26 04:38:11,668 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 - R:/172.20.1.11:43168] READ COMPLETE
2025-03-26 04:38:11,668 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 ! R:/172.20.1.11:43168] INACTIVE
2025-03-26 04:38:11,669 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0xef1d717d, L:/172.20.1.13:34623 ! R:/172.20.1.11:43168] UNREGISTERED
2025-03-26 04:38:11,675 INFO [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2025-03-26 04:38:11,675 INFO [CoarseGrainedExecutorBackend-stop-executor] org.apache.spark.storage.BlockManager: BlockManager stopped
2025-03-26 04:38:11,676 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 - R:/172.20.1.12:33186] READ COMPLETE
2025-03-26 04:38:11,676 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 ! R:/172.20.1.12:33186] INACTIVE
2025-03-26 04:38:11,676 INFO [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-03-26 04:38:11,677 DEBUG [rpc-server-4-1] org.apache.spark.network.util.NettyLogger: [id: 0x8115354e, L:/172.20.1.13:34623 ! R:/172.20.1.12:33186] UNREGISTERED
2025-03-26 04:38:11,677 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 - R:slave2/172.20.1.13:34623] CLOSE
2025-03-26 04:38:11,677 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 ! R:slave2/172.20.1.13:34623] INACTIVE
2025-03-26 04:38:11,677 DEBUG [rpc-client-3-1] org.apache.spark.network.util.NettyLogger: [id: 0x0dd380fa, L:/172.20.1.13:38318 ! R:slave2/172.20.1.13:34623] UNREGISTERED
2025-03-26 04:38:11,677 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 - R:/172.20.1.13:38318] READ COMPLETE
2025-03-26 04:38:11,678 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 ! R:/172.20.1.13:38318] INACTIVE
2025-03-26 04:38:11,678 DEBUG [rpc-server-4-2] org.apache.spark.network.util.NettyLogger: [id: 0x92ba27a6, L:/172.20.1.13:34623 ! R:/172.20.1.13:38318] UNREGISTERED
2025-03-26 04:38:11,687 INFO [Driver] org.apache.spark.storage.memory.MemoryStore: MemoryStore cleared
2025-03-26 04:38:11,687 INFO [Driver] org.apache.spark.storage.BlockManager: BlockManager stopped
2025-03-26 04:38:11,693 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2025-03-26 04:38:11,695 INFO [Driver] org.apache.spark.storage.BlockManagerMaster: BlockManagerMaster stopped
2025-03-26 04:38:11,695 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: Completed shutdown in 0.011 seconds; Timeouts: 0
2025-03-26 04:38:11,697 INFO [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-03-26 04:38:11,702 INFO [Driver] org.apache.spark.SparkContext: Successfully stopped SparkContext
2025-03-26 04:38:11,702 INFO [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
2025-03-26 04:38:11,707 DEBUG [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: shutting down reporter thread
2025-03-26 04:38:11,708 DEBUG [Driver] org.apache.spark.deploy.yarn.ApplicationMaster: Done running user class
2025-03-26 04:38:11,710 INFO [shutdown-hook-0] org.apache.spark.deploy.yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
2025-03-26 04:38:11,713 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #11 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.finishApplicationMaster
2025-03-26 04:38:11,716 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: ShutdownHookManager completed shutdown.
2025-03-26 04:38:11,720 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #11
2025-03-26 04:38:11,720 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: finishApplicationMaster took 7ms
2025-03-26 04:38:11,734 INFO [shutdown-hook-0] org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
2025-03-26 04:38:11,840 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root sending #12 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.finishApplicationMaster
2025-03-26 04:38:11,841 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:8030 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:8030 from root got value #12
2025-03-26 04:38:11,841 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: finishApplicationMaster took 1ms
2025-03-26 04:38:11,841 DEBUG [shutdown-hook-0] org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl entered state STOPPED
2025-03-26 04:38:11,841 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: stopping client from cache: Client-dd5ed27233bf44d986fbe0731c03d648
2025-03-26 04:38:11,842 INFO [shutdown-hook-0] org.apache.spark.deploy.yarn.ApplicationMaster: Deleting staging directory hdfs://master:9000/user/root/.sparkStaging/application_1742963823806_0001
2025-03-26 04:38:11,843 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2025-03-26 04:38:11,843 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: Connecting to master/172.20.1.10:9000
2025-03-26 04:38:11,843 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: Setup connection to master/172.20.1.10:9000
2025-03-26 04:38:11,843 DEBUG [shutdown-hook-0] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.ipc.Client$Connection$2@51e7208]
java.lang.Exception
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy36.delete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:655)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy37.delete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1662)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:992)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:989)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:999)
	at org.apache.spark.deploy.yarn.ApplicationMaster.cleanupStagingDir(ApplicationMaster.scala:686)
	at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$run$2(ApplicationMaster.scala:265)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2025-03-26 04:38:11,844 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: NEGOTIATE

2025-03-26 04:38:11,844 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
2025-03-26 04:38:11,844 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one
2025-03-26 04:38:11,844 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Use SIMPLE authentication for protocol ClientNamenodeProtocolPB
2025-03-26 04:38:11,844 DEBUG [shutdown-hook-0] org.apache.hadoop.security.SaslRpcClient: Sending sasl message state: INITIATE
auths {
  method: "SIMPLE"
  mechanism: ""
}

2025-03-26 04:38:11,844 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root: starting, having connections 2
2025-03-26 04:38:11,845 DEBUG [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete
2025-03-26 04:38:11,852 DEBUG [IPC Client (1244880808) connection to master/172.20.1.10:9000 from root] org.apache.hadoop.ipc.Client: IPC Client (1244880808) connection to master/172.20.1.10:9000 from root got value #13
2025-03-26 04:38:11,852 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.ProtobufRpcEngine2: Call: delete took 9ms
2025-03-26 04:38:11,853 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Shutdown hook called
2025-03-26 04:38:11,853 INFO [shutdown-hook-0] org.apache.spark.util.ShutdownHookManager: Deleting directory /data/tmp/nm-local-dir/usercache/root/appcache/application_1742963823806_0001/spark-db564513-a775-4883-aa6e-899518aef777
2025-03-26 04:38:11,855 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.fs.FilterFileSystem.close(FilterFileSystem.java:529)); Key: (root (auth:SIMPLE))@file://; URI: file:///; Object Identity Hash: 411b8f70
2025-03-26 04:38:11,855 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.fs.RawLocalFileSystem.close(RawLocalFileSystem.java:759)); Key: null; URI: file:///; Object Identity Hash: 4fb42aaa
2025-03-26 04:38:11,855 DEBUG [shutdown-hook-0] org.apache.hadoop.fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1518)); Key: (root (auth:SIMPLE))@hdfs://master:9000; URI: hdfs://master:9000; Object Identity Hash: 65c71bc2
2025-03-26 04:38:11,856 DEBUG [shutdown-hook-0] org.apache.hadoop.ipc.Client: stopping client from cache: Client-dd5ed27233bf44d986fbe0731c03d648
2025-03-26 04:38:11,856 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: Completed shutdown in 0.148 seconds; Timeouts: 0
2025-03-26 04:38:11,867 DEBUG [Thread-2] org.apache.hadoop.util.ShutdownHookManager: ShutdownHookManager completed shutdown.
