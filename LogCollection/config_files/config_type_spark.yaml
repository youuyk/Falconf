spark-config:
  spark.app.name: string
  spark.archives: string
  spark.barrier.sync.timeout: timeduration
  spark.blockManager.port: port
  spark.broadcast.blockSize: size
  spark.broadcast.checksum: boolean
  spark.broadcast.compress: boolean
  spark.checkpoint.compress: boolean
  spark.cleaner.periodicGC.interval: timeduration
  spark.cleaner.referenceTracking: boolean
  spark.cleaner.referenceTracking.blocking: boolean
  spark.cleaner.referenceTracking.blocking.shuffle: boolean
  spark.cleaner.referenceTracking.cleanCheckpoints: boolean
  spark.cores.max: size
  spark.default.parallelism: int
  spark.deploy.recoveryMode: string
  spark.deploy.zookeeper.dir: string
  spark.deploy.zookeeper.url: string
  spark.driver.bindAddress: ip
  spark.driver.blockManager.port: port
  spark.driver.cores: int
  spark.driver.defaultJavaOptions: string
  spark.driver.extraClassPath: class
  spark.driver.extraJavaOptions: string
  spark.driver.extraLibraryPath: path
  spark.driver.host: ip
  spark.driver.log.allowErasureCoding: boolean
  spark.driver.log.dfsDir: directory
  spark.driver.log.layout: string
  spark.driver.log.persistToDfs.enabled: boolean
  spark.driver.maxResultSize: size
  spark.driver.memory: size
  spark.driver.memoryOverhead: size
  spark.driver.memoryOverheadFactor: float
  spark.driver.port: port
  spark.driver.supervise: boolean
  spark.driver.userClassPathFirst: boolean
  spark.dynamicAllocation.cachedExecutorIdleTimeout: timeduration
  spark.dynamicAllocation.enabled: boolean
  spark.dynamicAllocation.executorAllocationRatio: float
  spark.dynamicAllocation.executorIdleTimeout: timeduration
  spark.dynamicAllocation.initialExecutors: int
  spark.dynamicAllocation.maxExecutors: int
  spark.dynamicAllocation.minExecutors: int
  spark.dynamicAllocation.schedulerBacklogTimeout: timeduration
  spark.dynamicAllocation.shuffleTracking.enabled: boolean
  spark.dynamicAllocation.shuffleTracking.timeout: timeduration
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout: timeduration
  spark.eventLog.buffer.kb: size
  spark.eventLog.compress: boolean
  spark.eventLog.compression.codec: class
  spark.eventLog.dir: directory
  spark.eventLog.enabled: boolean
  spark.eventLog.erasureCoding.enabled: boolean
  spark.eventLog.logBlockUpdates.enabled: boolean
  spark.eventLog.logStageExecutorMetrics: boolean
  spark.eventLog.longForm.enabled: boolean
  spark.eventLog.overwrite: boolean
  spark.eventLog.rolling.enabled: boolean
  spark.eventLog.rolling.maxFileSize: size
  spark.excludeOnFailure.application.fetchFailure.enabled: boolean
  spark.excludeOnFailure.application.maxFailedExecutorsPerNode: int
  spark.excludeOnFailure.application.maxFailedTasksPerExecutor: int
  spark.excludeOnFailure.enabled: boolean
  spark.excludeOnFailure.killExcludedExecutors: boolean
  spark.excludeOnFailure.stage.maxFailedExecutorsPerNode: int
  spark.excludeOnFailure.stage.maxFailedTasksPerExecutor: int
  spark.excludeOnFailure.task.maxTaskAttemptsPerExecutor: int
  spark.excludeOnFailure.task.maxTaskAttemptsPerNode: int
  spark.excludeOnFailure.timeout: timeduration
  spark.executor.cores: int
  spark.executor.defaultJavaOptions: string
  spark.executor.extraClassPath: class
  spark.executor.extraJavaOptions: string
  spark.executor.extraLibraryPath: path
  spark.executor.heartbeatInterval: timeduration
  spark.executor.logs.rolling.enableCompression: boolean
  spark.executor.logs.rolling.maxRetainedFiles: int
  spark.executor.logs.rolling.maxSize: size
  spark.executor.logs.rolling.strategy: size
  spark.executor.logs.rolling.time.interval: timeduration
  spark.executor.memory: size
  spark.executor.memoryOverhead: size
  spark.executor.memoryOverheadFactor: float
  spark.executor.metrics.pollingInterval: int
  spark.executor.pyspark.memory: size
  spark.executor.userClassPathFirst: boolean
  spark.extraListeners: class
  spark.files: directory
  spark.files.fetchTimeout: timeduration
  spark.files.io.connectionTimeout: timeduration
  spark.files.maxPartitionBytes: int
  spark.files.openCostInBytes: int
  spark.files.overwrite: boolean
  spark.files.useFetchCache: boolean
  spark.graphx.pregel.checkpointInterval: timeduration
  spark.hadoop.cloneConf: boolean
  spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: int
  spark.hadoop.validateOutputSpecs: boolean
  spark.io.compression.codec: class
  spark.io.compression.lz4.blockSize: size
  spark.io.compression.snappy.blockSize: size
  spark.io.compression.zstd.bufferSize: size
  spark.io.compression.zstd.level: int
  spark.jars: class
  spark.jars.excludes: class
  spark.jars.ivy: path
  spark.jars.ivySettings: path
  spark.jars.packages: class
  spark.jars.repositories: class
  spark.kryo.classesToRegister: class
  spark.kryo.referenceTracking: boolean
  spark.kryo.registrationRequired: boolean
  spark.kryo.registrator: class
  spark.kryo.unsafe: boolean
  spark.kryoserializer.buffer: size
  spark.kryoserializer.buffer.max: size
  spark.local.dir: directory
  spark.locality.wait: timeduration
  spark.locality.wait.node: string
  spark.locality.wait.process: string
  spark.locality.wait.rack: string
  spark.log.callerContext: string
  spark.logConf: boolean
  spark.master: string
  spark.memory.fraction: float
  spark.memory.offHeap.enabled: boolean
  spark.memory.offHeap.size: int
  spark.memory.storageFraction: float
  spark.network.io.preferDirectBufs: boolean
  spark.network.maxRemoteBlockSizeFetchToMem: size
  spark.network.timeout: timeduration
  spark.port.maxRetries: int
  spark.pyspark.driver.python: string
  spark.pyspark.python: string
  spark.python.profile: boolean
  spark.python.profile.dump: directory
  spark.python.worker.memory: size
  spark.python.worker.reuse: int
  spark.r.backendConnectionTimeout: int
  spark.r.command: string
  spark.r.driver.command: string
  spark.r.heartBeatInterval: int
  spark.r.numRBackendThreads: int
  spark.r.shell.command: string
  spark.rdd.compress: boolean
  spark.redaction.regex: string
  spark.reducer.maxBlocksInFlightPerAddress: int
  spark.reducer.maxReqsInFlight: int
  spark.reducer.maxSizeInFlight: size
  spark.resources.discoveryPlugin: class
  spark.rpc.askTimeout: timeduration
  spark.rpc.io.backLog: int
  spark.rpc.io.connectionTimeout: timeduration
  spark.rpc.lookupTimeout: timeduration
  spark.rpc.message.maxSize: int
  spark.rpc.numRetries: int
  spark.rpc.retry.wait: timeduration
  spark.scheduler.barrier.maxConcurrentTasksCheck.interval: timeduration
  spark.scheduler.barrier.maxConcurrentTasksCheck.maxFailures: int
  spark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout: timeduration
  spark.scheduler.listenerbus.eventqueue.appStatus.capacity: int
  spark.scheduler.listenerbus.eventqueue.capacity: int
  spark.scheduler.listenerbus.eventqueue.eventLog.capacity: int
  spark.scheduler.listenerbus.eventqueue.executorManagement.capacity: int
  spark.scheduler.listenerbus.eventqueue.shared.capacity: int
  spark.scheduler.listenerbus.eventqueue.streams.capacity: int
  spark.scheduler.maxRegisteredResourcesWaitingTime: size
  spark.scheduler.minRegisteredResourcesRatio: float
  spark.scheduler.mode: string
  spark.scheduler.resource.profileMergeConflicts: boolean
  spark.scheduler.revive.interval: timeduration
  spark.serializer: class
  spark.serializer.objectStreamReset: int
  spark.shuffle.accurateBlockThreshold: size
  spark.shuffle.checksum.algorithm: string
  spark.shuffle.checksum.enabled: boolean
  spark.shuffle.compress: boolean
  spark.shuffle.file.buffer: size
  spark.shuffle.io.backLog: int
  spark.shuffle.io.connectionTimeout: timeduration
  spark.shuffle.io.maxRetries: int
  spark.shuffle.io.numConnectionsPerPeer: int
  spark.shuffle.io.preferDirectBufs: boolean
  spark.shuffle.io.retryWait: timeduration
  spark.shuffle.maxChunksBeingTransferred: int
  spark.shuffle.push.enabled: boolean
  spark.shuffle.push.finalize.timeout: size
  spark.shuffle.push.maxBlockBatchSize: size
  spark.shuffle.push.maxBlockSizeToPush: size
  spark.shuffle.push.maxRetainedMergerLocations: int
  spark.shuffle.push.mergersMinStaticThreshold: int
  spark.shuffle.push.mergersMinThresholdRatio: float
  spark.shuffle.push.minCompletedPushRatio: float
  spark.shuffle.push.minShuffleSizeToWait: size
  spark.shuffle.push.server.mergedIndexCacheSize: size
  spark.shuffle.push.server.mergedShuffleFileManagerImpl: string
  spark.shuffle.push.server.minChunkSizeInMergedShuffleFile: size
  spark.shuffle.registration.maxAttempts: int
  spark.shuffle.registration.timeout: int
  spark.shuffle.service.enabled: boolean
  spark.shuffle.service.fetch.rdd.enabled: boolean
  spark.shuffle.service.index.cache.size: size
  spark.shuffle.service.port: int
  spark.shuffle.service.removeShuffle: boolean
  spark.shuffle.sort.bypassMergeThreshold: int
  spark.shuffle.spill.compress: boolean
  spark.speculation: boolean
  spark.speculation.interval: int
  spark.speculation.minTaskRuntime: size
  spark.speculation.multiplier: int
  spark.speculation.quantile: float
  spark.speculation.task.duration.threshold: timeduration
  spark.sql.adaptive.advisoryPartitionSizeInBytes: size
  spark.sql.adaptive.autoBroadcastJoinThreshold: int
  spark.sql.adaptive.coalescePartitions.enabled: boolean
  spark.sql.adaptive.coalescePartitions.initialPartitionNum: int
  spark.sql.adaptive.coalescePartitions.minPartitionSize: size
  spark.sql.adaptive.coalescePartitions.parallelismFirst: boolean
  spark.sql.adaptive.customCostEvaluatorClass: class
  spark.sql.adaptive.enabled: boolean
  spark.sql.adaptive.forceOptimizeSkewedJoin: boolean
  spark.sql.adaptive.localShuffleReader.enabled: boolean
  spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold: size
  spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled: boolean
  spark.sql.adaptive.optimizer.excludedRules: string
  spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor: float
  spark.sql.adaptive.skewJoin.enabled: boolean
  spark.sql.adaptive.skewJoin.skewedPartitionFactor: int
  spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes: size
  spark.sql.ansi.enabled: boolean
  spark.sql.ansi.enforceReservedKeywords: boolean
  spark.sql.autoBroadcastJoinThreshold: size
  spark.sql.avro.compression.codec: string
  spark.sql.avro.deflate.level: int
  spark.sql.avro.filterPushdown.enabled: boolean
  spark.sql.broadcastTimeout: int
  spark.sql.bucketing.coalesceBucketsInJoin.enabled: boolean
  spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio: int
  spark.sql.cache.serializer: string
  spark.sql.catalog.spark_catalog: string
  spark.sql.cbo.enabled: boolean
  spark.sql.cbo.joinReorder.dp.star.filter: boolean
  spark.sql.cbo.joinReorder.dp.threshold: int
  spark.sql.cbo.joinReorder.enabled: boolean
  spark.sql.cbo.planStats.enabled: boolean
  spark.sql.cbo.starSchemaDetection: boolean
  spark.sql.charAsVarchar: boolean
  spark.sql.cli.print.header: boolean
  spark.sql.columnNameOfCorruptRecord: string
  spark.sql.csv.filterPushdown.enabled: boolean
  spark.sql.datetime.java8API.enabled: boolean
  spark.sql.debug.maxToStringFields: int
  spark.sql.defaultCatalog: string
  spark.sql.event.truncate.length: int
  spark.sql.execution.arrow.enabled: boolean
  spark.sql.execution.arrow.fallback.enabled: boolean
  spark.sql.execution.arrow.maxRecordsPerBatch: int
  spark.sql.execution.arrow.pyspark.enabled: boolean
  spark.sql.execution.arrow.pyspark.fallback.enabled: boolean
  spark.sql.execution.arrow.pyspark.selfDestruct.enabled: boolean
  spark.sql.execution.arrow.sparkr.enabled: boolean
  spark.sql.execution.pandas.udf.buffer.size: int
  spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled: boolean
  spark.sql.execution.topKSortFallbackThreshold: int
  spark.sql.extensions: string
  spark.sql.files.ignoreCorruptFiles: boolean
  spark.sql.files.ignoreMissingFiles: boolean
  spark.sql.files.maxPartitionBytes: size
  spark.sql.files.maxRecordsPerFile: int
  spark.sql.files.minPartitionNum: int
  spark.sql.function.concatBinaryAsString: boolean
  spark.sql.function.eltOutputAsString: boolean
  spark.sql.groupByAliases: boolean
  spark.sql.groupByOrdinal: boolean
  spark.sql.hive.convertInsertingPartitionedTable: boolean
  spark.sql.hive.convertMetastoreCtas: boolean
  spark.sql.hive.convertMetastoreInsertDir: boolean
  spark.sql.hive.convertMetastoreOrc: boolean
  spark.sql.hive.convertMetastoreParquet: boolean
  spark.sql.hive.convertMetastoreParquet.mergeSchema: boolean
  spark.sql.hive.filesourcePartitionFileCacheSize: int
  spark.sql.hive.manageFilesourcePartitions: boolean
  spark.sql.hive.metastore.barrierPrefixes: string
  spark.sql.hive.metastore.jars: string
  spark.sql.hive.metastore.jars.path: string
  spark.sql.hive.metastore.sharedPrefixes: string
  spark.sql.hive.metastore.version: string
  spark.sql.hive.metastorePartitionPruning: boolean
  spark.sql.hive.metastorePartitionPruningFallbackOnException: boolean
  spark.sql.hive.metastorePartitionPruningFastFallback: boolean
  spark.sql.hive.thriftServer.async: boolean
  spark.sql.hive.thriftServer.singleSession: boolean
  spark.sql.hive.verifyPartitionPath: boolean
  spark.sql.hive.version: string
  spark.sql.inMemoryColumnarStorage.batchSize: int
  spark.sql.inMemoryColumnarStorage.compressed: boolean
  spark.sql.inMemoryColumnarStorage.enableVectorizedReader: boolean
  spark.sql.json.filterPushdown.enabled: boolean
  spark.sql.jsonGenerator.ignoreNullFields: boolean
  spark.sql.leafNodeDefaultParallelism: string
  spark.sql.mapKeyDedupPolicy: string
  spark.sql.maven.additionalRemoteRepositories: string
  spark.sql.maxMetadataStringLength: int
  spark.sql.maxPlanStringLength: int
  spark.sql.metadataCacheTTLSeconds: timeduration
  spark.sql.optimizer.collapseProjectAlwaysInline: boolean
  spark.sql.optimizer.dynamicPartitionPruning.enabled: boolean
  spark.sql.optimizer.enableCsvExpressionOptimization: boolean
  spark.sql.optimizer.enableJsonExpressionOptimization: boolean
  spark.sql.optimizer.excludedRules: string
  spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold: size
  spark.sql.optimizer.runtime.bloomFilter.creationSideThreshold: size
  spark.sql.optimizer.runtime.bloomFilter.enabled: boolean
  spark.sql.optimizer.runtime.bloomFilter.expectedNumItems: int
  spark.sql.optimizer.runtime.bloomFilter.maxNumBits: int
  spark.sql.optimizer.runtime.bloomFilter.maxNumItems: int
  spark.sql.optimizer.runtime.bloomFilter.numBits: int
  spark.sql.optimizer.runtimeFilter.number.threshold: int
  spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled: boolean
  spark.sql.orc.aggregatePushdown: boolean
  spark.sql.orc.columnarReaderBatchSize: int
  spark.sql.orc.compression.codec: string
  spark.sql.orc.enableNestedColumnVectorizedReader: boolean
  spark.sql.orc.enableVectorizedReader: boolean
  spark.sql.orc.filterPushdown: boolean
  spark.sql.orc.mergeSchema: boolean
  spark.sql.orderByOrdinal: boolean
  spark.sql.parquet.aggregatePushdown: boolean
  spark.sql.parquet.binaryAsString: boolean
  spark.sql.parquet.columnarReaderBatchSize: int
  spark.sql.parquet.compression.codec: string
  spark.sql.parquet.enableNestedColumnVectorizedReader: boolean
  spark.sql.parquet.enableVectorizedReader: boolean
  spark.sql.parquet.fieldId.read.enabled: boolean
  spark.sql.parquet.fieldId.read.ignoreMissing: boolean
  spark.sql.parquet.fieldId.write.enabled: boolean
  spark.sql.parquet.filterPushdown: boolean
  spark.sql.parquet.int96AsTimestamp: boolean
  spark.sql.parquet.int96TimestampConversion: boolean
  spark.sql.parquet.mergeSchema: boolean
  spark.sql.parquet.outputTimestampType: string
  spark.sql.parquet.recordLevelFilter.enabled: boolean
  spark.sql.parquet.respectSummaryFiles: boolean
  spark.sql.parquet.writeLegacyFormat: boolean
  spark.sql.parser.quotedRegexColumnNames: boolean
  spark.sql.pivotMaxValues: int
  spark.sql.pyspark.inferNestedDictAsStruct.enabled: boolean
  spark.sql.pyspark.jvmStacktrace.enabled: boolean
  spark.sql.queryExecutionListeners: string
  spark.sql.redaction.options.regex: string
  spark.sql.redaction.string.regex: string
  spark.sql.repl.eagerEval.enabled: boolean
  spark.sql.repl.eagerEval.maxNumRows: int
  spark.sql.repl.eagerEval.truncate: int
  spark.sql.session.timeZone: string
  spark.sql.shuffle.partitions: int
  spark.sql.shuffledHashJoinFactor: int
  spark.sql.sources.bucketing.autoBucketedScan.enabled: boolean
  spark.sql.sources.bucketing.enabled: boolean
  spark.sql.sources.bucketing.maxBuckets: int
  spark.sql.sources.default: string
  spark.sql.sources.disabledJdbcConnProviderList: string
  spark.sql.sources.parallelPartitionDiscovery.threshold: int
  spark.sql.sources.partitionColumnTypeInference.enabled: boolean
  spark.sql.sources.partitionOverwriteMode: string
  spark.sql.sources.v2.bucketing.enabled: boolean
  spark.sql.statistics.fallBackToHdfs: boolean
  spark.sql.statistics.histogram.enabled: boolean
  spark.sql.statistics.size.autoUpdate.enabled: boolean
  spark.sql.storeAssignmentPolicy: string
  spark.sql.streaming.checkpointLocation: string
  spark.sql.streaming.continuous.epochBacklogQueueSize: int
  spark.sql.streaming.disabledV2Writers: string
  spark.sql.streaming.fileSource.cleaner.numThreads: int
  spark.sql.streaming.forceDeleteTempCheckpointLocation: boolean
  spark.sql.streaming.metricsEnabled: boolean
  spark.sql.streaming.multipleWatermarkPolicy: string
  spark.sql.streaming.noDataMicroBatches.enabled: boolean
  spark.sql.streaming.numRecentProgressUpdates: int
  spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition: boolean
  spark.sql.streaming.stateStore.stateSchemaCheck: boolean
  spark.sql.streaming.stopActiveRunOnRestart: boolean
  spark.sql.streaming.stopTimeout: int
  spark.sql.streaming.streamingQueryListeners: string
  spark.sql.streaming.ui.enabled: boolean
  spark.sql.streaming.ui.retainedProgressUpdates: int
  spark.sql.streaming.ui.retainedQueries: int
  spark.sql.thriftServer.interruptOnCancel: boolean
  spark.sql.thriftServer.queryTimeout: timeduration
  spark.sql.thriftserver.scheduler.pool: string
  spark.sql.thriftserver.ui.retainedSessions: int
  spark.sql.thriftserver.ui.retainedStatements: int
  spark.sql.ui.explainMode: string
  spark.sql.ui.retainedExecutions: int
  spark.sql.variable.substitute: boolean
  spark.sql.warehouse.dir: directory
  spark.stage.maxConsecutiveAttempts: int
  spark.storage.memoryMapThreshold: size
  spark.storage.replication.proactive: boolean
  spark.streaming.backpressure.enabled: boolean
  spark.streaming.backpressure.initialRate: float
  spark.streaming.blockInterval: timeduration
  spark.streaming.driver.writeAheadLog.closeFileAfterWrite: boolean
  spark.streaming.kafka.maxRatePerPartition: float
  spark.streaming.kafka.minRatePerPartition: int
  spark.streaming.receiver.maxRate: float
  spark.streaming.receiver.writeAheadLog.closeFileAfterWrite: boolean
  spark.streaming.receiver.writeAheadLog.enable: boolean
  spark.streaming.stopGracefullyOnShutdown: boolean
  spark.streaming.ui.retainedBatches: int
  spark.streaming.unpersist: boolean
  spark.submit.deployMode: string
  spark.submit.pyFiles: path
  spark.task.cpus: int
  spark.task.maxFailures: int
  spark.task.reaper.enabled: boolean
  spark.task.reaper.killTimeout: timeduration
  spark.task.reaper.pollingInterval: timeduration
  spark.task.reaper.threadDump: boolean
  spark.ui.custom.executor.log.url: path
  spark.ui.dagGraph.retainedRootRDDs: int
  spark.ui.enabled: boolean
  spark.ui.filters: string
  spark.ui.killEnabled: boolean
  spark.ui.liveUpdate.minFlushPeriod: timeduration
  spark.ui.liveUpdate.period: size
  spark.ui.port: int
  spark.ui.proxyRedirectUri: path
  spark.ui.requestHeaderSize: size
  spark.ui.retainedDeadExecutors: int
  spark.ui.retainedJobs: int
  spark.ui.retainedStages: int
  spark.ui.retainedTasks: int
  spark.ui.reverseProxy: boolean
  spark.ui.reverseProxyUrl: path
  spark.ui.showConsoleProgress: boolean
  spark.ui.timeline.executors.maximum: int
  spark.ui.timeline.jobs.maximum: int
  spark.ui.timeline.stages.maximum: int
  spark.ui.timeline.tasks.maximum: int
  spark.worker.ui.retainedDrivers: int
  spark.worker.ui.retainedExecutors: int
  spark.driver.resource.gpu.amount: int 
  spark.driver.resource.gpu.discoveryScript: string 
  spark.driver.resource.gpu.vendor: string 
  spark.executor.resource.gpu.amount: int 
  spark.executor.resource.gpu.discoveryScript: string 
  spark.executor.resource.gpu.vendor: string 
  spark.executorEnv.JAVA_HOME: string
  spark.executorEnv.PYSPARK_PYTHON: string
  spark.executorEnv.PYSPARK_DRIVER_PYTHON: string
  spark.executorEnv.SPARKR_DRIVER_R: string
  spark.executorEnv.SPARK_LOCAL_IP: string
  spark.executorEnv.SPARK_PUBLIC_DNS: string
  spark.task.resource.gpu.amount: int 
  spark.driver.rpc.io.serverThreads: int
  spark.driver.rpc.io.clientThreads: int 
  spark.driver.rpc.netty.dispatcher.numThreads: int 
  spark.executor.rpc.io.serverThreads: int
  spark.executor.rpc.io.clientThreads: int 
  spark.executor.rpc.netty.dispatcher.numThreads: int 
  
